[ {
  "elapsedMillis" : 7714,
  "totalScheduledMillis" : 227384,
  "cpuMillis" : 29188,
  "queuedMillis" : 2,
  "executeMillis" : 7023,
  "getResultMillis" : 0,
  "iterateMillis" : 1172,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 604108272,
  "query" : "SELECT COUNT(DISTINCT accountid) AS distinct_account_count\nFROM datalake.imhotep.passsigninattempt\nWHERE unixtime > IMHOTEP_UNIXTIME('1month') AND successful = 1",
  "queryTables" : [ "datalakehive.imhotep.passsigninattempt" ],
  "queryIndex" : 0,
  "runStartToQueryComplete" : 10
}, {
  "elapsedMillis" : 2669,
  "totalScheduledMillis" : 5137,
  "cpuMillis" : 561,
  "queuedMillis" : 1,
  "executeMillis" : 1097,
  "getResultMillis" : 0,
  "iterateMillis" : 1667,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 21326980,
  "query" : "SELECT *\nFROM datalake.imhotep.ruleContentResult_communications\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.rulecontentresult_communications" ],
  "queryIndex" : 1,
  "runStartToQueryComplete" : 9
}, {
  "elapsedMillis" : 11679,
  "totalScheduledMillis" : 43167015,
  "cpuMillis" : 3260628,
  "queuedMillis" : 1,
  "executeMillis" : 3314,
  "getResultMillis" : 0,
  "iterateMillis" : 8471,
  "rows" : 988,
  "error" : null,
  "scannedBytes" : 151576557379,
  "query" : "-- Calculate the block rate change for each day compared to a rolling average of the previous 7 (over a total window of 30 days)\nWITH daily_data AS (\n    SELECT\n        date(from_unixtime(lp.unixtime)) AS date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(CAST(lp.flagged  AS DOUBLE)) as block_count,\n        COUNT(*) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date(from_unixtime(lp.unixtime)) ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date(from_unixtime(lp.unixtime)) ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(DATE(CURRENT_DATE) - interval '60' day) AND imhotep_unixtime(DATE(CURRENT_DATE))\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date(from_unixtime(lp.unixtime)),\n        appName, \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        appName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        block_count * 1.0 / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        (block_count + 4.5) * 1.0 / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        weekly_block_count * 1.0 /  weekly_request_count as raw_weekly_block_rate,  \n        (weekly_block_count + 4.5) * 1.0 / ( weekly_request_count + 9.0) as  weekly_block_rate, -- corrected/smoothed\n        ((block_count + 4.5) * 1.0 / (request_count + 9.0)) - ((weekly_block_count + 4.5) * 1.0 / ( weekly_request_count + 9.0)) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        appName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        block_rate - weekly_block_rate as change,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        appName, \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 3.0 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 3.0 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        request_count,\n        change - 3.0 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 3.0 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count        \n    FROM\n        block_rate_change\n    WHERE\n    \tweekly_request_count > 100\n        and request_count > 100\n)\nSELECT\n    date,\n    appName, \n    moderationApi, \n    moderationType,\n    request_count,\n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change_lower_bound,\n    change_upper_bound,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 6), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n     -- see rate_change query for filters by appName etc.    \nORDER BY\n    date DESC, appName ASC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 2,
  "runStartToQueryComplete" : 21
}, {
  "elapsedMillis" : 11287,
  "totalScheduledMillis" : 1319676,
  "cpuMillis" : 939545,
  "queuedMillis" : 2,
  "executeMillis" : 6796,
  "getResultMillis" : 0,
  "iterateMillis" : 4622,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0\n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 3,
  "runStartToQueryComplete" : 21
}, {
  "elapsedMillis" : 11246,
  "totalScheduledMillis" : 1398592,
  "cpuMillis" : 944228,
  "queuedMillis" : 4,
  "executeMillis" : 6512,
  "getResultMillis" : 0,
  "iterateMillis" : 4848,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_4 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0  \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 4,
  "runStartToQueryComplete" : 21
}, {
  "elapsedMillis" : 7150,
  "totalScheduledMillis" : 23317977,
  "cpuMillis" : 3747540,
  "queuedMillis" : 1,
  "executeMillis" : 1422,
  "getResultMillis" : 0,
  "iterateMillis" : 5802,
  "rows" : 714,
  "error" : null,
  "scannedBytes" : 151573204518,
  "query" : "-- Calculate the block rate change for each day compared to a rolling average of the previous 7 (over a total window of 30 days)\nWITH expected_stats AS (\n    SELECT * FROM (\n        VALUES\n        ('autojob-v2', 'mcmurdoDenylist', 'output', 0.0000592),\n        ('autojob-v2', 'mcmurdoDetoxify', 'output', 0.0238791),\n        ('ContentGenerationService', 'openAIModerations', 'input', 0.000150625942320615), -- 0.0017878\n        ('ContentGenerationService', 'mcmurdoDenylist', 'output', 0.0000442415572580617), -- 0.0000505\n        ('ContentGenerationService', 'mcmurdoDetoxify', 'output', 0.018290404666108200), -- 0.0190764\n        ('ContentGenerationService', 'openAIModerations', 'output', 0.00000863446347650164), --0.0000279\n        ('hire-candidate-rematch-api', 'openAIModerations', 'input', 0.001641), \n        ('hire-candidate-rematch-api', 'mcmurdoDenylist', 'output', 0.0002042),\n        ('hire-candidate-rematch-api', 'openAIModerations', 'output', 0.0000157),\n        ('jobsearch-chatbot', 'openAIModerations', 'input', 0.000245824682575176), --0.0003198\n        ('jobsearch-chatbot', 'mcmurdoDenylist', 'output', 0.0000542340524768692), -- 0.0001485\n        ('jobsearch-chatbot', 'openAIModerations', 'output', 0.0), -- 0.000308 why did this one change so much?\n        ('monai', 'openAIModerations', 'input', 0.0054594),\n        ('mrp-rm-explore-match-labeler', 'openAIModerations', 'input', 0.0035183),\n        ('mrp-rm-explore-match-labeler', 'mcmurdoDenylist', 'output', 0.0016293),\n        ('mrp-rm-explore-match-labeler', 'openAIModerations', 'output', 0.0000355),\n        ('pathfinder-llm-service', 'mcmurdoDenylist', 'output', 0.0),\n      \t('pathfinder-llm-service', 'openAIModerations', 'input', 0.0036826247070639400),\n      \t('pathfinder-llm-service', 'openAIModerations', 'output', 0.0),\n        ('profile-suggestion-service', 'mcmurdoDenylist', 'output', 0.0000481), -- 0.0000217362277708256\n        ('profile-suggestion-service', 'mcmurdoDetoxify', 'output', 0.0449264), -- 0.0454137542347355\n        ('sourcing-match-explanation', 'mcmurdoDenylist', 'output', 0.000552),\n        ('sourcing-match-explanation', 'openAIModerations', 'output', 0.0001181)\n    ) AS t(appName, moderationApi, moderationType, expected_block_rate)),\n\nweekly_data AS (\n    SELECT\n        date(from_unixtime(lp.unixtime)) AS date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(CAST(lp.flagged  AS DOUBLE)) as block_count,\n        COUNT(*) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date(from_unixtime(lp.unixtime)) ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date(from_unixtime(lp.unixtime)) ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp   \n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(DATE(CURRENT_DATE) - interval '60' day) AND imhotep_unixtime(DATE(CURRENT_DATE))\n        -- see rate_change query for filters by appName etc.        \n    GROUP BY\n        date(from_unixtime(lp.unixtime)),\n        appName, \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        wd.date,\n        wd.appName, \n        wd.moderationApi, \n        wd.moderationType,\n        es.expected_block_rate as raw_expected_block_rate,\n        request_count,\n        weekly_request_count,\n        weekly_block_count,\n        block_count * 1.0 / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        (block_count + 4.5) * 1.0 / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        weekly_block_count * 1.0 /  weekly_request_count as raw_weekly_block_rate,  \n        (weekly_block_count + 4.5) * 1.0 / ( weekly_request_count + 9.0) as  weekly_block_rate, -- corrected/smoothed\n        ((es.expected_block_rate * weekly_request_count)  + 4.5) * 1.0 / ( weekly_request_count + 9.0) as  expected_block_rate, -- corrected/smoothed\n        --((block_count + 4.5) * 1.0 / (request_count + 9.0)) - ((weekly_block_count + 4.5) * 1.0 / ( weekly_request_count + 9.0)) as change,\n        sum(1.0) over (partition by wd.appName, wd.moderationApi, wd.moderationType ORDER BY date ROWS BETWEEN unbounded PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        weekly_data wd\n    JOIN \n    \texpected_stats as es\n    ON \n        wd.appName = es.appName \n        AND wd.moderationApi = es.moderationApi\n        AND wd.moderationType = es.moderationType              \n),\n\nblock_rate_estimates AS (\n    SELECT\n        date,\n        appName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n\t\tweekly_block_count,        \n--        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS stddev_obs,\n--        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n--\t\tSQRT(block_rate * (1.0 - block_rate) / request_count) as stddev_approx,        \n        SQRT(weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count) as stddev_approx_weekly,\n\t\tSTDDEV(weekly_block_rate - expected_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS stddev_obs_weekly,\n\t\texpected_block_rate,        \n\t\tweekly_block_rate,        \n        weekly_block_rate - expected_block_rate as change,\n        raw_weekly_block_rate,\n\t\traw_expected_block_rate,        \n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\n\nconfidence_interval AS (\n    SELECT\n\t\tdate,    \n        appName, \n        moderationApi, \n        moderationType,\n        expected_block_rate,\n\t\tgreatest(stddev_obs_weekly,stddev_approx_weekly) as stddev_weekly,\n\t\tstddev_obs_weekly,\n        stddev_approx_weekly,       \n        weekly_block_rate,\n        weekly_block_rate - 3.0 * greatest(stddev_obs_weekly,stddev_approx_weekly) AS weekly_block_rate_lower_bound,\n        weekly_block_rate + 3.0 * greatest(stddev_obs_weekly,stddev_approx_weekly) AS weekly_block_rate_upper_bound,       \n\t\traw_weekly_block_rate,\n\t\traw_expected_block_rate,                \n        change,\n        change/expected_block_rate as relative_change,\n        change - 3.0 * greatest(stddev_obs_weekly,stddev_approx_weekly) AS change_lower_bound,\n        change + 3.0 * greatest(stddev_obs_weekly,stddev_approx_weekly) AS change_upper_bound,\n        weekly_request_count,\n\t\tweekly_block_count,\n\t\tprevious_dates_count        \n    FROM\n        block_rate_estimates\n)\nSELECT\n\tdate,\n    appName, \n    moderationApi, \n    moderationType,\n\texpected_block_rate,\n    weekly_block_rate,\n\tweekly_block_rate_lower_bound,\n    weekly_block_rate_upper_bound,\n    raw_weekly_block_rate, \n\traw_expected_block_rate,            \n    change,\n    relative_change,\n    change_lower_bound,\n    change_upper_bound,\n    weekly_request_count,\n\tweekly_block_count, \n\tstddev_obs_weekly,\n    stddev_approx_weekly,      \n    IF(abs(change/stddev_weekly) > 3.0, 1, 0) as is_sig_change,\n    IF(abs(change/stddev_weekly) > 3.0 and (date = (max(date) over ())), 1, 0) as is_latest_date_sig_change,\n    change/stddev_approx_weekly as z_score_alternate,\n    change/stddev_weekly as z_score,\n\tprevious_dates_count    \nFROM\n    confidence_interval -- Note: r",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 5,
  "runStartToQueryComplete" : 22
}, {
  "elapsedMillis" : 5808,
  "totalScheduledMillis" : 111525,
  "cpuMillis" : 48774,
  "queuedMillis" : 2,
  "executeMillis" : 5176,
  "getResultMillis" : 0,
  "iterateMillis" : 685,
  "rows" : 475,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_3 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '')\nORDER BY date, t0, t1, t2, t3 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 6,
  "runStartToQueryComplete" : 22
}, {
  "elapsedMillis" : 5330,
  "totalScheduledMillis" : 111228,
  "cpuMillis" : 49154,
  "queuedMillis" : 1,
  "executeMillis" : 4734,
  "getResultMillis" : 0,
  "iterateMillis" : 627,
  "rows" : 1224,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_4 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '')\nORDER BY date, t0, t1, t2, t3, t4 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 7,
  "runStartToQueryComplete" : 23
}, {
  "elapsedMillis" : 6037,
  "totalScheduledMillis" : 753633,
  "cpuMillis" : 695231,
  "queuedMillis" : 1,
  "executeMillis" : 3977,
  "getResultMillis" : 0,
  "iterateMillis" : 2092,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_7 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 8,
  "runStartToQueryComplete" : 27
}, {
  "elapsedMillis" : 4138,
  "totalScheduledMillis" : 116148,
  "cpuMillis" : 52067,
  "queuedMillis" : 1,
  "executeMillis" : 3448,
  "getResultMillis" : 0,
  "iterateMillis" : 716,
  "rows" : 2777,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_6 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '')\nORDER BY date, t0, t1, t2, t3, t4, t5, t6 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 9,
  "runStartToQueryComplete" : 25
}, {
  "elapsedMillis" : 3165,
  "totalScheduledMillis" : 110460,
  "cpuMillis" : 47569,
  "queuedMillis" : 1,
  "executeMillis" : 2567,
  "getResultMillis" : 0,
  "iterateMillis" : 624,
  "rows" : 90,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        Lower(tw.tier_1) NOT IN ('global revenue', 'marketing')\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n\t(''='' OR t1 = '')\nORDER BY date, t0, t1 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 10,
  "runStartToQueryComplete" : 28
}, {
  "elapsedMillis" : 9117,
  "totalScheduledMillis" : 58104,
  "cpuMillis" : 21748,
  "queuedMillis" : 1,
  "executeMillis" : 6280,
  "getResultMillis" : 0,
  "iterateMillis" : 2880,
  "rows" : 3876,
  "error" : null,
  "scannedBytes" : 145514085,
  "query" : "WITH ec2_instances AS (\n  SELECT name, id, image_id, account_id, launched_on\n  FROM datalake.imhotep.aws_ec2\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND state='running' \n  AND (emr_role is NULL or emr_role = '') -- Don't pull in EMR. Not Centos, centrally managed mostly, and has lots of churn. \n)\n, golden_name_amis AS (\n  SELECT id, name, display_name\n  FROM datalake.imhotep.aws_ami\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND LOWER(display_name) LIKE '%golden%'\n)\n, amis AS (\n  SELECT id, name, display_name\n  FROM datalake.imhotep.aws_ami\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n)\n, aws_accounts AS (\n  SELECT \"key\" AS account_id, accountablePartyId AS teamId, accountablePartyType AS teamType\n  FROM datalake.imhotep.ownershipsnapshot\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND namespace='AWS_ACCOUNT' AND statusflagvalue='ACTIVE'\n)\n, all_hosts AS (\n\tSELECT resource_id\n    FROM aws_inspector_findings\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n)\n, ownership AS (\n  SELECT teamid, displayname, intakejira, tier_0, tier_1, tier_2, tier_3, tier_4, tier_5, tier_6, email, engineeringleadldap, productleadldap, programleadldap, technologyleadldap, usersslackchannel\n  FROM datalake.imhotep.teamwrkssnapshot \n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n)\n\nSELECT DISTINCT(e.id), e.name AS ec2_name, e.account_id, o.teamid, o.tier_0, o.tier_1, o.tier_2, o.tier_3, o.tier_4, o.tier_5, o.tier_6, ch.resource_id, amis.name AS ami_name, amis.display_name AS ami_display_name\n\t, CASE WHEN ch.resource_id IS NOT NULL AND LOWER(amis.display_name) LIKE '%golden%' THEN 1 ELSE 0 END AS is_golden\n    , 1 as count\nFROM ec2_instances e\nLEFT OUTER JOIN aws_accounts a ON e.account_id= a.account_id\nLEFT OUTER JOIN all_hosts ch ON ch.resource_id = e.id\nLEFT OUTER JOIN amis ON amis.id = e.image_id\nLEFT OUTER JOIN ownership o ON a.teamId=o.teamid\nWHERE \n\t(''='' OR o.tier_0 = '')\n    AND (''='' OR o.tier_1 = '')\n    AND (''='' OR o.tier_2 = '')\n    AND (''='' OR o.tier_3 = '')\n    AND (''='' OR o.tier_4 = '')\n    AND o.tier_6 != 'Cluster'",
  "queryTables" : [ "datalakehive.imhotep.aws_ami", "datalakehive.imhotep.aws_ec2", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.aws_inspector_findings" ],
  "queryIndex" : 11,
  "runStartToQueryComplete" : 33
}, {
  "elapsedMillis" : 4841,
  "totalScheduledMillis" : 34828,
  "cpuMillis" : 13645,
  "queuedMillis" : 1,
  "executeMillis" : 3886,
  "getResultMillis" : 0,
  "iterateMillis" : 1010,
  "rows" : 190,
  "error" : null,
  "scannedBytes" : 270934201,
  "query" : "WITH populated_day AS (\n    SELECT MIN(day) AS most_recent_day\n    FROM (\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"jiracloudissues$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"ownershipSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"teamwrksSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n    )\n)\n\n-- Count of a11y violations remediated within SLO in the last 30 days / (All a11y violations remediated within last 30 days + Open issues outside SLO)\nSELECT\n    teamId,\n\tproject,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Closed_Within_SLO,\n    ALL_REM_30 AS All_Remediated_Last_30,\n    UNREM AS Open_Outside_SLO,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7\nFROM (\n\tSELECT\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        \n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- last 30 days\n                  ((ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 1000) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  ((TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 1000) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- last 30 days\n                  (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                -- last 30 days\n                (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.day = (SELECT most_recent_day from populated_day) AND\n        ow.day = (SELECT most_recent_day from populated_day) AND\n        tw.day = (SELECT most_recent_day from populated_day) AND\n        ji.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        ow.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        tw.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        tw.tier_1 NOT IN ('Global Revenue', 'Marketing')\n    GROUP BY ow.accountablePartyId, ji.projectkey, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n        HAVING COUNT(\n        CASE WHEN\n            ji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            -- last 30 days\n            (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n            THEN 1 END\n    ) IS NOT NULL AND (\n        COUNT(\n            CASE WHEN\n                ji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                -- last 30 days\n                (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n                THEN 1 END\n        ) + COUNT(\n            CASE WHEN\n                (\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n                ) AND (\n                    (\n                        ji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                        (ji.unixtime - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400 > 30\n                    ) OR (\n                        -- SLO days to complete\n                        ji.priority NOT IN ('Blocker') AND\n                        (ji.unixtime - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) > 0\n    )\n)\nWHERE\n\t(''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY teamId, project, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.jiracloudissues$partitions", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.ownershipsnapshot$partitions", "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot$partitions" ],
  "queryIndex" : 12,
  "runStartToQueryComplete" : 29
}, {
  "elapsedMillis" : 2308,
  "totalScheduledMillis" : 1530,
  "cpuMillis" : 1081,
  "queuedMillis" : 1,
  "executeMillis" : 2258,
  "getResultMillis" : 0,
  "iterateMillis" : 82,
  "rows" : 1584,
  "error" : null,
  "scannedBytes" : 940980,
  "query" : "WITH golden_amis AS (\n    SELECT name, id, created_on, day\n    FROM datalake.imhotep.aws_ami\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND REGEXP_LIKE(name, '^indeed_(ubuntu|(dsp_ecs_)?centos_7|amzn2|AL2023).*')\n)\n, ec2 AS (\n\tSELECT name, image_id, id, account_id, launched_on\n    FROM datalake.imhotep.aws_ec2\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND state='running'\n    AND (emr_role='' OR emr_role IS NULL)\n)\nSELECT DISTINCT(e.id) AS ec2_id, e.name AS ec2_name, e.image_id, e.account_id, e.launched_on, g.name as ami_name\n\t, day(current_timestamp - from_iso8601_timestamp(e.launched_on)) as ec2_age_days\n    , day(current_timestamp - from_iso8601_timestamp(g.created_on)) as ami_age_days\n    /*\n    Cases:\n    \tlaunched_on > 90 days ago = not patched\n        launched_on < 90 days ago\n        \tcreated_on > 120 days ago = not patched -- Giving 30 day buffer given the golden AMI rebuild schedule. \n            created_on <= 120 days ago = patched \n    This does not flag instances that are not picking up the newest image on rebuild but that image is still less than 120 days old. Technically still patched, but on track to not be patched. \n    */\n    , CASE WHEN \n        day(current_timestamp - from_iso8601_timestamp(e.launched_on)) <= 90 -- Must update at least every 90 days\n        \tAND day(current_timestamp - from_iso8601_timestamp(g.created_on)) <= 120 -- If the image it built from is over 120 days old, it's not following the golden image releases\n        THEN 1\n        ELSE 0\n      END AS patched\n    , 1 AS count\nfrom ec2 e\nINNER JOIN golden_amis g\nON e.image_id=g.id\n\n",
  "queryTables" : [ "datalakehive.imhotep.aws_ami", "datalakehive.imhotep.aws_ec2" ],
  "queryIndex" : 13,
  "runStartToQueryComplete" : 31
}, {
  "elapsedMillis" : 5997,
  "totalScheduledMillis" : 1307684,
  "cpuMillis" : 952013,
  "queuedMillis" : 1,
  "executeMillis" : 2459,
  "getResultMillis" : 0,
  "iterateMillis" : 3567,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '')\nORDER BY date, teamId, project, t0, t1 ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 14,
  "runStartToQueryComplete" : 35
}, {
  "elapsedMillis" : 6485,
  "totalScheduledMillis" : 1301512,
  "cpuMillis" : 957475,
  "queuedMillis" : 1,
  "executeMillis" : 2495,
  "getResultMillis" : 0,
  "iterateMillis" : 4029,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '')\nORDER BY date, teamId, project, t0, t1, t2 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 15,
  "runStartToQueryComplete" : 36
}, {
  "elapsedMillis" : 5504,
  "totalScheduledMillis" : 54567,
  "cpuMillis" : 20302,
  "queuedMillis" : 1,
  "executeMillis" : 4477,
  "getResultMillis" : 0,
  "iterateMillis" : 1082,
  "rows" : 3876,
  "error" : null,
  "scannedBytes" : 145514085,
  "query" : "WITH ec2_instances AS (\n  SELECT name, id, image_id, account_id, launched_on\n  FROM datalake.imhotep.aws_ec2\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND state='running' \n  AND (emr_role is NULL or emr_role = '') -- Don't pull in EMR. Not Centos, centrally managed mostly, and has lots of churn. \n)\n, golden_name_amis AS (\n  SELECT id, name, display_name\n  FROM datalake.imhotep.aws_ami\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND LOWER(display_name) LIKE '%golden%'\n)\n, amis AS (\n  SELECT id, name, display_name\n  FROM datalake.imhotep.aws_ami\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n)\n, aws_accounts AS (\n  SELECT \"key\" AS account_id, accountablePartyId AS teamId, accountablePartyType AS teamType\n  FROM datalake.imhotep.ownershipsnapshot\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND namespace='AWS_ACCOUNT' AND statusflagvalue='ACTIVE'\n)\n, all_hosts AS (\n\tSELECT resource_id\n    FROM aws_inspector_findings\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n)\n, ownership AS (\n  SELECT teamid, displayname, intakejira, tier_0, tier_1, tier_2, tier_3, tier_4, tier_5, tier_6, email, engineeringleadldap, productleadldap, programleadldap, technologyleadldap, usersslackchannel\n  FROM datalake.imhotep.teamwrkssnapshot \n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n)\n\nSELECT DISTINCT(e.id), e.name AS ec2_name, e.account_id, o.teamid, o.tier_0, o.tier_1, o.tier_2, o.tier_3, o.tier_4, o.tier_5, o.tier_6, ch.resource_id, amis.name AS ami_name, amis.display_name AS ami_display_name\n\t, CASE WHEN ch.resource_id IS NOT NULL AND LOWER(amis.display_name) LIKE '%golden%' THEN 1 ELSE 0 END AS is_golden\n    , 1 as count\nFROM ec2_instances e\nLEFT OUTER JOIN aws_accounts a ON e.account_id= a.account_id\nLEFT OUTER JOIN all_hosts ch ON ch.resource_id = e.id\nLEFT OUTER JOIN amis ON amis.id = e.image_id\nLEFT OUTER JOIN ownership o ON a.teamId=o.teamid\nWHERE \n\t(''='' OR o.tier_0 = '')\n    AND (''='' OR o.tier_1 = '')\n    AND (''='' OR o.tier_2 = '')\n    AND (''='' OR o.tier_3 = '')\n    AND (''='' OR o.tier_4 = '')\n    AND o.tier_6 != 'Cluster'",
  "queryTables" : [ "datalakehive.imhotep.aws_ami", "datalakehive.imhotep.aws_ec2", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.aws_inspector_findings" ],
  "queryIndex" : 16,
  "runStartToQueryComplete" : 35
}, {
  "elapsedMillis" : 6310,
  "totalScheduledMillis" : 26339650,
  "cpuMillis" : 3750357,
  "queuedMillis" : 1,
  "executeMillis" : 1128,
  "getResultMillis" : 0,
  "iterateMillis" : 5206,
  "rows" : 988,
  "error" : null,
  "scannedBytes" : 151577517567,
  "query" : "-- Calculate the block rate change for each day compared to a rolling average of the previous 7 (over a total window of 30 days)\nWITH daily_data AS (\n    SELECT\n        date(from_unixtime(lp.unixtime)) AS date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(CAST(lp.flagged  AS DOUBLE)) as block_count,\n        COUNT(*) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date(from_unixtime(lp.unixtime)) ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date(from_unixtime(lp.unixtime)) ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(DATE(CURRENT_DATE) - interval '60' day) AND imhotep_unixtime(DATE(CURRENT_DATE))\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date(from_unixtime(lp.unixtime)),\n        appName, \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        appName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        block_count * 1.0 / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        (block_count + 4.5) * 1.0 / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        weekly_block_count * 1.0 /  weekly_request_count as raw_weekly_block_rate,  \n        (weekly_block_count + 4.5) * 1.0 / ( weekly_request_count + 9.0) as  weekly_block_rate, -- corrected/smoothed\n        ((block_count + 4.5) * 1.0 / (request_count + 9.0)) - ((weekly_block_count + 4.5) * 1.0 / ( weekly_request_count + 9.0)) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        appName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        block_rate - weekly_block_rate as change,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        appName, \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 3.0 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 3.0 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        request_count,\n        change - 3.0 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 3.0 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count        \n    FROM\n        block_rate_change\n    WHERE\n    \tweekly_request_count > 100\n        and request_count > 100\n)\nSELECT\n    date,\n    appName, \n    moderationApi, \n    moderationType,\n    request_count,\n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change_lower_bound,\n    change_upper_bound,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 6), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n     -- see rate_change query for filters by appName etc.    \nORDER BY\n    date DESC, appName ASC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 17,
  "runStartToQueryComplete" : 36
}, {
  "elapsedMillis" : 5093,
  "totalScheduledMillis" : 22630784,
  "cpuMillis" : 3627095,
  "queuedMillis" : 1,
  "executeMillis" : 1164,
  "getResultMillis" : 0,
  "iterateMillis" : 3947,
  "rows" : 988,
  "error" : null,
  "scannedBytes" : 151579474482,
  "query" : "-- Calculate the block rate change for each day compared to a rolling average of the previous 7 (over a total window of 30 days)\nWITH daily_data AS (\n    SELECT\n        date(from_unixtime(lp.unixtime)) AS date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(CAST(lp.flagged  AS DOUBLE)) as block_count,\n        COUNT(*) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date(from_unixtime(lp.unixtime)) ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date(from_unixtime(lp.unixtime)) ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(DATE(CURRENT_DATE) - interval '60' day) AND imhotep_unixtime(DATE(CURRENT_DATE))\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date(from_unixtime(lp.unixtime)),\n        appName, \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        appName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        block_count * 1.0 / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        (block_count + 4.5) * 1.0 / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        weekly_block_count * 1.0 /  weekly_request_count as raw_weekly_block_rate,  \n        (weekly_block_count + 4.5) * 1.0 / ( weekly_request_count + 9.0) as  weekly_block_rate, -- corrected/smoothed\n        ((block_count + 4.5) * 1.0 / (request_count + 9.0)) - ((weekly_block_count + 4.5) * 1.0 / ( weekly_request_count + 9.0)) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        appName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 7 PRECEDING AND 1 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        block_rate - weekly_block_rate as change,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        appName, \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 3.0 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 3.0 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        request_count,\n        change - 3.0 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 3.0 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count        \n    FROM\n        block_rate_change\n    WHERE\n    \tweekly_request_count > 100\n        and request_count > 100\n)\nSELECT\n    date,\n    appName, \n    moderationApi, \n    moderationType,\n    request_count,\n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change_lower_bound,\n    change_upper_bound,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 6), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n     -- see rate_change query for filters by appName etc.    \nORDER BY\n    date DESC, appName ASC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 18,
  "runStartToQueryComplete" : 38
}, {
  "elapsedMillis" : 4135,
  "totalScheduledMillis" : 113890,
  "cpuMillis" : 51689,
  "queuedMillis" : 1,
  "executeMillis" : 2625,
  "getResultMillis" : 0,
  "iterateMillis" : 1555,
  "rows" : 240,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing')\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '')\nORDER BY date, t0, t1, t2 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 19,
  "runStartToQueryComplete" : 38
}, {
  "elapsedMillis" : 4656,
  "totalScheduledMillis" : 710784,
  "cpuMillis" : 648752,
  "queuedMillis" : 1,
  "executeMillis" : 2495,
  "getResultMillis" : 0,
  "iterateMillis" : 2208,
  "rows" : 2407,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        Lower(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0\n)\nWHERE\n\t(t0 = 'Indeed')\nORDER BY date, teamId, project, t0 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 20,
  "runStartToQueryComplete" : 40
}, {
  "elapsedMillis" : 4736,
  "totalScheduledMillis" : 772082,
  "cpuMillis" : 712011,
  "queuedMillis" : 0,
  "executeMillis" : 2497,
  "getResultMillis" : 0,
  "iterateMillis" : 2261,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_5 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 21,
  "runStartToQueryComplete" : 46
}, {
  "elapsedMillis" : 3268,
  "totalScheduledMillis" : 117060,
  "cpuMillis" : 50571,
  "queuedMillis" : 1,
  "executeMillis" : 2507,
  "getResultMillis" : 0,
  "iterateMillis" : 777,
  "rows" : 3029,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_7 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '') \nORDER BY date, t0, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 22,
  "runStartToQueryComplete" : 45
}, {
  "elapsedMillis" : 3271,
  "totalScheduledMillis" : 120561,
  "cpuMillis" : 60081,
  "queuedMillis" : 1,
  "executeMillis" : 2663,
  "getResultMillis" : 0,
  "iterateMillis" : 641,
  "rows" : 2116,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_5 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '')\nORDER BY date, t0, t1, t2, t3, t4, t5 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 23,
  "runStartToQueryComplete" : 49
}, {
  "elapsedMillis" : 1852,
  "totalScheduledMillis" : 1460,
  "cpuMillis" : 1133,
  "queuedMillis" : 0,
  "executeMillis" : 1839,
  "getResultMillis" : 0,
  "iterateMillis" : 46,
  "rows" : 1584,
  "error" : null,
  "scannedBytes" : 940980,
  "query" : "WITH golden_amis AS (\n    SELECT name, id, created_on, day\n    FROM datalake.imhotep.aws_ami\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND REGEXP_LIKE(name, '^indeed_(ubuntu|(dsp_ecs_)?centos_7|amzn2|AL2023).*')\n)\n, ec2 AS (\n\tSELECT name, image_id, id, account_id, launched_on\n    FROM datalake.imhotep.aws_ec2\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND state='running'\n    AND (emr_role='' OR emr_role IS NULL)\n)\nSELECT DISTINCT(e.id) AS ec2_id, e.name AS ec2_name, e.image_id, e.account_id, e.launched_on, g.name as ami_name\n\t, day(current_timestamp - from_iso8601_timestamp(e.launched_on)) as ec2_age_days\n    , day(current_timestamp - from_iso8601_timestamp(g.created_on)) as ami_age_days\n    /*\n    Cases:\n    \tlaunched_on > 90 days ago = not patched\n        launched_on < 90 days ago\n        \tcreated_on > 120 days ago = not patched -- Giving 30 day buffer given the golden AMI rebuild schedule. \n            created_on <= 120 days ago = patched \n    This does not flag instances that are not picking up the newest image on rebuild but that image is still less than 120 days old. Technically still patched, but on track to not be patched. \n    */\n    , CASE WHEN \n        day(current_timestamp - from_iso8601_timestamp(e.launched_on)) <= 90 -- Must update at least every 90 days\n        \tAND day(current_timestamp - from_iso8601_timestamp(g.created_on)) <= 120 -- If the image it built from is over 120 days old, it's not following the golden image releases\n        THEN 1\n        ELSE 0\n      END AS patched\n    , 1 AS count\nfrom ec2 e\nINNER JOIN golden_amis g\nON e.image_id=g.id\n\n",
  "queryTables" : [ "datalakehive.imhotep.aws_ami", "datalakehive.imhotep.aws_ec2" ],
  "queryIndex" : 24,
  "runStartToQueryComplete" : 47
}, {
  "elapsedMillis" : 4630,
  "totalScheduledMillis" : 792337,
  "cpuMillis" : 734915,
  "queuedMillis" : 1,
  "executeMillis" : 2493,
  "getResultMillis" : 0,
  "iterateMillis" : 2168,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_6 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5, t6 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 25,
  "runStartToQueryComplete" : 52
}, {
  "elapsedMillis" : 5970,
  "totalScheduledMillis" : 40192,
  "cpuMillis" : 21061,
  "queuedMillis" : 1,
  "executeMillis" : 5171,
  "getResultMillis" : 0,
  "iterateMillis" : 830,
  "rows" : 3876,
  "error" : null,
  "scannedBytes" : 145514085,
  "query" : "WITH ec2_instances AS (\n  SELECT name, id, image_id, account_id, launched_on\n  FROM datalake.imhotep.aws_ec2\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND state='running' \n  AND (emr_role is NULL or emr_role = '') -- Don't pull in EMR. Not Centos, centrally managed mostly, and has lots of churn. \n)\n, golden_name_amis AS (\n  SELECT id, name, display_name\n  FROM datalake.imhotep.aws_ami\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND LOWER(display_name) LIKE '%golden%'\n)\n, amis AS (\n  SELECT id, name, display_name\n  FROM datalake.imhotep.aws_ami\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n)\n, aws_accounts AS (\n  SELECT \"key\" AS account_id, accountablePartyId AS teamId, accountablePartyType AS teamType\n  FROM datalake.imhotep.ownershipsnapshot\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND namespace='AWS_ACCOUNT' AND statusflagvalue='ACTIVE'\n)\n, all_hosts AS (\n\tSELECT resource_id\n    FROM aws_inspector_findings\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n)\n, ownership AS (\n  SELECT teamid, displayname, intakejira, tier_0, tier_1, tier_2, tier_3, tier_4, tier_5, tier_6, email, engineeringleadldap, productleadldap, programleadldap, technologyleadldap, usersslackchannel\n  FROM datalake.imhotep.teamwrkssnapshot \n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n)\n\nSELECT DISTINCT(e.id), e.name AS ec2_name, e.account_id, o.teamid, o.tier_0, o.tier_1, o.tier_2, o.tier_3, o.tier_4, o.tier_5, o.tier_6, ch.resource_id, amis.name AS ami_name, amis.display_name AS ami_display_name\n\t, CASE WHEN ch.resource_id IS NOT NULL AND LOWER(amis.display_name) LIKE '%golden%' THEN 1 ELSE 0 END AS is_golden\n    , 1 as count\nFROM ec2_instances e\nLEFT OUTER JOIN aws_accounts a ON e.account_id= a.account_id\nLEFT OUTER JOIN all_hosts ch ON ch.resource_id = e.id\nLEFT OUTER JOIN amis ON amis.id = e.image_id\nLEFT OUTER JOIN ownership o ON a.teamId=o.teamid\nWHERE \n\t(''='' OR o.tier_0 = '')\n    AND (''='' OR o.tier_1 = '')\n    AND (''='' OR o.tier_2 = '')\n    AND (''='' OR o.tier_3 = '')\n    AND (''='' OR o.tier_4 = '')\n    AND o.tier_6 != 'Cluster'",
  "queryTables" : [ "datalakehive.imhotep.aws_ami", "datalakehive.imhotep.aws_ec2", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.aws_inspector_findings" ],
  "queryIndex" : 26,
  "runStartToQueryComplete" : 54
}, {
  "elapsedMillis" : 3100,
  "totalScheduledMillis" : 118220,
  "cpuMillis" : 57478,
  "queuedMillis" : 1,
  "executeMillis" : 2458,
  "getResultMillis" : 0,
  "iterateMillis" : 677,
  "rows" : 30,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        ji.issuetype = 'Bug'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d')\n)\n\nSELECT\n\tdate,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nORDER BY date ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 27,
  "runStartToQueryComplete" : 54
}, {
  "elapsedMillis" : 4149,
  "totalScheduledMillis" : 5906,
  "cpuMillis" : 3465,
  "queuedMillis" : 1,
  "executeMillis" : 3368,
  "getResultMillis" : 0,
  "iterateMillis" : 821,
  "rows" : 67,
  "error" : null,
  "scannedBytes" : 81755945,
  "query" : "WITH\n\npopulated_day AS (\n    SELECT MIN(day) AS most_recent_day\n    FROM (\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"jiracloudissues$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"ownershipSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"teamwrksSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n    )\n)\n\n-- Count of a11y violations remediated within SLO in the last 30 days / (All a11y violations remediated within last 30 days + Open issues)\nSELECT\n    inslo,\n    DATE_FORMAT(duedate, '%Y-%m-%d') AS duedate,\n    teamId,\n\t-- project,\n    issuekey,\n    -- issuetype,\n    priority,\n    status,\n    summary,\n    lastupdated,\n    DATE_FORMAT(createdate, '%Y-%m-%d') AS createdate,\n    labels,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7\nFROM (\n\tSELECT\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        ji.issuekey AS issuekey,\n        ji.issuetype AS issuetype,\n        ji.priority AS priority,\n        ji.summary AS summary,\n        ji.status AS status,\n        DATE_FORMAT(FROM_UNIXTIME(CAST(ji.lastupdated AS BIGINT) / 1000), '%Y-%m-%d') AS lastupdated,\n\t\tDATE_TRUNC('day', DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')) AS createdate,\n        DATE_TRUNC('day', DATE_ADD('day',\n        \tCASE \n                WHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        )) AS duedate,\n\t\tCAST(DATE_ADD('day',\n            CASE \n                WHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        ) > CURRENT_DATE AS BOOLEAN) AS inslo,\n        ji.labels AS labels,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.day = (SELECT most_recent_day from populated_day) AND\n        ow.day = (SELECT most_recent_day from populated_day) AND\n        tw.day = (SELECT most_recent_day from populated_day) AND\n        ji.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        ow.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        tw.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            -- The line below will include \"Pending Closure\" tickets in the resulting table\n            -- OR ji.resolutiontimestamp = 0\n        )\n    GROUP BY\n    \tow.accountablePartyId,\n    \tji.projectkey,\n        ji.issuekey,\n\t\tji.issuetype,\n        ji.priority,\n        ji.summary,\n        ji.status,\n        DATE_FORMAT(FROM_UNIXTIME(CAST(ji.lastupdated AS BIGINT) / 1000), '%Y-%m-%d'),\n\t\tDATE_TRUNC('day', DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')),\n        DATE_TRUNC('day', DATE_ADD('day',\n            CASE \n                WHEN ji.priority = 'Blocker' THEN 30\n                ELSE 90 \n           \tEND, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        )),\n        CAST(DATE_ADD('day',\n            CASE \n                WHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        ) > CURRENT_DATE AS BOOLEAN),\n        ji.labels,\n        tw.tier_0,\n        tw.tier_1,\n        tw.tier_2,\n        tw.tier_3,\n        tw.tier_4,\n        tw.tier_5,\n        tw.tier_6,\n        tw.tier_7\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Business Technology & Trust'='' OR t1 = 'Business Technology & Trust') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY \n\tinslo ASC,\n\tduedate ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.jiracloudissues$partitions", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.ownershipsnapshot$partitions", "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot$partitions" ],
  "queryIndex" : 28,
  "runStartToQueryComplete" : 64
}, {
  "elapsedMillis" : 2832,
  "totalScheduledMillis" : 2750629,
  "cpuMillis" : 166800,
  "queuedMillis" : 1,
  "executeMillis" : 1095,
  "getResultMillis" : 0,
  "iterateMillis" : 1757,
  "rows" : 555,
  "error" : null,
  "scannedBytes" : 10159490313,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('hour', from_unixtime(lp.unixtime)) as date,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-30')\n        and True\n    GROUP BY\n        date_trunc('hour', from_unixtime(lp.unixtime)),\n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 6 as double) / cast(1 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1))) as change,\n        sum(1.0) over (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count             \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n\t\traw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n         \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \nchange,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,        \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 6), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 1 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 6)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 29,
  "runStartToQueryComplete" : 74
}, {
  "elapsedMillis" : 2417,
  "totalScheduledMillis" : 1424585,
  "cpuMillis" : 241549,
  "queuedMillis" : 1,
  "executeMillis" : 1251,
  "getResultMillis" : 0,
  "iterateMillis" : 1181,
  "rows" : 2425,
  "error" : null,
  "scannedBytes" : 10523618296,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('hour', from_unixtime(lp.unixtime)) as date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-30')\n        and True\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date_trunc('hour', from_unixtime(lp.unixtime)),\n\t\tappName,         \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n\t\tappName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 6 as double) / cast(1 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1))) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n\tappName,     \n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,\n    \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 6), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 1 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 6)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 30,
  "runStartToQueryComplete" : 75
}, {
  "elapsedMillis" : 1761,
  "totalScheduledMillis" : 1547,
  "cpuMillis" : 1191,
  "queuedMillis" : 1,
  "executeMillis" : 1734,
  "getResultMillis" : 0,
  "iterateMillis" : 63,
  "rows" : 1584,
  "error" : null,
  "scannedBytes" : 940980,
  "query" : "WITH golden_amis AS (\n    SELECT name, id, created_on, day\n    FROM datalake.imhotep.aws_ami\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND REGEXP_LIKE(name, '^indeed_(ubuntu|(dsp_ecs_)?centos_7|amzn2|AL2023).*')\n)\n, ec2 AS (\n\tSELECT name, image_id, id, account_id, launched_on\n    FROM datalake.imhotep.aws_ec2\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND state='running'\n    AND (emr_role='' OR emr_role IS NULL)\n)\nSELECT DISTINCT(e.id) AS ec2_id, e.name AS ec2_name, e.image_id, e.account_id, e.launched_on, g.name as ami_name\n\t, day(current_timestamp - from_iso8601_timestamp(e.launched_on)) as ec2_age_days\n    , day(current_timestamp - from_iso8601_timestamp(g.created_on)) as ami_age_days\n    /*\n    Cases:\n    \tlaunched_on > 90 days ago = not patched\n        launched_on < 90 days ago\n        \tcreated_on > 120 days ago = not patched -- Giving 30 day buffer given the golden AMI rebuild schedule. \n            created_on <= 120 days ago = patched \n    This does not flag instances that are not picking up the newest image on rebuild but that image is still less than 120 days old. Technically still patched, but on track to not be patched. \n    */\n    , CASE WHEN \n        day(current_timestamp - from_iso8601_timestamp(e.launched_on)) <= 90 -- Must update at least every 90 days\n        \tAND day(current_timestamp - from_iso8601_timestamp(g.created_on)) <= 120 -- If the image it built from is over 120 days old, it's not following the golden image releases\n        THEN 1\n        ELSE 0\n      END AS patched\n    , 1 AS count\nfrom ec2 e\nINNER JOIN golden_amis g\nON e.image_id=g.id\n\n",
  "queryTables" : [ "datalakehive.imhotep.aws_ami", "datalakehive.imhotep.aws_ec2" ],
  "queryIndex" : 31,
  "runStartToQueryComplete" : 75
}, {
  "elapsedMillis" : 6291,
  "totalScheduledMillis" : 35776,
  "cpuMillis" : 19603,
  "queuedMillis" : 1,
  "executeMillis" : 5601,
  "getResultMillis" : 0,
  "iterateMillis" : 721,
  "rows" : 3876,
  "error" : null,
  "scannedBytes" : 145514085,
  "query" : "WITH ec2_instances AS (\n  SELECT name, id, image_id, account_id, launched_on\n  FROM datalake.imhotep.aws_ec2\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND state='running' \n  AND (emr_role is NULL or emr_role = '') -- Don't pull in EMR. Not Centos, centrally managed mostly, and has lots of churn. \n)\n, golden_name_amis AS (\n  SELECT id, name, display_name\n  FROM datalake.imhotep.aws_ami\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND LOWER(display_name) LIKE '%golden%'\n)\n, amis AS (\n  SELECT id, name, display_name\n  FROM datalake.imhotep.aws_ami\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n)\n, aws_accounts AS (\n  SELECT \"key\" AS account_id, accountablePartyId AS teamId, accountablePartyType AS teamType\n  FROM datalake.imhotep.ownershipsnapshot\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND namespace='AWS_ACCOUNT' AND statusflagvalue='ACTIVE'\n)\n, all_hosts AS (\n\tSELECT resource_id\n    FROM aws_inspector_findings\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n)\n, ownership AS (\n  SELECT teamid, displayname, intakejira, tier_0, tier_1, tier_2, tier_3, tier_4, tier_5, tier_6, email, engineeringleadldap, productleadldap, programleadldap, technologyleadldap, usersslackchannel\n  FROM datalake.imhotep.teamwrkssnapshot \n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n)\n\nSELECT DISTINCT(e.id), e.name AS ec2_name, e.account_id, o.teamid, o.tier_0, o.tier_1, o.tier_2, o.tier_3, o.tier_4, o.tier_5, o.tier_6, ch.resource_id, amis.name AS ami_name, amis.display_name AS ami_display_name\n\t, CASE WHEN ch.resource_id IS NOT NULL AND LOWER(amis.display_name) LIKE '%golden%' THEN 1 ELSE 0 END AS is_golden\n    , 1 as count\nFROM ec2_instances e\nLEFT OUTER JOIN aws_accounts a ON e.account_id= a.account_id\nLEFT OUTER JOIN all_hosts ch ON ch.resource_id = e.id\nLEFT OUTER JOIN amis ON amis.id = e.image_id\nLEFT OUTER JOIN ownership o ON a.teamId=o.teamid\nWHERE \n\t(''='' OR o.tier_0 = '')\n    AND (''='' OR o.tier_1 = '')\n    AND (''='' OR o.tier_2 = '')\n    AND (''='' OR o.tier_3 = '')\n    AND (''='' OR o.tier_4 = '')\n    AND o.tier_6 != 'Cluster'",
  "queryTables" : [ "datalakehive.imhotep.aws_ami", "datalakehive.imhotep.aws_ec2", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.aws_inspector_findings" ],
  "queryIndex" : 32,
  "runStartToQueryComplete" : 83
}, {
  "elapsedMillis" : 9288,
  "totalScheduledMillis" : 3450692,
  "cpuMillis" : 256660,
  "queuedMillis" : 0,
  "executeMillis" : 7757,
  "getResultMillis" : 0,
  "iterateMillis" : 1560,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 6967024883,
  "query" : "SELECT \n\tCOUNT(*) as total_profile_views, \n   \tSUM(r.sent) as outreaches,\n    SUM(r.resp_pos) as resp_pos,\n    COUNT(js.account_id) as profile_with_linkedIn,\n    ROUND(CAST(SUM(r.sent) AS DOUBLE) * 100 / COUNT(*), 1) as outreach_rate\nFROM \n\tdatalake.imhotep.rsresumeview v\n\nLEFT JOIN\n\tdatalake.imhotep.rscontacts r\nON\n\tv.recruiterAccountId = r.senderAccountId\n    AND v.resumeAccountId = r.recipientAccountId\n    AND r.unixtime BETWEEN IMHOTEP_UNIXTIME('5d') AND IMHOTEP_UNIXTIME('4d')\n\nLEFT JOIN \n\tdatalake.imhotep.js_fact_store_snapshot_prod js\nON\n\tv.resumeAccountId = js.account_id\n\tAND js.resume_links_size > 0\n\tAND CARDINALITY(FILTER(js.resume_links_array, x -> x LIKE '%linkedin%')) > 0\n    AND js.unixtime BETWEEN IMHOTEP_UNIXTIME('5d') AND IMHOTEP_UNIXTIME('4d')\n\nWHERE \n\tv.unixtime BETWEEN IMHOTEP_UNIXTIME('5d') AND IMHOTEP_UNIXTIME('4d')\n    AND js.account_id IS NOT NULL -- with LinkedIn link\n--    AND js.account_id IS NULL -- exclude linkedIn users\n--    AND r.batchSize = 1",
  "queryTables" : [ "datalakehive.imhotep.js_fact_store_snapshot_prod", "datalakehive.imhotep.rscontacts", "datalakehive.imhotep.rsresumeview" ],
  "queryIndex" : 33,
  "runStartToQueryComplete" : 97
}, {
  "elapsedMillis" : 68820,
  "totalScheduledMillis" : 97794805,
  "cpuMillis" : 24011014,
  "queuedMillis" : 0,
  "executeMillis" : 42643,
  "getResultMillis" : 0,
  "iterateMillis" : 26219,
  "rows" : 1817,
  "error" : null,
  "scannedBytes" : 423961841285,
  "query" : "with rules as (SELECT distinct rule_id\nFROM datalake.imhotep.waldorulesnapshot\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') \nAND CONTAINS(rule_tags, 'cat_risk') AND type=0 AND visibility='nowhere'),\n\nmatches as (SELECT distinct jobid, ruleid\nFROM datalake.imhotep.waldorulematch\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')),\n\nsj as (SELECT distinct jobid, 1 as nowhere\nFROM datalake.imhotep.searchablejobs\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nAND waldovisibilitylevel='nowhere' AND feedid!=50461),\n\nfiji as (select distinct jobid, feedSourceId, feedid, sourceid\nfrom datalake.imhotep.fijievents \nwhere unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nAND event='jobScanned'),\n\nfraudulent_jobs as (select matches.jobid, 1 as fraud\nfrom matches\ninner join rules\nON matches.ruleid=rules.rule_id\ninner join sj\nON matches.jobid = sj.jobid\nwhere sj.nowhere=1)\n\nselect feedSourceId, CAST(sourceid as VARCHAR) as sourceid, CAST(feedid as VARCHAR) as feedid, count() as numJobs, sum(fraud) as numFrauds, sum(fraud) * 100.000/count() as fraud_percent\nfrom (select fiji.*, coalesce(fraud, 0) as fraud\n      from fiji\n      left join fraudulent_jobs\n      on fiji.jobid=fraudulent_jobs.jobid)\ngroup by feedSourceId, sourceid, feedid\nHAVING SUM(fraud) * 100.000 / COUNT(*) > 0\nORDER BY fraud_percent, numJobs DESC\n",
  "queryTables" : [ "datalakehive.imhotep.fijievents", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.waldorulematch", "datalakehive.imhotep.waldorulesnapshot" ],
  "queryIndex" : 34,
  "runStartToQueryComplete" : 167
}, {
  "elapsedMillis" : 121590,
  "totalScheduledMillis" : 145194447,
  "cpuMillis" : 29132073,
  "queuedMillis" : 1,
  "executeMillis" : 12749,
  "getResultMillis" : 0,
  "iterateMillis" : 108926,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 612705707914,
  "query" : "WITH ratg_advids AS (\n    -- get all advertisers in our test and control groups\n    SELECT advid as advertiser_id,\n        MAX(CASE WHEN CARDINALITY(FILTER(recent_advertiser_test_groups, x -> x LIKE '%freevalue_so_job_copies_gb1'))>0\n             THEN 'Test' ELSE 'Control' END) as experiment_group\n    FROM datalake.imhotep.recent_advertiser_test_groups\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('3d') AND IMHOTEP_UNIXTIME('today')\n        AND CARDINALITY(FILTER(recent_advertiser_test_groups, x -> x LIKE '%B%:freevalue_so_job_copies_gb%'))>0\n    GROUP BY 1\n)\n\n, advids AS (\n    -- get all advertisers in our test and control groups\n    SELECT experiment_group, advertiser_id\n    FROM ratg_advids\n    JOIN datalake.imhotep.daily_employer2 de2 USING (advertiser_id)\n    WHERE de2.unixtime BETWEEN IMHOTEP_UNIXTIME('3d') AND IMHOTEP_UNIXTIME('2d')\n        AND de2.billing_country = 'GB'\n        AND de2.type != 'Test'\n)\n\n, jobs_raw AS (\n\t-- get all of the live jobs for our advertisers\n\tSELECT experiment_group\n\t     , advertiser_id\n\t     , jam.job_hash\n\t     , job_city\n\t     , job_state as job_admin_1_code\n\t     , (is_job_searchable+impressions+clicks+apply_starts)>0 as is_live\n\t     , unixtime\n    FROM datalake.imhotep.jobactivitymetrics as jam\n    JOIN advids USING (advertiser_id)\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-23') AND IMHOTEP_UNIXTIME('today')\n        AND job_country_code='GB'\n        AND job_city IS NOT NULL\n        AND job_state IS NOT NULL\n        AND CARDINALITY(FILTER(ratgs_lite, x -> x LIKE '%B%:freevalue_so_job_copies_gb%'))>0\n)\n\n, jobs_raw_w_dj2 AS (\n    -- reduce to only jobs that were created or updated during the test window\n    SELECT jam.experiment_group\n\t     , jam.advertiser_id\n\t     , jam.job_hash\n\t     , jam.job_city\n\t     , jam.job_admin_1_code\n\t     , jam.is_live\n\t     , jam.unixtime\n    FROM jobs_raw jam\n    JOIN  datalake.imhotep.dradis_job2 as dj2 ON (dj2.job_hash_underscore = jam.job_hash)\n    WHERE dj2.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-23') AND IMHOTEP_UNIXTIME('today')\n        AND (dj2.date_created>=20240923\n            OR (previous_status IN ('PAUSED','DELETED') AND status='ACTIVE'))\n)\n\n, ordered_jobs AS (\n    SELECT experiment_group\n\t     , advertiser_id\n\t     , job_hash\n         , job_city\n\t     , job_admin_1_code\n         , unixtime\n\t     , is_live\n\t     , LAG(is_live) OVER (PARTITION BY job_hash, experiment_group ORDER BY unixtime) AS prev_is_live\n\t     , LAG(unixtime) OVER (PARTITION BY job_hash, experiment_group ORDER BY unixtime) AS prev_unixtime\n    FROM jobs_raw_w_dj2\n)\n\n, jobs_w_live_periods AS (\n    SELECT experiment_group\n\t     , advertiser_id\n\t     , job_hash\n         , job_city\n\t     , job_admin_1_code\n         , unixtime as live_start_date\n         , LEAD(unixtime, 1, TO_UNIXTIME(CURRENT_DATE)) OVER (PARTITION BY job_hash, experiment_group ORDER BY unixtime) AS live_end_date\n    FROM ordered_jobs\n    WHERE is_live = true\n        AND (prev_is_live IS NULL OR prev_is_live = false)\n)\n\n, jobs_w_live_periods_copy_attrs AS (\n    -- get the fields we need for copy comparison\n    SELECT experiment_group\n         , jca.advertiser_id\n         , job_hash\n         , job_on_indeed_unixtime\n         , jca.job_city\n         , jca.job_admin_1_code\n         , job_type_id\n         , job_normalized_title_sim_hash\n         , job_description_sim_hash\n         , taxo_occupations_most_specific_suid\n         , job_occupations\n         , jwlp.live_start_date\n         , jwlp.live_end_date\n    FROM jobs_w_live_periods jwlp\n    JOIN datalake.imhotep.job_copy_attributes as jca USING (job_hash)\n    WHERE jca.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-23') AND IMHOTEP_UNIXTIME('today')\n    AND jca.job_admin_1_code IS NOT NULL\n    AND jca.job_city IS NOT NULL\n)\n\n, live_jobs_w_copy_pair AS (\n    SELECT orig.experiment_group\n        , orig.advertiser_id as advertiser_id\n        , orig.job_hash as job_hash\n        , copy.job_hash as copy_job_hash\n        , orig.job_city\n        , orig.job_admin_1_code\n        , orig.live_start_date as orig_live_start_date\n        , orig.live_end_date as orig_live_end_date\n        , copy.live_start_date as copy_live_start_date\n        , copy.live_end_date as copy_live_end_date\n        , GREATEST(orig.live_start_date, copy.live_start_date) AS overlap_start\n        , LEAST(orig.live_end_date, copy.live_end_date) AS overlap_end\n        , orig.live_start_date <= copy.live_end_date AND copy.live_start_date <= orig.live_end_date as has_overlap -- original and copy were live at the same time\n         -- split the 128-bit simhash into two 64-bit representations\n        , cast( from_big_endian_64(lpad(substr(from_base64(orig.job_description_sim_hash), 1, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as orig_high\n        , cast( from_big_endian_64(lpad(substr(from_base64(orig.job_description_sim_hash), 9, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as orig_low\n        , cast( from_big_endian_64(lpad(substr(from_base64(copy.job_description_sim_hash), 1, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as copy_high\n        , cast( from_big_endian_64(lpad(substr(from_base64(copy.job_description_sim_hash), 9, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as copy_low\n    FROM jobs_w_live_periods_copy_attrs orig\n    LEFT JOIN jobs_w_live_periods_copy_attrs copy ON (\n        orig.experiment_group = copy.experiment_group\n        AND orig.advertiser_id = copy.advertiser_id\n        AND orig.job_city = copy.job_city\n        AND orig.job_admin_1_code = copy.job_admin_1_code\n        AND orig.job_hash != copy.job_hash\n        AND orig.job_on_indeed_unixtime < copy.job_on_indeed_unixtime -- original must be created before the copy\n\n        AND bitwise_or(orig.job_type_id, copy.job_type_id) > 0 -- job_type_id any match\n\n        -- compare normalized titles when they are both not null. otherwise, ignore them\n        AND CASE WHEN orig.job_normalized_title_sim_hash IS NOT NULL AND copy.job_normalized_title_sim_hash IS NOT NULL THEN orig.job_normalized_title_sim_hash = copy.job_normalized_title_sim_hash ELSE TRUE END\n\n        -- compare occupation when they are both not null. otherwise, ignore them\n        AND CASE WHEN orig.taxo_occupations_most_specific_suid IS NOT NULL AND CARDINALITY(copy.job_occupations) > 0 THEN CONTAINS(copy.job_occupations, orig.taxo_occupations_most_specific_suid) ELSE TRUE END\n        )\n)\n\n, live_jobs_final AS (\n    SELECT *\n         -- the right job is a copy if we have a copy_job_hash defined and the description hamming distance is <= 7\n         , copy_job_hash IS NOT NULL AND bit_count(bitwise_xor(orig_high, copy_high), 64) + bit_count(bitwise_xor(orig_low, copy_low), 64) <= 7 as has_copy\n        , (overlap_end - overlap_start) / 86400 as overlap_days\n        , (orig_live_end_date - orig_live_start_date) / 86400 as orig_days_live\n        , (copy_live_end_date - copy_live_start_date) / 86400 as copy_days_live\n    FROM live_jobs_w_copy_pair\n)\n\n\nSELECT experiment_group\n     , COUNT(DISTINCT advertiser_id) as advertisers\n     , COUNT(DISTINCT job_hash) as all_live_jobs\n     , COUNT(DISTINCT CASE WHEN has_copy THEN copy_job_hash END) as live_copies\n     , COUNT(DISTINCT CASE WHEN has_copy AND has_overlap THEN copy_job_hash END) as live_copies_w_overlap -- there was a point when original and copy were both live at the same imt\n     , COUNT(DISTINCT CASE WHEN has_copy AND has_overlap = FALSE THEN copy_job_hash END) as live_copies_no_overlap -- there was a point when original and copy were not live at the same time\n     , AVG(CASE WHEN has_copy AND has_overlap THEN overlap_days ELSE NULL END) as avg_days_overlap\n     , AVG(CASE WHEN has_copy THEN orig_days_live ELSE NULL END) as orig_avg_days_live\n     , AVG(CASE WHEN has_copy THEN copy_days_live ELSE NULL END) as copy_avg_days_live\n     -- subtract the count of copy jobs from all jobs to get the total unique jobs live\n     , COUNT(DISTINCT job_hash) - COUNT(DISTINCT CASE WHEN has_copy THEN copy_job_hash END) as unique_live_jobs\nFROM live_jobs_final\nGROUP BY 1\nORDER BY 1 ASC",
  "queryTables" : [ "datalakehive.imhotep.daily_employer2", "datalakehive.imhotep.daily_employer2", "datalakehive.imhotep.dradis_job2", "datalakehive.imhotep.dradis_job2", "datalakehive.imhotep.job_copy_attributes", "datalakehive.imhotep.job_copy_attributes", "datalakehive.imhotep.jobactivitymetrics", "datalakehive.imhotep.jobactivitymetrics", "datalakehive.imhotep.recent_advertiser_test_groups", "datalakehive.imhotep.recent_advertiser_test_groups" ],
  "queryIndex" : 35,
  "runStartToQueryComplete" : 244
}, {
  "elapsedMillis" : 14830,
  "totalScheduledMillis" : 23176730,
  "cpuMillis" : 8715673,
  "queuedMillis" : 1,
  "executeMillis" : 4213,
  "getResultMillis" : 0,
  "iterateMillis" : 10643,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 180435960387,
  "query" : "WITH date_range AS (\n    SELECT\n        IMHOTEP_UNIXTIME('1month') AS start_date,\n        IMHOTEP_UNIXTIME('today') AS end_date\n)\nSELECT\n    COUNT(DISTINCT signin.accountid)\nFROM\n    datalake.imhotep.passdailysnapshot snapshot\nJOIN\n    datalake.imhotep.passsigninattempt signin ON snapshot.accountid = signin.accountid\nJOIN\n    date_range dr ON snapshot.unixtime BETWEEN dr.start_date AND dr.end_date\n    AND signin.unixtime BETWEEN dr.start_date AND dr.end_date\nWHERE\n    snapshot.deleted = 0\n    AND signin.successful = 1",
  "queryTables" : [ "datalakehive.imhotep.passdailysnapshot", "datalakehive.imhotep.passsigninattempt" ],
  "queryIndex" : 36,
  "runStartToQueryComplete" : 140
}, {
  "elapsedMillis" : 1494,
  "totalScheduledMillis" : 1960,
  "cpuMillis" : 434,
  "queuedMillis" : 1,
  "executeMillis" : 1294,
  "getResultMillis" : 0,
  "iterateMillis" : 217,
  "rows" : 18,
  "error" : null,
  "scannedBytes" : 1608144,
  "query" : "SELECT SPLIT_PART(job_profile,',',1), COUNT(*)\nFROM datalake.imhotep.indeedemployeesnapshot\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n\tAND total_reports <= 3\n    AND managers_ldap LIKE '%lafawnd%'\n    AND is_manager = 1\nGROUP BY 1\nORDER BY 2 DESC\n",
  "queryTables" : [ "datalakehive.imhotep.indeedemployeesnapshot" ],
  "queryIndex" : 37,
  "runStartToQueryComplete" : 159
}, {
  "elapsedMillis" : 4339,
  "totalScheduledMillis" : 141801,
  "cpuMillis" : 53452,
  "queuedMillis" : 1,
  "executeMillis" : 2456,
  "getResultMillis" : 0,
  "iterateMillis" : 1922,
  "rows" : 240,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing')\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '')\nORDER BY date, t0, t1, t2 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 38,
  "runStartToQueryComplete" : 176
}, {
  "elapsedMillis" : 4618,
  "totalScheduledMillis" : 135208,
  "cpuMillis" : 50735,
  "queuedMillis" : 0,
  "executeMillis" : 2566,
  "getResultMillis" : 0,
  "iterateMillis" : 2096,
  "rows" : 475,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_3 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '')\nORDER BY date, t0, t1, t2, t3 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 39,
  "runStartToQueryComplete" : 176
}, {
  "elapsedMillis" : 4898,
  "totalScheduledMillis" : 143408,
  "cpuMillis" : 51404,
  "queuedMillis" : 1,
  "executeMillis" : 2435,
  "getResultMillis" : 0,
  "iterateMillis" : 2493,
  "rows" : 2116,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_5 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '')\nORDER BY date, t0, t1, t2, t3, t4, t5 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 40,
  "runStartToQueryComplete" : 178
}, {
  "elapsedMillis" : 5801,
  "totalScheduledMillis" : 133721,
  "cpuMillis" : 47092,
  "queuedMillis" : 1,
  "executeMillis" : 4118,
  "getResultMillis" : 0,
  "iterateMillis" : 1726,
  "rows" : 30,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        ji.issuetype = 'Bug'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d')\n)\n\nSELECT\n\tdate,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nORDER BY date ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 41,
  "runStartToQueryComplete" : 179
}, {
  "elapsedMillis" : 4695,
  "totalScheduledMillis" : 12450,
  "cpuMillis" : 5356,
  "queuedMillis" : 1,
  "executeMillis" : 3068,
  "getResultMillis" : 0,
  "iterateMillis" : 1641,
  "rows" : 878,
  "error" : null,
  "scannedBytes" : 81755945,
  "query" : "WITH\n\npopulated_day AS (\n    SELECT MIN(day) AS most_recent_day\n    FROM (\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"jiracloudissues$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"ownershipSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"teamwrksSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n    )\n)\n\n-- Count of a11y violations remediated within SLO in the last 30 days / (All a11y violations remediated within last 30 days + Open issues)\nSELECT\n    inslo,\n    DATE_FORMAT(duedate, '%Y-%m-%d') AS duedate,\n    teamId,\n\t-- project,\n    issuekey,\n    -- issuetype,\n    priority,\n    status,\n    summary,\n    lastupdated,\n    DATE_FORMAT(createdate, '%Y-%m-%d') AS createdate,\n    labels,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7\nFROM (\n\tSELECT\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        ji.issuekey AS issuekey,\n        ji.issuetype AS issuetype,\n        ji.priority AS priority,\n        ji.summary AS summary,\n        ji.status AS status,\n        DATE_FORMAT(FROM_UNIXTIME(CAST(ji.lastupdated AS BIGINT) / 1000), '%Y-%m-%d') AS lastupdated,\n\t\tDATE_TRUNC('day', DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')) AS createdate,\n        DATE_TRUNC('day', DATE_ADD('day',\n        \tCASE \n                WHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        )) AS duedate,\n\t\tCAST(DATE_ADD('day',\n            CASE \n                WHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        ) > CURRENT_DATE AS BOOLEAN) AS inslo,\n        ji.labels AS labels,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.day = (SELECT most_recent_day from populated_day) AND\n        ow.day = (SELECT most_recent_day from populated_day) AND\n        tw.day = (SELECT most_recent_day from populated_day) AND\n        ji.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        ow.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        tw.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            -- The line below will include \"Pending Closure\" tickets in the resulting table\n            -- OR ji.resolutiontimestamp = 0\n        )\n    GROUP BY\n    \tow.accountablePartyId,\n    \tji.projectkey,\n        ji.issuekey,\n\t\tji.issuetype,\n        ji.priority,\n        ji.summary,\n        ji.status,\n        DATE_FORMAT(FROM_UNIXTIME(CAST(ji.lastupdated AS BIGINT) / 1000), '%Y-%m-%d'),\n\t\tDATE_TRUNC('day', DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')),\n        DATE_TRUNC('day', DATE_ADD('day',\n            CASE \n                WHEN ji.priority = 'Blocker' THEN 30\n                ELSE 90 \n           \tEND, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        )),\n        CAST(DATE_ADD('day',\n            CASE \n                WHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        ) > CURRENT_DATE AS BOOLEAN),\n        ji.labels,\n        tw.tier_0,\n        tw.tier_1,\n        tw.tier_2,\n        tw.tier_3,\n        tw.tier_4,\n        tw.tier_5,\n        tw.tier_6,\n        tw.tier_7\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY \n\tinslo ASC,\n\tduedate ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.jiracloudissues$partitions", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.ownershipsnapshot$partitions", "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot$partitions" ],
  "queryIndex" : 42,
  "runStartToQueryComplete" : 180
}, {
  "elapsedMillis" : 5574,
  "totalScheduledMillis" : 1062370,
  "cpuMillis" : 762358,
  "queuedMillis" : 1,
  "executeMillis" : 2391,
  "getResultMillis" : 0,
  "iterateMillis" : 3213,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0\n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 43,
  "runStartToQueryComplete" : 184
}, {
  "elapsedMillis" : 3396,
  "totalScheduledMillis" : 100682,
  "cpuMillis" : 40278,
  "queuedMillis" : 1,
  "executeMillis" : 2470,
  "getResultMillis" : 0,
  "iterateMillis" : 966,
  "rows" : 90,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        Lower(tw.tier_1) NOT IN ('global revenue', 'marketing')\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n\t(''='' OR t1 = '')\nORDER BY date, t0, t1 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 44,
  "runStartToQueryComplete" : 182
}, {
  "elapsedMillis" : 3970,
  "totalScheduledMillis" : 126048,
  "cpuMillis" : 52482,
  "queuedMillis" : 1,
  "executeMillis" : 2428,
  "getResultMillis" : 0,
  "iterateMillis" : 1555,
  "rows" : 3029,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_7 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '') \nORDER BY date, t0, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 45,
  "runStartToQueryComplete" : 184
}, {
  "elapsedMillis" : 3304,
  "totalScheduledMillis" : 115891,
  "cpuMillis" : 47647,
  "queuedMillis" : 1,
  "executeMillis" : 2531,
  "getResultMillis" : 0,
  "iterateMillis" : 866,
  "rows" : 1224,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_4 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '')\nORDER BY date, t0, t1, t2, t3, t4 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 46,
  "runStartToQueryComplete" : 185
}, {
  "elapsedMillis" : 4745,
  "totalScheduledMillis" : 742137,
  "cpuMillis" : 651660,
  "queuedMillis" : 1,
  "executeMillis" : 2303,
  "getResultMillis" : 0,
  "iterateMillis" : 2470,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_6 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5, t6 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 47,
  "runStartToQueryComplete" : 189
}, {
  "elapsedMillis" : 4778,
  "totalScheduledMillis" : 729313,
  "cpuMillis" : 633129,
  "queuedMillis" : 1,
  "executeMillis" : 2363,
  "getResultMillis" : 0,
  "iterateMillis" : 2442,
  "rows" : 2407,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        Lower(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0\n)\nWHERE\n\t(t0 = 'Indeed')\nORDER BY date, teamId, project, t0 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 48,
  "runStartToQueryComplete" : 190
}, {
  "elapsedMillis" : 4734,
  "totalScheduledMillis" : 705714,
  "cpuMillis" : 626565,
  "queuedMillis" : 1,
  "executeMillis" : 2531,
  "getResultMillis" : 0,
  "iterateMillis" : 2230,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_5 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 49,
  "runStartToQueryComplete" : 194
}, {
  "elapsedMillis" : 4642,
  "totalScheduledMillis" : 727815,
  "cpuMillis" : 645029,
  "queuedMillis" : 0,
  "executeMillis" : 2593,
  "getResultMillis" : 0,
  "iterateMillis" : 2304,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '')\nORDER BY date, teamId, project, t0, t1 ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 50,
  "runStartToQueryComplete" : 196
}, {
  "elapsedMillis" : 4851,
  "totalScheduledMillis" : 753162,
  "cpuMillis" : 667452,
  "queuedMillis" : 1,
  "executeMillis" : 2315,
  "getResultMillis" : 0,
  "iterateMillis" : 2566,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_7 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 51,
  "runStartToQueryComplete" : 197
}, {
  "elapsedMillis" : 3203,
  "totalScheduledMillis" : 112473,
  "cpuMillis" : 46941,
  "queuedMillis" : 1,
  "executeMillis" : 2421,
  "getResultMillis" : 0,
  "iterateMillis" : 808,
  "rows" : 2777,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_6 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '')\nORDER BY date, t0, t1, t2, t3, t4, t5, t6 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 52,
  "runStartToQueryComplete" : 197
}, {
  "elapsedMillis" : 4616,
  "totalScheduledMillis" : 694560,
  "cpuMillis" : 612002,
  "queuedMillis" : 0,
  "executeMillis" : 2421,
  "getResultMillis" : 0,
  "iterateMillis" : 2222,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '')\nORDER BY date, teamId, project, t0, t1, t2 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 53,
  "runStartToQueryComplete" : 200
}, {
  "elapsedMillis" : 4472,
  "totalScheduledMillis" : 686714,
  "cpuMillis" : 615101,
  "queuedMillis" : 0,
  "executeMillis" : 2270,
  "getResultMillis" : 0,
  "iterateMillis" : 2232,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_4 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0  \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 54,
  "runStartToQueryComplete" : 203
}, {
  "elapsedMillis" : 2368,
  "totalScheduledMillis" : 13519,
  "cpuMillis" : 4243,
  "queuedMillis" : 0,
  "executeMillis" : 881,
  "getResultMillis" : 0,
  "iterateMillis" : 1506,
  "rows" : 6,
  "error" : null,
  "scannedBytes" : 325020168,
  "query" : "SELECT\n  platform, provider,\n  count_if(cnt_empty_field_names > 0) as cnt_datasets_with_empty_field_names,\n  count_if(cnt_empty_field_names > 1) as cnt_datasets_with_multiple_empty_field_names\nFROM (\n  SELECT platform, provider, indeedid, count(1) as cnt_empty_field_names\n  FROM  metadataBusFieldSnapshot\n  WHERE day >= '2024-09-11' and day < '2024-09-12' and LENGTH(coalesce(fieldFullName, '')) = 0\n    AND datasource != 'automated-healthcheck'\n  GROUP BY 1,2,3\n)\nGROUP BY 1,2\nORDER BY 1,2\n\n\n",
  "queryTables" : [ "skipperhive.imhotep.metadatabusfieldsnapshot" ],
  "queryIndex" : 55,
  "runStartToQueryComplete" : 201
}, {
  "elapsedMillis" : 2286,
  "totalScheduledMillis" : 11998,
  "cpuMillis" : 4196,
  "queuedMillis" : 0,
  "executeMillis" : 1019,
  "getResultMillis" : 0,
  "iterateMillis" : 1297,
  "rows" : 3,
  "error" : null,
  "scannedBytes" : 322561301,
  "query" : "SELECT\n  platform, provider,\n  count_if(cnt_empty_field_names > 0) as cnt_datasets_with_empty_field_names,\n  count_if(cnt_empty_field_names > 1) as cnt_datasets_with_multiple_empty_field_names\nFROM (\n  SELECT platform, provider, indeedid, count(1) as cnt_empty_field_names\n  FROM  metadataBusFieldSnapshot\n  WHERE day >= '2024-09-11' and day < '2024-09-12' and LENGTH(coalesce(fieldName, '')) = 0\n    AND datasource != 'automated-healthcheck'\n  GROUP BY 1,2,3\n)\nGROUP BY 1,2\nORDER BY 1,2\n\n\n",
  "queryTables" : [ "skipperhive.imhotep.metadatabusfieldsnapshot" ],
  "queryIndex" : 56,
  "runStartToQueryComplete" : 201
}, {
  "elapsedMillis" : 62614,
  "totalScheduledMillis" : 60433138,
  "cpuMillis" : 25700227,
  "queuedMillis" : 1,
  "executeMillis" : 11777,
  "getResultMillis" : 0,
  "iterateMillis" : 50888,
  "rows" : 52,
  "error" : null,
  "scannedBytes" : 181400802133,
  "query" : "WITH \n\nparentcompanysizesegmentation AS (\n\n    SELECT\n        advertiser_id,\n        parent_company_size_segment\n    FROM\n        datalake.imhotep.parentcompanysizesegmentation\n    WHERE\n        day = CAST(DATE_ADD('day', -1, CURRENT_DATE) AS varchar)\n\n),\n\njob_attributes_latest AS (\n\n\tSELECT \n    \tjob_id,\n        advertiser_mapping[1].advertiser_id AS advertiser_id\n    FROM \n    \tdatalake.core.jobs_dim_job_attributes_latest\n\n),\n\nsubs_history AS (\n\n    SELECT \n        DATE_TRUNC('month', DATE(day)) AS activity_month, \n        COALESCE(parent_company_size_segment, 'null') AS parent_company_size_segment,\n        COUNT(DISTINCT subscription_id)/1000.00 AS paid_active_subscriptions_thousands\n    FROM \n        datalake.core.product_analytics_dim_resume_subscription_history subs\n    LEFT JOIN \n        parentcompanysizesegmentation pcss\n            ON subs.advertiser_id = pcss.advertiser_id\n    WHERE \n        day >= '2023-10-01'\n        AND day < '2024-11-01'\n        AND product = 'resume'\n        AND current_status IN ('active','expiring','pausing')\n        AND NOT is_trial\n        AND NOT is_shared_subscription\n    GROUP BY 1, 2\n\n),\n\ngrdm AS (\n\n    SELECT\n        DATE_TRUNC('month', DATE(SUBSTR(day, 1, 10))) AS activity_month,\n        COALESCE(parent_company_size_segment, 'null') AS parent_company_size_segment,\n        SUM(net_revenue_cents/100.00)/1000000 AS revenue_usd_millions\n    FROM \n        datalake.imhotep.grdm\n    WHERE\n        day >= '2023-10-01'\n        AND day < '2024-11-01'\n        AND product_group IN ('Resume','Sourcing')\n        AND credit_source_name != 'Agency Discount - Resume Subscription'\n    GROUP BY 1, 2\n\n),\n\nsponsored_jobs AS (\n\n    SELECT \n        activity_month,\n        COALESCE(parent_company_size_segment, 'null') AS parent_company_size_segment,\n    \tCOUNT(DISTINCT job_id) / 1000000.00 AS sponsored_jobs_millions\n    FROM \n        datalake.core.jobs_fct_job_performance_sponsored_monthly spon_jobs\n    LEFT JOIN\n        parentcompanysizesegmentation pcss\n            ON spon_jobs.advertiser_id = pcss.advertiser_id\n    GROUP BY \n        1, 2\n\n),\n\norganic_jobs AS (\n\n    SELECT \n        activity_month,\n        COALESCE(parent_company_size_segment, 'null') AS parent_company_size_segment,\n    \tCOUNT(DISTINCT org_jobs.job_id) / 1000000.00 AS organic_jobs_millions\n    FROM \n        datalake.core.jobs_fct_job_performance_organic_monthly org_jobs\n    LEFT JOIN \n    \tjob_attributes_latest\n        \tON org_jobs.job_id = job_attributes_latest.job_id\n    LEFT JOIN\n        parentcompanysizesegmentation pcss\n            ON job_attributes_latest.advertiser_id = pcss.advertiser_id\n    GROUP BY \n        1, 2\n\n),\n\nfinal AS (\n\n    SELECT \n        subs_history.activity_month,\n        subs_history.parent_company_size_segment,\n        subs_history.paid_active_subscriptions_thousands,\n        grdm.revenue_usd_millions,\n        sponsored_jobs.sponsored_jobs_millions + organic_jobs.organic_jobs_millions AS searchable_jobs_millions\n    FROM \n        subs_history\n    INNER JOIN \n        grdm \n            ON subs_history.activity_month = grdm.activity_month\n            AND subs_history.parent_company_size_segment = grdm.parent_company_size_segment\n    INNER JOIN \n        sponsored_jobs \n            ON subs_history.activity_month = sponsored_jobs.activity_month\n            AND subs_history.parent_company_size_segment = sponsored_jobs.parent_company_size_segment\n    INNER JOIN \n        organic_jobs \n            ON subs_history.activity_month = organic_jobs.activity_month\n            AND subs_history.parent_company_size_segment = organic_jobs.parent_company_size_segment\n\n)\n\nSELECT *\nFROM final\nORDER BY 1, 2",
  "queryTables" : [ "datalakehive.core.jobs_dim_job_attributes_latest", "datalakehive.core.jobs_fct_job_performance_organic_monthly", "datalakehive.core.jobs_fct_job_performance_sponsored_monthly", "datalakehive.core.product_analytics_dim_resume_subscription_history", "datalakehive.imhotep.grdm", "datalakehive.imhotep.parentcompanysizesegmentation", "datalakehive.imhotep.parentcompanysizesegmentation", "datalakehive.imhotep.parentcompanysizesegmentation" ],
  "queryIndex" : 57,
  "runStartToQueryComplete" : 270
}, {
  "elapsedMillis" : 2141,
  "totalScheduledMillis" : 15900,
  "cpuMillis" : 1979,
  "queuedMillis" : 1,
  "executeMillis" : 2031,
  "getResultMillis" : 0,
  "iterateMillis" : 123,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 47410,
  "query" : "SELECT \n\tevent_id, agg_job_id\nFROM datalake.hiring_events.dim_hiringevent_job\nWHERE event_id=327078",
  "queryTables" : [ "datalake.hiring_events.dim_hiringevent_job" ],
  "queryIndex" : 58,
  "runStartToQueryComplete" : 235
}, {
  "elapsedMillis" : 18313,
  "totalScheduledMillis" : 27207054,
  "cpuMillis" : 2035674,
  "queuedMillis" : 1,
  "executeMillis" : 12450,
  "getResultMillis" : 0,
  "iterateMillis" : 5891,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 75383721655,
  "query" : "SELECT \n\tCOUNT(*) as total_profile_views, \n   \tSUM(r.sent) as outreaches,\n    SUM(r.resp_pos) as resp_pos,\n    COUNT(js.account_id) as profile_with_linkedIn,\n    ROUND(CAST(SUM(r.sent) AS DOUBLE) * 100 / COUNT(*), 1) as outreach_rate\nFROM \n\tdatalake.imhotep.rsresumeview v\n\nLEFT JOIN\n\tdatalake.imhotep.rscontacts r\nON\n\tv.recruiterAccountId = r.senderAccountId\n    AND v.resumeAccountId = r.recipientAccountId\n    AND r.unixtime BETWEEN IMHOTEP_UNIXTIME('15d') AND IMHOTEP_UNIXTIME('4d')\n\nLEFT JOIN \n\tdatalake.imhotep.js_fact_store_snapshot_prod js\nON\n\tv.resumeAccountId = js.account_id\n\tAND js.resume_links_size > 0\n\tAND CARDINALITY(FILTER(js.resume_links_array, x -> x LIKE '%linkedin%')) > 0\n    AND js.unixtime BETWEEN IMHOTEP_UNIXTIME('15d') AND IMHOTEP_UNIXTIME('4d')\n\nWHERE \n\tv.unixtime BETWEEN IMHOTEP_UNIXTIME('15d') AND IMHOTEP_UNIXTIME('4d')\n    AND js.account_id IS NOT NULL -- with LinkedIn link\n--    AND js.account_id IS NULL -- exclude linkedIn users\n--    AND r.batchSize = 1",
  "queryTables" : [ "datalakehive.imhotep.js_fact_store_snapshot_prod", "datalakehive.imhotep.rscontacts", "datalakehive.imhotep.rsresumeview" ],
  "queryIndex" : 59,
  "runStartToQueryComplete" : 267
}, {
  "elapsedMillis" : 2589,
  "totalScheduledMillis" : 1323354,
  "cpuMillis" : 104746,
  "queuedMillis" : 1,
  "executeMillis" : 1087,
  "getResultMillis" : 0,
  "iterateMillis" : 1526,
  "rows" : 894,
  "error" : null,
  "scannedBytes" : 8797716733,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('minute', from_unixtime(lp.unixtime)) as date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-29')\n        and moderationApi='mcmurdoDetoxify'\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date_trunc('minute', from_unixtime(lp.unixtime)),\n\t\tappName,         \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n\t\tappName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 240 as double) / cast(30 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30))) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n\tappName,     \n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,\n    \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 240), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 15 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 240)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 60,
  "runStartToQueryComplete" : 271
}, {
  "elapsedMillis" : 2370,
  "totalScheduledMillis" : 1545972,
  "cpuMillis" : 92515,
  "queuedMillis" : 1,
  "executeMillis" : 945,
  "getResultMillis" : 0,
  "iterateMillis" : 1440,
  "rows" : 466,
  "error" : null,
  "scannedBytes" : 8500457018,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('minute', from_unixtime(lp.unixtime)) as date,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-29')\n        and moderationApi='mcmurdoDetoxify'\n    GROUP BY\n        date_trunc('minute', from_unixtime(lp.unixtime)),\n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 240 as double) / cast(30 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30))) as change,\n        sum(1.0) over (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count             \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n\t\traw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n         \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \nchange,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,        \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 240), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 15 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 240)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 61,
  "runStartToQueryComplete" : 271
}, {
  "elapsedMillis" : 12088,
  "totalScheduledMillis" : 17417068,
  "cpuMillis" : 1876748,
  "queuedMillis" : 1,
  "executeMillis" : 7732,
  "getResultMillis" : 0,
  "iterateMillis" : 4378,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 75382471338,
  "query" : "SELECT \n\tCOUNT(*) as total_profile_views, \n   \tSUM(r.sent) as outreaches,\n    SUM(r.resp_pos) as resp_pos,\n    COUNT(js.account_id) as profile_with_linkedIn,\n    ROUND(CAST(SUM(r.sent) AS DOUBLE) * 100 / COUNT(*), 1) as outreach_rate\nFROM \n\tdatalake.imhotep.rsresumeview v\n\nLEFT JOIN\n\tdatalake.imhotep.rscontacts r\nON\n\tv.recruiterAccountId = r.senderAccountId\n    AND v.resumeAccountId = r.recipientAccountId\n    AND r.unixtime BETWEEN IMHOTEP_UNIXTIME('15d') AND IMHOTEP_UNIXTIME('4d')\n\nLEFT JOIN \n\tdatalake.imhotep.js_fact_store_snapshot_prod js\nON\n\tv.resumeAccountId = js.account_id\n\tAND js.resume_links_size > 0\n\tAND CARDINALITY(FILTER(js.resume_links_array, x -> x LIKE '%linkedin%')) > 0\n    AND js.unixtime BETWEEN IMHOTEP_UNIXTIME('15d') AND IMHOTEP_UNIXTIME('4d')\n\nWHERE \n\tv.unixtime BETWEEN IMHOTEP_UNIXTIME('15d') AND IMHOTEP_UNIXTIME('4d')\n--    AND js.account_id IS NOT NULL -- with LinkedIn link\n    AND js.account_id IS NULL -- exclude linkedIn users\n--    AND r.batchSize = 1",
  "queryTables" : [ "datalakehive.imhotep.js_fact_store_snapshot_prod", "datalakehive.imhotep.rscontacts", "datalakehive.imhotep.rsresumeview" ],
  "queryIndex" : 62,
  "runStartToQueryComplete" : 290
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 11316,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250109_235744_00065_mzvde): Division by zero",
  "scannedBytes" : 0,
  "query" : "with \n-- Pull all conversations initiated by employers \ndremrToConvs as (\nselect  unixtime, advertiserId,  eventId, conversationId\n  \t\t, case when starts_with(subEventType, 'INBOUND_EMAIL') then 1 else 0 end as is_email\n        , case when starts_with(source, 'Dremr')  then 1 else 0 end as is_Dremr  \n  from conversationEvents \nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n  and context = 'APPLICATION'\n  and ownerRole != 'JOBSEEKER'\n  and cast(isInitialMessage as int)=1 -- Look at initial messages only, as a proxy for when a conversation is created\ngroup by 1,2,3,4,5,6\n\n              )\n              \n-- Employers with any ATS Integration\n,atsEmployers as (\n\nselect advertiserId  \n  from partnerEmployerMap\n where unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n   and confidenceScore >0\ngroup by  1 \n\n                )\n\n-- advertisers that have had at least 1 candidate delivered to the ATS with a DREMR email\n, atsSync as (\n\nselect advertiser_id, connection_type as ats \n      , agg_job_id\n      ,concat('DRADIS/', cast(advertiser_id as varchar), '-', cast(dradis_candidate_id as varchar)) as conversationId\n\n  from acdcTransferLifecycle\n where unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n   and pipeline_internal_type='INTERNAL_LOCATION_TYPE_EMPLOYER_JOB'\n   and connection_type!= 'h2ia'\n   and driver_type not in ('pipeline-csul', 'pipeline-csul-historical')\ngroup by 1,2,3,4\n            \n            )\n -- Dradis bulk export           \n,dradisBulkExport as (    \t\n\n        SELECT advertiserId\n\t\t\t  ,concat('DRADIS/', cast(advertiserId as varchar), '-', cast(candidateId as varchar)) as conversationId\n        FROM datalake.imhotep.dradisCandidateExportRequest\n        CROSS JOIN UNNEST(split(candidateIds, ',')) as t (candidateId)\n        WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n       group by 1,2\n  )     \n\n\n-- advertiser segment details\n,advertiserDetails as ( \n\nselect advertiser_id\n   , max_by(parent_company_size_segment, unixtime) as parent_company_size_segment\n   , max_by(type, unixtime) as type\n   , max_by(billing_country, unixtime) as billing_country\n  from  daily_employer2 \n  where unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\ngroup by 1\n\n   )\n\n\n -- Aggregate convo counts per query\n, empDataAgg as (\n\n\n select \n\tdremrToConvs.advertiserId\n   ,case when atsEmployers.advertiserId is not null then 1 else 0 end as isAtsIntegrated   \n   ,count(*) as allConvos\n   ,count_if(is_Dremr=1) as dremrConvos\n   --,count_if(is_Dremr=1 and is_email=1) as dremrEmailConvos\n   ,count_if(is_Dremr=1 and atsSync.conversationId is not null) as dremrAtsSyncAll\n   ,count_if(is_Dremr=1 and dradisBulkExport.conversationId is not null) as dremrDradisBulkExport\n   ,count_if(is_Dremr=1 and (atsSync.conversationId is not null or dradisBulkExport.conversationId is not null)) as dremrAtsSyncOrBulkExport\n   \n   \n  from dremrToConvs \n  left join atsEmployers on dremrToConvs.advertiserId=atsEmployers.advertiserId\n  left join atsSync on dremrToConvs.advertiserId=atsSync.advertiser_id\n        and dremrToConvs.conversationId=atsSync.conversationId\n  left join dradisBulkExport on dremrToConvs.advertiserId=dradisBulkExport.advertiserId\n        and dremrToConvs.conversationId=dradisBulkExport.conversationId\n    \n  group by 1,2\n                )\n                \n \n,segConvos as (\nselect\n    ce.advertiserId\n   ,eventId\n   ,ce.encryptedConversationId\n   ,(isImported = '1') as isEmail\nFROM conversationEvents ce\njoin convsConversationCreation cc on  ce.encryptedConversationId = cc.encryptedConversationId \n\nWHERE ce.unixtime BETWEEN IMHOTEP_UNIXTIME('209d') AND IMHOTEP_UNIXTIME('28d')\nand   cc.unixtime BETWEEN IMHOTEP_UNIXTIME('223d') AND IMHOTEP_UNIXTIME('28d')\nand ((ce.unixtime - cc.unixtime) < (60*60*24*14))\nand ce.context='APPLICATION'\nand ce.ownerRole != 'JOBSEEKER'\nGROUP BY 1,2,3,4\n)\n\n\n, segConvosAgg as (\n\nselect advertiserId\n      ,count(*) as totalMessages\n      ,count_if(isEmail) as emailMessages\n      ,count_if(NOT isEmail) as onPlatformMessages \n  from segConvos\ngroup by 1 \n\n\t\t)\n\n,segConvosAgg2 as (\n\n select advertiserId, case when emailMessages> 0 and onPlatformMessages=0 then 'Email Only'\n\t\t\t when  emailMessages=0 and onPlatformMessages>0 then 'Chat Only'\n\t\t\t when  emailMessages>0 and onPlatformMessages>0 then 'Email & Chat'\n             end as usageType\n\n  from segConvosAgg\n                 )\n\n\nselect \n        segConvosAgg2.usageType\n       ,count() as empCount\n       , 100.0*count()/sum(count()) over () pctEmpCountOfAll\n    --  ,count_if(isAtsIntegrated=1) as empCountAtsIntegrated\n      ,100.00* count_if(isAtsIntegrated=1)/count(*) as pctEmpAtsIntegrated\n      ,sum(allConvos) as ttlConvosInitiated\n\t  ,sum(DremrConvos) as DremrConvosInitiated\n  --    ,sum(dremrConvos) as ttlDremrConvosInitiated\n      ,100.00* sum(dremrConvos)/sum(allConvos) as dremrConvosPctOfAllInitiated\n      \n      ,100.00* sum(dremrAtsSyncAll)/sum(dremrConvos) as pctOfDremrConvosAtsSyncAll\n      ,100.00* sum(dremrDradisBulkExport)/sum(dremrConvos) as pctOfDremrConvosDradisBulkExport\n      ,100.00* sum(dremrAtsSyncOrBulkExport)/sum(dremrConvos) as pctOfDremrConvosAtsSyncOrBulkExport\n     \n from empDataAgg\nleft join advertiserDetails on empDataAgg.advertiserId=advertiserDetails.advertiser_id\nleft join segConvosAgg2     on empDataAgg.advertiserId=segConvosAgg2.advertiserId\n    where  parent_company_size_segment in ('L', 'XL') and usageType is not null\ngroup by 1\n                \n\n                \n\n              \n              \n              ",
  "queryTables" : [ "datalakehive.imhotep.dradiscandidateexportrequest", "skipperhive.imhotep.acdctransferlifecycle", "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.convsconversationcreation", "skipperhive.imhotep.daily_employer2", "skipperhive.imhotep.partneremployermap" ],
  "queryIndex" : 63,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 80612,
  "totalScheduledMillis" : 147532777,
  "cpuMillis" : 28244001,
  "queuedMillis" : 1,
  "executeMillis" : 3730,
  "getResultMillis" : 0,
  "iterateMillis" : 76947,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 548789268589,
  "query" : "WITH jobs_raw AS (\n\t-- get all of the live jobs for our advertisers\n\tSELECT CASE WHEN CARDINALITY(FILTER(ratgs_lite, x -> x LIKE '%freevalue_so_job_copies_gb1'))>0\n             THEN 'Test' ELSE 'Control' END as experiment_group\n\t     , advertiser_id\n\t     , job_hash\n\t     , job_city\n\t     , job_state as job_admin_1_code\n\t     , (is_job_searchable+impressions+clicks+apply_starts)>0 as is_live\n\t     , unixtime\n    FROM datalake.imhotep.jobactivitymetrics as jam\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-23') AND IMHOTEP_UNIXTIME('today')\n        AND job_country_code='GB'\n        AND job_city IS NOT NULL\n        AND job_state IS NOT NULL\n        AND CARDINALITY(FILTER(ratgs_lite, x -> x LIKE '%B%:freevalue_so_job_copies_gb%'))>0\n)\n\n, jobs_raw_w_dj2 AS (\n    -- reduce to only jobs that were created or updated during the test window\n    SELECT jam.experiment_group\n\t     , jam.advertiser_id\n\t     , jam.job_hash\n\t     , jam.job_city\n\t     , jam.job_admin_1_code\n\t     , jam.is_live\n\t     , jam.unixtime\n    FROM jobs_raw jam\n    JOIN  datalake.imhotep.dradis_job2 as dj2 ON (dj2.job_hash_underscore = jam.job_hash)\n    WHERE dj2.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-23') AND IMHOTEP_UNIXTIME('today')\n        AND (dj2.date_created>=20240923\n            OR (previous_status IN ('PAUSED','DELETED') AND status='ACTIVE'))\n)\n\n, ordered_jobs AS (\n    SELECT experiment_group\n\t     , advertiser_id\n\t     , job_hash\n         , job_city\n\t     , job_admin_1_code\n         , unixtime\n\t     , is_live\n\t     , LAG(is_live) OVER (PARTITION BY job_hash, experiment_group ORDER BY unixtime) AS prev_is_live\n\t     , LAG(unixtime) OVER (PARTITION BY job_hash, experiment_group ORDER BY unixtime) AS prev_unixtime\n    FROM jobs_raw_w_dj2\n)\n\n, jobs_w_live_periods AS (\n    SELECT experiment_group\n\t     , advertiser_id\n\t     , job_hash\n         , job_city\n\t     , job_admin_1_code\n         , unixtime as live_start_date\n         , LEAD(unixtime, 1, TO_UNIXTIME(CURRENT_DATE)) OVER (PARTITION BY job_hash, experiment_group ORDER BY unixtime) AS live_end_date\n    FROM ordered_jobs\n    WHERE is_live = true\n        AND (prev_is_live IS NULL OR prev_is_live = false)\n)\n\n, jobs_w_live_periods_copy_attrs AS (\n    -- get the fields we need for copy comparison\n    SELECT experiment_group\n         , jca.advertiser_id\n         , job_hash\n         , job_on_indeed_unixtime\n         , jca.job_city\n         , jca.job_admin_1_code\n         , job_type_id\n         , job_normalized_title_sim_hash\n         , job_description_sim_hash\n         , taxo_occupations_most_specific_suid\n         , job_occupations\n         , jwlp.live_start_date\n         , jwlp.live_end_date\n    FROM jobs_w_live_periods jwlp\n    JOIN datalake.imhotep.job_copy_attributes as jca USING (job_hash)\n    WHERE jca.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-23') AND IMHOTEP_UNIXTIME('today')\n    AND jca.job_admin_1_code IS NOT NULL\n    AND jca.job_city IS NOT NULL\n)\n\n, live_jobs_w_copy_pair AS (\n    SELECT orig.experiment_group\n        , orig.advertiser_id as advertiser_id\n        , orig.job_hash as job_hash\n        , copy.job_hash as copy_job_hash\n        , orig.job_city\n        , orig.job_admin_1_code\n        , orig.live_start_date as orig_live_start_date\n        , orig.live_end_date as orig_live_end_date\n        , copy.live_start_date as copy_live_start_date\n        , copy.live_end_date as copy_live_end_date\n        , GREATEST(orig.live_start_date, copy.live_start_date) AS overlap_start\n        , LEAST(orig.live_end_date, copy.live_end_date) AS overlap_end\n        , orig.live_start_date <= copy.live_end_date AND copy.live_start_date <= orig.live_end_date as has_overlap -- original and copy were live at the same time\n         -- split the 128-bit simhash into two 64-bit representations\n        , cast( from_big_endian_64(lpad(substr(from_base64(orig.job_description_sim_hash), 1, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as orig_high\n        , cast( from_big_endian_64(lpad(substr(from_base64(orig.job_description_sim_hash), 9, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as orig_low\n        , cast( from_big_endian_64(lpad(substr(from_base64(copy.job_description_sim_hash), 1, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as copy_high\n        , cast( from_big_endian_64(lpad(substr(from_base64(copy.job_description_sim_hash), 9, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as copy_low\n    FROM jobs_w_live_periods_copy_attrs orig\n    LEFT JOIN jobs_w_live_periods_copy_attrs copy ON (\n        orig.experiment_group = copy.experiment_group\n        AND orig.advertiser_id = copy.advertiser_id\n        AND orig.job_city = copy.job_city\n        AND orig.job_admin_1_code = copy.job_admin_1_code\n        AND orig.job_hash != copy.job_hash\n        AND orig.job_on_indeed_unixtime < copy.job_on_indeed_unixtime -- original must be created before the copy\n\n        AND bitwise_or(orig.job_type_id, copy.job_type_id) > 0 -- job_type_id any match\n\n        -- compare normalized titles when they are both not null. otherwise, ignore them\n        AND CASE WHEN orig.job_normalized_title_sim_hash IS NOT NULL AND copy.job_normalized_title_sim_hash IS NOT NULL THEN orig.job_normalized_title_sim_hash = copy.job_normalized_title_sim_hash ELSE TRUE END\n\n        -- compare occupation when they are both not null. otherwise, ignore them\n        AND CASE WHEN orig.taxo_occupations_most_specific_suid IS NOT NULL AND CARDINALITY(copy.job_occupations) > 0 THEN CONTAINS(copy.job_occupations, orig.taxo_occupations_most_specific_suid) ELSE TRUE END\n        )\n)\n\n, live_jobs_final AS (\n    SELECT *\n         -- the right job is a copy if we have a copy_job_hash defined and the description hamming distance is <= 7\n         , copy_job_hash IS NOT NULL AND bit_count(bitwise_xor(orig_high, copy_high), 64) + bit_count(bitwise_xor(orig_low, copy_low), 64) <= 7 as has_copy\n        , (overlap_end - overlap_start) / 86400 as overlap_days\n        , (orig_live_end_date - orig_live_start_date) / 86400 as orig_days_live\n        , (copy_live_end_date - copy_live_start_date) / 86400 as copy_days_live\n    FROM live_jobs_w_copy_pair\n)\n\n\nSELECT experiment_group\n     , COUNT(DISTINCT advertiser_id) as advertisers\n     , COUNT(DISTINCT job_hash) as all_live_jobs\n     , COUNT(DISTINCT CASE WHEN has_copy THEN copy_job_hash END) as live_copies\n     , COUNT(DISTINCT CASE WHEN has_copy AND has_overlap THEN copy_job_hash END) as live_copies_w_overlap -- there was a point when original and copy were both live at the same imt\n     , COUNT(DISTINCT CASE WHEN has_copy AND has_overlap = FALSE THEN copy_job_hash END) as live_copies_no_overlap -- there was a point when original and copy were not live at the same time\n     , AVG(CASE WHEN has_copy AND has_overlap THEN overlap_days ELSE NULL END) as avg_days_overlap\n     , AVG(CASE WHEN has_copy THEN orig_days_live ELSE NULL END) as orig_avg_days_live\n     , AVG(CASE WHEN has_copy THEN copy_days_live ELSE NULL END) as copy_avg_days_live\n     -- subtract the count of copy jobs from all jobs to get the total unique jobs live\n     , COUNT(DISTINCT job_hash) - COUNT(DISTINCT CASE WHEN has_copy THEN copy_job_hash END) as unique_live_jobs\nFROM live_jobs_final\nGROUP BY 1\nORDER BY 1 ASC",
  "queryTables" : [ "datalakehive.imhotep.dradis_job2", "datalakehive.imhotep.dradis_job2", "datalakehive.imhotep.job_copy_attributes", "datalakehive.imhotep.job_copy_attributes", "datalakehive.imhotep.jobactivitymetrics", "datalakehive.imhotep.jobactivitymetrics" ],
  "queryIndex" : 64,
  "runStartToQueryComplete" : 403
}, {
  "elapsedMillis" : 34839,
  "totalScheduledMillis" : 39037200,
  "cpuMillis" : 12540044,
  "queuedMillis" : 1,
  "executeMillis" : 4438,
  "getResultMillis" : 0,
  "iterateMillis" : 30429,
  "rows" : 1740,
  "error" : null,
  "scannedBytes" : 471776492657,
  "query" : "-- Original Creator: atruong\n-- Updated By: spencerbrown\n-- Description: AWS Pull similar to Product Ishbook for sponsored engagement\n-- Data Lake link: https://pda.indeed.tech/datalake\n-- If this errors out, you may need AWS access. Request on Jira\n\nselect\na.job_id,\na.advertiser_id,\nb.feed_id,\nb.source_id,\nb.fcc_id,\nb.job_visibility_level,\nb.job_city,\nb.job_state,\nb.job_country,\nb.norm_title,\nb.norm_title_category,\nb.actual_title,\nb.job_reference_number,\nsum(a.impressions) sj_impressions,\nsum(a.clicks) sj_clicks,\nsum(a.apply_starts) sj_apply_starts,\nsum(a.applies) sj_applies,\nsum(a.charged_amount_local) charged_amount_local\n\nfrom datalake.core.jobs_fct_job_performance_sponsored_daily a\n\nleft join datalake.core.jobs_dim_job_attributes_latest b\non a.job_id = b.job_id\n\nwhere a.activity_date between cast('2024-01-01' as date) and cast('2024-08-31' as date)\nand a.advertiser_id in (42599, 16911779)\n\ngroup by 1,2,3,4,5,6,7,8,9,10,11,12,13",
  "queryTables" : [ "datalakehive.core.jobs_dim_job_attributes_latest", "datalakehive.core.jobs_fct_job_performance_sponsored_daily" ],
  "queryIndex" : 65,
  "runStartToQueryComplete" : 366
}, {
  "elapsedMillis" : 1619,
  "totalScheduledMillis" : 488,
  "cpuMillis" : 115,
  "queuedMillis" : 0,
  "executeMillis" : 852,
  "getResultMillis" : 0,
  "iterateMillis" : 778,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 66,
  "runStartToQueryComplete" : 334
}, {
  "elapsedMillis" : 1228,
  "totalScheduledMillis" : 308741,
  "cpuMillis" : 26553,
  "queuedMillis" : 0,
  "executeMillis" : 1209,
  "getResultMillis" : 0,
  "iterateMillis" : 484,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 584803869,
  "query" : "SELECT *\nFROM datalake.imhotep.passdailysnapshot\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.passdailysnapshot" ],
  "queryIndex" : 67,
  "runStartToQueryComplete" : 348
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 0,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250109_235843_00070_mzvde): line 57:9: Column 'casenumber' cannot be resolved",
  "scannedBytes" : 0,
  "query" : "WITH messassingsession_raw AS (\n    SELECT \n        session_owner_adc_id__c AS rep_id,\n        session_owner__c AS rep_name,\n        csat_score__c AS csat_score,\n        how_could_we_improve_your_experience__c AS csat_feedback,\n        caseid,\n        lastmodifieddate AS survey_sent_date,\n        substring(createddate, 1, 10) AS survey_date,\n        name AS response_id,\n        conversationid AS survey_instance_id,\n        skillname__c AS queue_name,\n        messaging_user_parentcompanyid__c AS parent_id,\n        agenttype,\n        channelname,\n        locale__c\n    FROM datalake.salesforce.messagingsession\n),\nold_chat_table as (\n\tSELECT \n      adc_user_id__c as rep_id,\n      '' as rep_name,\n      csat_score__c as csat_score,\n      how_could_we_improve_your_experience__c as csat_feedback,\n      caseid,\n      createddate as survey_sent_date,\n      substring(createddate, 1, 10) as survey_date,\n      name AS response_id,\n      chatkey as survey_instance_id,\n      livechatbuttonid AS queue_name,\n      '' AS parent_id,\n      '' as agenttype,\n      '' as channelname,\n      browserlanguage as locale__c\n  \tFROM datalake.salesforce.livechattranscript\n),\njoined_chat_tables as (\n  SELECT * FROM messassingsession_raw \n  UNION ALL \n  SELECT * FROM old_chat_table\n),\nsalesforce_user_data AS (\n    SELECT\n        adc_user_id__c,\n        name,\n        email,\n        alias,\n        department\n    FROM datalake.salesforce.\"user\"\n),\nsalesforce_case_data AS (\n    SELECT\n        id AS sales_caseid,\n        advertiser_id,\n        type,\n        type_sub_type,\n        casenumber\n    FROM datalake.scss.salesforce_case\n),\nmessages_salesforce AS (\n    SELECT \n        ms.rep_id,\n        ms.rep_name,\n        ms.csat_score,\n        ms.csat_feedback,\n        ms.caseid,\n        ms.survey_sent_date,\n        ms.survey_date,\n        ms.response_id,\n        ms.survey_instance_id,\n        ms.queue_name,\n        ms.parent_id,\n        ms.agenttype,\n        ms.channelname,\n        ms.locale__c,\n        ud.adc_user_id__c,\n        ud.name AS user_name,\n        ud.email,\n        ud.alias,\n        ud.department,\n        scd.sales_caseid,\n        scd.advertiser_id,\n        scd.type AS case_type,\n        scd.type_sub_type,\n        scd.casenumber\n    FROM joined_chat_tables ms\n    LEFT JOIN salesforce_user_data ud\n        ON ud.adc_user_id__c = ms.rep_id\n    LEFT JOIN salesforce_case_data scd\n        ON ms.caseid = scd.sales_caseid\n),\nadvertiser_info AS (\n    SELECT DISTINCT\n        a.advertiser_id,\n        a.advertiser_name,\n        a.type AS advertiser_type,\n        a.billing_country,\n        p.parent_company_id,\n        p.parent_company_name,\n        p.company_size_segment AS parent_company_size_segment\n    FROM datalake.core.client_attributes_dim_advertiser_attributes_current a\n    LEFT JOIN datalake.core.client_attributes_dim_parent_attributes_current p\n        ON a.parent_company_id = p.parent_company_id\n),\ncsat_chat as (\n\tSELECT \n    ms.rep_id,\n    ms.rep_name,\n    TRY_CAST(ms.csat_score AS DOUBLE) as csat_score,\n    ms.csat_feedback,\n    ms.caseid,\n    ms.survey_sent_date,\n    ms.survey_date,\n    ms.response_id,\n    ms.survey_instance_id,\n    ms.queue_name,\n    ms.parent_id,\n    ms.agenttype,\n    ms.channelname,\n    ms.locale__c,\n    ms.adc_user_id__c,\n    ms.user_name,\n    ms.email,\n    ms.alias,\n    ms.department,\n    ms.sales_caseid,\n    ms.advertiser_id,\n    ms.case_type,\n    ms.type_sub_type,\n    ms.casenumber,\n    ai.advertiser_name,\n    ai.advertiser_type,\n    ai.billing_country,\n    ai.parent_company_id,\n    ai.parent_company_name,\n    ai.parent_company_size_segment\nFROM messages_salesforce ms\nLEFT JOIN advertiser_info ai\n    ON CAST(ai.advertiser_id AS VARCHAR) = CAST(ms.advertiser_id AS VARCHAR)\n)\nSELECT * FROM csat_chat WHERE csat_score IS NOT NULL AND csat_score > 0 LIMIT 100",
  "queryTables" : [ "datalakehive.core.client_attributes_dim_advertiser_attributes_current", "datalakehive.core.client_attributes_dim_parent_attributes_current", "datalakehive.salesforce.livechattranscript", "datalakehive.salesforce.messagingsession", "datalakehive.salesforce.user", "datalakehive.scss.salesforce_case" ],
  "queryIndex" : 68,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 1876,
  "totalScheduledMillis" : 119883,
  "cpuMillis" : 18063,
  "queuedMillis" : 0,
  "executeMillis" : 740,
  "getResultMillis" : 0,
  "iterateMillis" : 1153,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 3448420855,
  "query" : "SELECT *\nFROM datalake.imhotep.passdailysnapshot\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nand accountid = 1118152174\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.passdailysnapshot" ],
  "queryIndex" : 69,
  "runStartToQueryComplete" : 370
}, {
  "elapsedMillis" : 155541,
  "totalScheduledMillis" : 891794,
  "cpuMillis" : 418674,
  "queuedMillis" : 1,
  "executeMillis" : 5755,
  "getResultMillis" : 0,
  "iterateMillis" : 149798,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 181074159,
  "query" : "select count(1) from logrepo.raw_log.orgClk where unixtime >= imhotep_unixtime('2019-11-02 00:00:00') and unixtime < imhotep_unixtime('2019-11-02 01:00:00')",
  "queryTables" : [ "logrepo.raw_log.orgclk" ],
  "queryIndex" : 70,
  "runStartToQueryComplete" : 530
}, {
  "elapsedMillis" : 5422,
  "totalScheduledMillis" : 7527,
  "cpuMillis" : 3751,
  "queuedMillis" : 1,
  "executeMillis" : 4425,
  "getResultMillis" : 0,
  "iterateMillis" : 1034,
  "rows" : 131,
  "error" : null,
  "scannedBytes" : 81755945,
  "query" : "WITH\n\npopulated_day AS (\n    SELECT MIN(day) AS most_recent_day\n    FROM (\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"jiracloudissues$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"ownershipSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"teamwrksSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n    )\n)\n\n-- Count of a11y violations remediated within SLO in the last 30 days / (All a11y violations remediated within last 30 days + Open issues)\nSELECT\n    inslo,\n    DATE_FORMAT(duedate, '%Y-%m-%d') AS duedate,\n    teamId,\n\t-- project,\n    issuekey,\n    -- issuetype,\n    priority,\n    status,\n    summary,\n    lastupdated,\n    DATE_FORMAT(createdate, '%Y-%m-%d') AS createdate,\n    labels,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7\nFROM (\n\tSELECT\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        ji.issuekey AS issuekey,\n        ji.issuetype AS issuetype,\n        ji.priority AS priority,\n        ji.summary AS summary,\n        ji.status AS status,\n        DATE_FORMAT(FROM_UNIXTIME(CAST(ji.lastupdated AS BIGINT) / 1000), '%Y-%m-%d') AS lastupdated,\n\t\tDATE_TRUNC('day', DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')) AS createdate,\n        DATE_TRUNC('day', DATE_ADD('day',\n        \tCASE \n                WHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        )) AS duedate,\n\t\tCAST(DATE_ADD('day',\n            CASE \n                WHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        ) > CURRENT_DATE AS BOOLEAN) AS inslo,\n        ji.labels AS labels,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.day = (SELECT most_recent_day from populated_day) AND\n        ow.day = (SELECT most_recent_day from populated_day) AND\n        tw.day = (SELECT most_recent_day from populated_day) AND\n        ji.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        ow.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        tw.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            -- The line below will include \"Pending Closure\" tickets in the resulting table\n            -- OR ji.resolutiontimestamp = 0\n        )\n    GROUP BY\n    \tow.accountablePartyId,\n    \tji.projectkey,\n        ji.issuekey,\n\t\tji.issuetype,\n        ji.priority,\n        ji.summary,\n        ji.status,\n        DATE_FORMAT(FROM_UNIXTIME(CAST(ji.lastupdated AS BIGINT) / 1000), '%Y-%m-%d'),\n\t\tDATE_TRUNC('day', DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')),\n        DATE_TRUNC('day', DATE_ADD('day',\n            CASE \n                WHEN ji.priority = 'Blocker' THEN 30\n                ELSE 90 \n           \tEND, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        )),\n        CAST(DATE_ADD('day',\n            CASE \n                WHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, \n            DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')\n        ) > CURRENT_DATE AS BOOLEAN),\n        ji.labels,\n        tw.tier_0,\n        tw.tier_1,\n        tw.tier_2,\n        tw.tier_3,\n        tw.tier_4,\n        tw.tier_5,\n        tw.tier_6,\n        tw.tier_7\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    ('Job Seeker Journey'='' OR t3 = 'Job Seeker Journey') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY \n\tinslo ASC,\n\tduedate ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.jiracloudissues$partitions", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.ownershipsnapshot$partitions", "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot$partitions" ],
  "queryIndex" : 71,
  "runStartToQueryComplete" : 415
}, {
  "elapsedMillis" : 4386,
  "totalScheduledMillis" : 7601,
  "cpuMillis" : 3892,
  "queuedMillis" : 1,
  "executeMillis" : 3474,
  "getResultMillis" : 0,
  "iterateMillis" : 954,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 33829976,
  "query" : "WITH populated_day AS (\n    SELECT MIN(day) AS most_recent_day\n    FROM (\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"jiracloudissues$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"ownershipSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"teamwrksSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n    )\n),\n\nslo_data AS (\n    SELECT\n        teamId,\n        project,\n        CASE\n            WHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n            ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n        END AS SLO_Score,\n        REM_30 AS Closed_Within_SLO,\n        ALL_REM_30 AS All_Remediated_Last_30,\n        UNREM AS Open_Outside_SLO,\n        t1,\n        t2,\n        t3,\n        t4,\n        t5,\n        t6,\n        t7\n    FROM (\n        SELECT\n            ji.projectkey AS project,\n            ow.accountablePartyId AS teamId,\n            tw.tier_1 AS t1,\n            tw.tier_2 AS t2,\n            tw.tier_3 AS t3,\n            tw.tier_4 AS t4,\n            tw.tier_5 AS t5,\n            tw.tier_6 AS t6,\n            tw.tier_7 AS t7,\n\n            COUNT(CASE WHEN\n                ji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n                (\n                    (ji.priority = 'Blocker' AND\n                    ((ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 1000) / 86400 <= 30 AND\n                    ((TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 1000) / 86400000 <= 30) OR \n                    (ji.priority NOT IN ('Blocker') AND\n                    (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000) / 86400 <= 30 AND\n                    (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400000 <= 90)\n                )\n            THEN 1 END) AS REM_30,\n\n            COUNT(CASE WHEN\n                ji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n            THEN 1 END) AS ALL_REM_30,\n\n            COUNT(CASE WHEN\n                ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                (\n                    (ji.priority = 'Blocker' AND (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 > 30) OR\n                    (ji.priority NOT IN ('Blocker') AND (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 > 90)\n                )\n            THEN 1 END) AS UNREM\n        FROM datalake.imhotep.jiracloudissues AS ji\n        JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n        JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n        WHERE\n            ji.day = (SELECT most_recent_day FROM populated_day) AND\n            ow.day = (SELECT most_recent_day FROM populated_day) AND\n            tw.day = (SELECT most_recent_day FROM populated_day) AND\n            ji.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n            ow.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n            tw.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n            CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n            NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n            ji.projectkey NOT IN ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n            ji.issuetype = 'Bug' AND\n            tw.tier_1 NOT IN ('Global Revenue', 'Marketing')\n        GROUP BY ow.accountablePartyId, ji.projectkey, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n    )\n)\n\nSELECT *\nFROM slo_data\nWHERE \n    SLO_Score > 0 AND SLO_Score != 100 AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    ('Job Seeker Journey'='' OR t3 = 'Job Seeker Journey') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY teamId, project, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.jiracloudissues$partitions", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.ownershipsnapshot$partitions", "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot$partitions" ],
  "queryIndex" : 72,
  "runStartToQueryComplete" : 415
}, {
  "elapsedMillis" : 2487,
  "totalScheduledMillis" : 5044070,
  "cpuMillis" : 1204010,
  "queuedMillis" : 1,
  "executeMillis" : 868,
  "getResultMillis" : 0,
  "iterateMillis" : 1641,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 7936106304,
  "query" : "/*SELECT MIN(eventtime), MAX(eventtime)\nFROM datalake.mdp_offline_feature_store.job_feature_empx_enhanced_job_feature_by_advertiser_model\nWHERE top5occupationreceivepositivesentimentandcount IS NOT NULL;*/\n\nSELECT \n    100.0 * SUM(CASE \n    \t\t\t\tWHEN top5occupationreceivepositivesentimentandcount IS NOT NULL AND CARDINALITY(top5occupationreceivepositivesentimentandcount) > 0\n                    \tTHEN 1 \n                    ELSE 0 END) / COUNT(*) AS coverage_percentage\nFROM datalake.mdp_offline_feature_store.job_feature_empx_enhanced_job_feature_by_advertiser_model\nWHERE eventtime BETWEEN from_unixtime(1692511252) AND from_unixtime(1718344260) --Bombyx dataset timestamp range\n",
  "queryTables" : [ "datalake.mdp_offline_feature_store.job_feature_empx_enhanced_job_feature_by_advertiser_model" ],
  "queryIndex" : 73,
  "runStartToQueryComplete" : 414
}, {
  "elapsedMillis" : 20561,
  "totalScheduledMillis" : 81415154,
  "cpuMillis" : 3662773,
  "queuedMillis" : 0,
  "executeMillis" : 9894,
  "getResultMillis" : 0,
  "iterateMillis" : 10693,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 190949332503,
  "query" : "SELECT \n\tCOUNT(*) as total_profile_views, \n   \tSUM(r.sent) as outreaches,\n    SUM(r.resp_pos) as resp_pos,\n    COUNT(js.account_id) as profile_with_linkedIn,\n    ROUND(CAST(SUM(r.sent) AS DOUBLE) * 100 / COUNT(*), 1) as outreach_rate\nFROM \n\tdatalake.imhotep.rsresumeview v\n\nLEFT JOIN\n\tdatalake.imhotep.rscontacts r\nON\n\tv.recruiterAccountId = r.senderAccountId\n    AND v.resumeAccountId = r.recipientAccountId\n    AND r.unixtime BETWEEN IMHOTEP_UNIXTIME('29d') AND IMHOTEP_UNIXTIME('1d')\n\nLEFT JOIN \n\tdatalake.imhotep.js_fact_store_snapshot_prod js\nON\n\tv.resumeAccountId = js.account_id\n\tAND js.resume_links_size > 0\n\tAND CARDINALITY(FILTER(js.resume_links_array, x -> x LIKE '%linkedin%')) > 0\n    AND js.unixtime BETWEEN IMHOTEP_UNIXTIME('29d') AND IMHOTEP_UNIXTIME('1d')\n\nWHERE \n\tv.unixtime BETWEEN IMHOTEP_UNIXTIME('29d') AND IMHOTEP_UNIXTIME('1d')\n--    AND js.account_id IS NOT NULL -- with LinkedIn link\n    AND js.account_id IS NULL -- exclude linkedIn users\n--    AND r.batchSize = 1",
  "queryTables" : [ "datalakehive.imhotep.js_fact_store_snapshot_prod", "datalakehive.imhotep.rscontacts", "datalakehive.imhotep.rsresumeview" ],
  "queryIndex" : 74,
  "runStartToQueryComplete" : 442
}, {
  "elapsedMillis" : 10294,
  "totalScheduledMillis" : 4479413,
  "cpuMillis" : 1707082,
  "queuedMillis" : 1,
  "executeMillis" : 3381,
  "getResultMillis" : 0,
  "iterateMillis" : 6956,
  "rows" : 12,
  "error" : null,
  "scannedBytes" : 80368500492,
  "query" : "with convos as (\nselect\n    ce.advertiserId\n   ,eventId\n   ,ce.encryptedConversationId\n   ,(isImported = '1') as isEmail\nFROM conversationEvents ce\njoin convsConversationCreation cc on  ce.encryptedConversationId = cc.encryptedConversationId \n\nWHERE ce.unixtime BETWEEN IMHOTEP_UNIXTIME('181d') AND IMHOTEP_UNIXTIME('1d')\nand   cc.unixtime BETWEEN IMHOTEP_UNIXTIME('195d') AND IMHOTEP_UNIXTIME('1d')\nand ((ce.unixtime - cc.unixtime) < (60*60*24*14))\nand ce.context='APPLICATION'\nand ce.ownerRole != 'JOBSEEKER'\nGROUP BY 1,2,3,4\n)\n\n\n, agg as (\n\nselect advertiserId\n      ,count(*) as totalMessages\n      ,count_if(isEmail) as emailMessages\n      ,count_if(NOT isEmail) as onPlatformMessages \n  from convos\ngroup by 1 \n\n\t\t)\n        \n,advertiserDetails as ( \n\nselect advertiser_id\n   , max_by(parent_company_size_segment, unixtime) as parent_company_size_segment\n   , max_by(type, unixtime) as type\n   , max_by(billing_country, unixtime) as billing_country\n  from  daily_employer2 \n  where unixtime BETWEEN IMHOTEP_UNIXTIME('2d') and IMHOTEP_UNIXTIME('1d')\ngroup by 1\n\n   )\n        \n\n select \n            advertiserDetails.parent_company_size_segment         \n\t\t   , case when emailMessages> 0 and onPlatformMessages=0 then 'Email Only'\n\t\t\t when  emailMessages=0 and onPlatformMessages>0 then 'Chat Only'\n\t\t\t when  emailMessages>0 and onPlatformMessages>0 then 'Email & Chat'\n             end as usageType\n        ,count(distinct advertiserId) as empCount\n        ,100.0*count(distinct advertiserId)/sum(count(distinct advertiserId)) over(partition by parent_company_size_segment) as pctEmpCountWithinSegment\n\n  from agg\nleft join advertiserDetails on agg.advertiserId=advertiserDetails.advertiser_id\nwhere parent_company_size_segment is not null and parent_company_size_segment!='NULL'\ngroup by 1,2\norder by 1\n        \n\n\n\n\n",
  "queryTables" : [ "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.convsconversationcreation", "skipperhive.imhotep.daily_employer2" ],
  "queryIndex" : 75,
  "runStartToQueryComplete" : 474
}, {
  "elapsedMillis" : 1225,
  "totalScheduledMillis" : 1358,
  "cpuMillis" : 229,
  "queuedMillis" : 0,
  "executeMillis" : 773,
  "getResultMillis" : 0,
  "iterateMillis" : 467,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 4553540,
  "query" : "--Presto exportedVolume matches IQL\nSELECT SUM(exportedVolume) as exportedVolume\nFROM datalake.imhotep.dradisCandidateExportRequest\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('100d') AND IMHOTEP_UNIXTIME('today')\nAND advertiserid = 73440070",
  "queryTables" : [ "datalakehive.imhotep.dradiscandidateexportrequest" ],
  "queryIndex" : 76,
  "runStartToQueryComplete" : 491
}, {
  "elapsedMillis" : 2964,
  "totalScheduledMillis" : 1902339,
  "cpuMillis" : 219631,
  "queuedMillis" : 1,
  "executeMillis" : 1387,
  "getResultMillis" : 0,
  "iterateMillis" : 1676,
  "rows" : 2425,
  "error" : null,
  "scannedBytes" : 10522326154,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('hour', from_unixtime(lp.unixtime)) as date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-30')\n        and True\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date_trunc('hour', from_unixtime(lp.unixtime)),\n\t\tappName,         \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n\t\tappName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 6 as double) / cast(1 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1))) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n\tappName,     \n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,\n    \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 6), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 1 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 6)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 77,
  "runStartToQueryComplete" : 528
}, {
  "elapsedMillis" : 3074,
  "totalScheduledMillis" : 2034576,
  "cpuMillis" : 187905,
  "queuedMillis" : 1,
  "executeMillis" : 1278,
  "getResultMillis" : 0,
  "iterateMillis" : 1817,
  "rows" : 555,
  "error" : null,
  "scannedBytes" : 10159305819,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('hour', from_unixtime(lp.unixtime)) as date,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-30')\n        and True\n    GROUP BY\n        date_trunc('hour', from_unixtime(lp.unixtime)),\n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 6 as double) / cast(1 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1))) as change,\n        sum(1.0) over (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count             \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n\t\traw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n         \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \nchange,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,        \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 6), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 1 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 6)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 78,
  "runStartToQueryComplete" : 528
}, {
  "elapsedMillis" : 6327,
  "totalScheduledMillis" : 3322043,
  "cpuMillis" : 1675014,
  "queuedMillis" : 1,
  "executeMillis" : 2781,
  "getResultMillis" : 0,
  "iterateMillis" : 3590,
  "rows" : 3,
  "error" : null,
  "scannedBytes" : 79733025059,
  "query" : "with convos as (\nselect\n    ce.advertiserId\n   ,eventId\n   ,ce.encryptedConversationId\n   ,(isImported = '1') as isEmail\nFROM conversationEvents ce\njoin convsConversationCreation cc on  ce.encryptedConversationId = cc.encryptedConversationId \n\nWHERE ce.unixtime BETWEEN IMHOTEP_UNIXTIME('181d') AND IMHOTEP_UNIXTIME('1d')\nand   cc.unixtime BETWEEN IMHOTEP_UNIXTIME('195d') AND IMHOTEP_UNIXTIME('1d')\nand ((ce.unixtime - cc.unixtime) < (60*60*24*14))\nand ce.context='APPLICATION'\nand ce.ownerRole != 'JOBSEEKER'\nGROUP BY 1,2,3,4\n)\n\n\n, agg as (\n\nselect advertiserId\n      ,count(*) as totalMessages\n      ,count_if(isEmail) as emailMessages\n      ,count_if(NOT isEmail) as onPlatformMessages \n  from convos\ngroup by 1 \n\n\t\t)\n\n select case when emailMessages> 0 and onPlatformMessages=0 then 'Email Only'\n\t\t\t when  emailMessages=0 and onPlatformMessages>0 then 'Chat Only'\n\t\t\t when  emailMessages>0 and onPlatformMessages>0 then 'Email & Chat'\n             end as usageType\n        ,count(distinct advertiserId) as empCount\n        ,100.0*count(distinct advertiserId)/sum(count(distinct advertiserId)) over() as pctEmpCount\n\n  from agg\ngroup by 1 \n\n\n\n\n",
  "queryTables" : [ "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.convsconversationcreation" ],
  "queryIndex" : 79,
  "runStartToQueryComplete" : 539
}, {
  "elapsedMillis" : 1496,
  "totalScheduledMillis" : 541,
  "cpuMillis" : 107,
  "queuedMillis" : 1,
  "executeMillis" : 1503,
  "getResultMillis" : 0,
  "iterateMillis" : 5,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 80,
  "runStartToQueryComplete" : 584
}, {
  "elapsedMillis" : 1260,
  "totalScheduledMillis" : 305,
  "cpuMillis" : 96,
  "queuedMillis" : 1,
  "executeMillis" : 835,
  "getResultMillis" : 0,
  "iterateMillis" : 439,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 81,
  "runStartToQueryComplete" : 585
}, {
  "elapsedMillis" : 3515,
  "totalScheduledMillis" : 957403,
  "cpuMillis" : 149072,
  "queuedMillis" : 1,
  "executeMillis" : 2369,
  "getResultMillis" : 0,
  "iterateMillis" : 1168,
  "rows" : 1121,
  "error" : null,
  "scannedBytes" : 8491745293,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('minute', from_unixtime(lp.unixtime)) as date,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-29')\n        and moderationApi='openAIModerations'\n    GROUP BY\n        date_trunc('minute', from_unixtime(lp.unixtime)),\n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 240 as double) / cast(30 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30))) as change,\n        sum(1.0) over (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count             \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n\t\traw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n         \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \nchange,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,        \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 240), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 15 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 240)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 82,
  "runStartToQueryComplete" : 593
}, {
  "elapsedMillis" : 2581,
  "totalScheduledMillis" : 1318562,
  "cpuMillis" : 176939,
  "queuedMillis" : 1,
  "executeMillis" : 1106,
  "getResultMillis" : 0,
  "iterateMillis" : 1493,
  "rows" : 2842,
  "error" : null,
  "scannedBytes" : 8789848517,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('minute', from_unixtime(lp.unixtime)) as date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-29')\n        and moderationApi='openAIModerations'\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date_trunc('minute', from_unixtime(lp.unixtime)),\n\t\tappName,         \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n\t\tappName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 240 as double) / cast(30 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30))) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n\tappName,     \n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,\n    \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 240), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 15 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 240)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 83,
  "runStartToQueryComplete" : 593
}, {
  "elapsedMillis" : 73592,
  "totalScheduledMillis" : 136056402,
  "cpuMillis" : 26255812,
  "queuedMillis" : 1,
  "executeMillis" : 10839,
  "getResultMillis" : 0,
  "iterateMillis" : 62803,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 548785261423,
  "query" : "WITH advids AS (\n    -- get all advertisers in our test and control groups\n    SELECT de2.advertiser_id\n    FROM datalake.imhotep.daily_employer2 de2\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('3d') AND IMHOTEP_UNIXTIME('2d')\n        AND de2.billing_country = 'GB'\n        AND de2.type != 'Test'\n)\n\n, jobs_raw AS (\n\t-- get all of the live jobs for our advertisers\n\tSELECT CASE WHEN CARDINALITY(FILTER(ratgs_lite, x -> x LIKE '%freevalue_so_job_copies_gb1'))>0\n             THEN 'Test' ELSE 'Control' END as experiment_group\n\t     , advertiser_id\n\t     , job_hash\n\t     , job_city\n\t     , job_state as job_admin_1_code\n\t     , (is_job_searchable+impressions+clicks+apply_starts)>0 as is_live\n\t     , unixtime\n    FROM datalake.imhotep.jobactivitymetrics as jam\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-23') AND IMHOTEP_UNIXTIME('today')\n        AND job_country_code='GB'\n        AND job_city IS NOT NULL\n        AND job_state IS NOT NULL\n        AND CARDINALITY(FILTER(ratgs_lite, x -> x LIKE '%B%:freevalue_so_job_copies_gb%'))>0\n)\n\n, jobs_raw_w_dj2 AS (\n    -- reduce to only jobs that were created or updated during the test window\n    SELECT jam.experiment_group\n\t     , jam.advertiser_id\n\t     , jam.job_hash\n\t     , jam.job_city\n\t     , jam.job_admin_1_code\n\t     , jam.is_live\n\t     , jam.unixtime\n    FROM jobs_raw jam\n    JOIN  datalake.imhotep.dradis_job2 as dj2 ON (dj2.job_hash_underscore = jam.job_hash)\n    WHERE dj2.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-23') AND IMHOTEP_UNIXTIME('today')\n        AND (dj2.date_created>=20240923\n            OR (previous_status IN ('PAUSED','DELETED') AND status='ACTIVE'))\n)\n\n, ordered_jobs AS (\n    SELECT experiment_group\n\t     , advertiser_id\n\t     , job_hash\n         , job_city\n\t     , job_admin_1_code\n         , unixtime\n\t     , is_live\n\t     , LAG(is_live) OVER (PARTITION BY job_hash, experiment_group ORDER BY unixtime) AS prev_is_live\n\t     , LAG(unixtime) OVER (PARTITION BY job_hash, experiment_group ORDER BY unixtime) AS prev_unixtime\n    FROM jobs_raw_w_dj2\n)\n\n, jobs_w_live_periods AS (\n    SELECT experiment_group\n\t     , advertiser_id\n\t     , job_hash\n         , job_city\n\t     , job_admin_1_code\n         , unixtime as live_start_date\n         , LEAD(unixtime, 1, TO_UNIXTIME(CURRENT_DATE)) OVER (PARTITION BY job_hash, experiment_group ORDER BY unixtime) AS live_end_date\n    FROM ordered_jobs\n    WHERE is_live = true\n        AND (prev_is_live IS NULL OR prev_is_live = false)\n)\n\n, jobs_w_live_periods_copy_attrs AS (\n    -- get the fields we need for copy comparison\n    SELECT experiment_group\n         , jca.advertiser_id\n         , job_hash\n         , job_on_indeed_unixtime\n         , jca.job_city\n         , jca.job_admin_1_code\n         , job_type_id\n         , job_normalized_title_sim_hash\n         , job_description_sim_hash\n         , taxo_occupations_most_specific_suid\n         , job_occupations\n         , jwlp.live_start_date\n         , jwlp.live_end_date\n    FROM jobs_w_live_periods jwlp\n    JOIN datalake.imhotep.job_copy_attributes as jca USING (job_hash)\n    WHERE jca.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-23') AND IMHOTEP_UNIXTIME('today')\n    AND jca.job_admin_1_code IS NOT NULL\n    AND jca.job_city IS NOT NULL\n)\n\n, live_jobs_w_copy_pair AS (\n    SELECT orig.experiment_group\n        , orig.advertiser_id as advertiser_id\n        , orig.job_hash as job_hash\n        , copy.job_hash as copy_job_hash\n        , orig.job_city\n        , orig.job_admin_1_code\n        , orig.live_start_date as orig_live_start_date\n        , orig.live_end_date as orig_live_end_date\n        , copy.live_start_date as copy_live_start_date\n        , copy.live_end_date as copy_live_end_date\n        , GREATEST(orig.live_start_date, copy.live_start_date) AS overlap_start\n        , LEAST(orig.live_end_date, copy.live_end_date) AS overlap_end\n        , orig.live_start_date <= copy.live_end_date AND copy.live_start_date <= orig.live_end_date as has_overlap -- original and copy were live at the same time\n         -- split the 128-bit simhash into two 64-bit representations\n        , cast( from_big_endian_64(lpad(substr(from_base64(orig.job_description_sim_hash), 1, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as orig_high\n        , cast( from_big_endian_64(lpad(substr(from_base64(orig.job_description_sim_hash), 9, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as orig_low\n        , cast( from_big_endian_64(lpad(substr(from_base64(copy.job_description_sim_hash), 1, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as copy_high\n        , cast( from_big_endian_64(lpad(substr(from_base64(copy.job_description_sim_hash), 9, 8), 8, from_base64('AAAAAAAAAAA='))) as bigint) as copy_low\n    FROM jobs_w_live_periods_copy_attrs orig\n    LEFT JOIN jobs_w_live_periods_copy_attrs copy ON (\n        orig.experiment_group = copy.experiment_group\n        AND orig.advertiser_id = copy.advertiser_id\n        AND orig.job_city = copy.job_city\n        AND orig.job_admin_1_code = copy.job_admin_1_code\n        AND orig.job_hash != copy.job_hash\n        AND orig.job_on_indeed_unixtime < copy.job_on_indeed_unixtime -- original must be created before the copy\n\n        AND bitwise_or(orig.job_type_id, copy.job_type_id) > 0 -- job_type_id any match\n\n        -- compare normalized titles when they are both not null. otherwise, ignore them\n        AND CASE WHEN orig.job_normalized_title_sim_hash IS NOT NULL AND copy.job_normalized_title_sim_hash IS NOT NULL THEN orig.job_normalized_title_sim_hash = copy.job_normalized_title_sim_hash ELSE TRUE END\n\n        -- compare occupation when they are both not null. otherwise, ignore them\n        AND CASE WHEN orig.taxo_occupations_most_specific_suid IS NOT NULL AND CARDINALITY(copy.job_occupations) > 0 THEN CONTAINS(copy.job_occupations, orig.taxo_occupations_most_specific_suid) ELSE TRUE END\n        )\n)\n\n, live_jobs_final AS (\n    SELECT *\n         -- the right job is a copy if we have a copy_job_hash defined and the description hamming distance is <= 7\n         , copy_job_hash IS NOT NULL AND bit_count(bitwise_xor(orig_high, copy_high), 64) + bit_count(bitwise_xor(orig_low, copy_low), 64) <= 7 as has_copy\n        , (overlap_end - overlap_start) / 86400 as overlap_days\n        , (orig_live_end_date - orig_live_start_date) / 86400 as orig_days_live\n        , (copy_live_end_date - copy_live_start_date) / 86400 as copy_days_live\n    FROM live_jobs_w_copy_pair\n)\n\n\nSELECT experiment_group\n     , COUNT(DISTINCT advertiser_id) as advertisers\n     , COUNT(DISTINCT job_hash) as all_live_jobs\n     , COUNT(DISTINCT CASE WHEN has_copy THEN copy_job_hash END) as live_copies\n     , COUNT(DISTINCT CASE WHEN has_copy AND has_overlap THEN copy_job_hash END) as live_copies_w_overlap -- there was a point when original and copy were both live at the same imt\n     , COUNT(DISTINCT CASE WHEN has_copy AND has_overlap = FALSE THEN copy_job_hash END) as live_copies_no_overlap -- there was a point when original and copy were not live at the same time\n     , AVG(CASE WHEN has_copy AND has_overlap THEN overlap_days ELSE NULL END) as avg_days_overlap\n     , AVG(CASE WHEN has_copy THEN orig_days_live ELSE NULL END) as orig_avg_days_live\n     , AVG(CASE WHEN has_copy THEN copy_days_live ELSE NULL END) as copy_avg_days_live\n     -- subtract the count of copy jobs from all jobs to get the total unique jobs live\n     , COUNT(DISTINCT job_hash) - COUNT(DISTINCT CASE WHEN has_copy THEN copy_job_hash END) as unique_live_jobs\nFROM live_jobs_final\nGROUP BY 1\nORDER BY 1 ASC",
  "queryTables" : [ "datalakehive.imhotep.dradis_job2", "datalakehive.imhotep.dradis_job2", "datalakehive.imhotep.job_copy_attributes", "datalakehive.imhotep.job_copy_attributes", "datalakehive.imhotep.jobactivitymetrics", "datalakehive.imhotep.jobactivitymetrics" ],
  "queryIndex" : 84,
  "runStartToQueryComplete" : 664
}, {
  "elapsedMillis" : 1889,
  "totalScheduledMillis" : 1483,
  "cpuMillis" : 371,
  "queuedMillis" : 1,
  "executeMillis" : 1546,
  "getResultMillis" : 0,
  "iterateMillis" : 356,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 5535097,
  "query" : "--Presto exportedVolume matches IQL\nSELECT yyyymmdd, hhmm, SUM(exportedVolume) as exportedVolume\nFROM datalake.imhotep.dradisCandidateExportRequest\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('100d') AND IMHOTEP_UNIXTIME('today')\nAND advertiserid = 73440070\nGROUP BY yyyymmdd, hhmm\nORDER BY yyyymmdd, hhmm",
  "queryTables" : [ "datalakehive.imhotep.dradiscandidateexportrequest" ],
  "queryIndex" : 85,
  "runStartToQueryComplete" : 596
}, {
  "elapsedMillis" : 2423,
  "totalScheduledMillis" : 11042,
  "cpuMillis" : 1601,
  "queuedMillis" : 0,
  "executeMillis" : 977,
  "getResultMillis" : 0,
  "iterateMillis" : 1472,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 21334022,
  "query" : "WITH cte_1 as (\n\tSELECT \n    advertiserid,\n    COUNT(DISTINCT(id)) as unique_id_count\n    \nFROM \n\tdatalake.imhotep.dradisCandidateExportRequest\n\tCROSS JOIN UNNEST(split(candidateIds, ',')) AS t(id)\n    \nWHERE \n\tunixtime BETWEEN IMHOTEP_UNIXTIME('100d') AND IMHOTEP_UNIXTIME('today')\n\tAND advertiserid = 73440070\n    \nGROUP BY \n\tadvertiserid\n), \n\ncte_2 as (\nSELECT \n\tadvertiserid,\n    SUM(exportedVolume) as exportVolume\n    \nFROM \n\tdatalake.imhotep.dradisCandidateExportRequest\n    \nWHERE \n\tunixtime BETWEEN IMHOTEP_UNIXTIME('2024-07-01') AND IMHOTEP_UNIXTIME('today')\n\tAND advertiserid = 73440070\n    \nGROUP BY \n\tadvertiserid\n)\n\nSELECT\t\n\tcte_1.advertiserid,\n    unique_id_count,\n    exportVolume\n    \nFROM \n\tcte_1 INNER JOIN cte_2 ON cte_1.advertiserid = cte_2.advertiserid\n",
  "queryTables" : [ "datalakehive.imhotep.dradiscandidateexportrequest", "datalakehive.imhotep.dradiscandidateexportrequest" ],
  "queryIndex" : 86,
  "runStartToQueryComplete" : 604
}, {
  "elapsedMillis" : 3096,
  "totalScheduledMillis" : 9129,
  "cpuMillis" : 2150,
  "queuedMillis" : 0,
  "executeMillis" : 862,
  "getResultMillis" : 0,
  "iterateMillis" : 2275,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 19733722,
  "query" : "WITH cte_1 as (\n\tSELECT \n    advertiserid,\n\tyyyymmdd, \n    hhmm, \n    COUNT(DISTINCT(id)) as unique_id_count\n    \nFROM \n\tdatalake.imhotep.dradisCandidateExportRequest\n\tCROSS JOIN UNNEST(split(candidateIds, ',')) AS t(id)\n    \nWHERE \n\tunixtime BETWEEN IMHOTEP_UNIXTIME('100d') AND IMHOTEP_UNIXTIME('today')\n\tAND advertiserid = 73440070\n    \nGROUP BY \n\tadvertiserid, yyyymmdd, hhmm \n    \nORDER BY \n\tyyyymmdd, hhmm\n), \n\ncte_2 as (\nSELECT \n\tadvertiserid,\n\tyyyymmdd, \n    hhmm, \n    SUM(exportedVolume) as exportVolume\n    \nFROM \n\tdatalake.imhotep.dradisCandidateExportRequest\n    \nWHERE \n\tunixtime BETWEEN IMHOTEP_UNIXTIME('100d') AND IMHOTEP_UNIXTIME('today')\n\tAND advertiserid = 73440070\n    \nGROUP BY \n\tadvertiserid, yyyymmdd, hhmm \n    \nORDER BY \n\tyyyymmdd, hhmm\n)\n\nSELECT\t\n\tcte_1.advertiserid,\n\tcte_1.yyyymmdd, \n    cte_1.hhmm, \n    unique_id_count,\n    exportVolume\n    \nFROM \n\tcte_1 INNER JOIN cte_2 ON cte_1.advertiserid = cte_2.advertiserid\n    \tAND cte_1.yyyymmdd = cte_2.yyyymmdd\n\t\tAND cte_1.hhmm = cte_2.hhmm\n\nORDER BY \n\tyyyymmdd, hhmm",
  "queryTables" : [ "datalakehive.imhotep.dradiscandidateexportrequest", "datalakehive.imhotep.dradiscandidateexportrequest" ],
  "queryIndex" : 87,
  "runStartToQueryComplete" : 605
}, {
  "elapsedMillis" : 2575,
  "totalScheduledMillis" : 155126,
  "cpuMillis" : 9289,
  "queuedMillis" : 0,
  "executeMillis" : 1412,
  "getResultMillis" : 0,
  "iterateMillis" : 1178,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 256810749,
  "query" : "select count(1) from clickanalytics where unixtime between imhotep_unixtime('2019-03-01') and imhotep_unixtime('2019-03-02')",
  "queryTables" : [ "skipperhive.imhotep.clickanalytics" ],
  "queryIndex" : 88,
  "runStartToQueryComplete" : 606
}, {
  "elapsedMillis" : 3252,
  "totalScheduledMillis" : 7025,
  "cpuMillis" : 1841,
  "queuedMillis" : 0,
  "executeMillis" : 787,
  "getResultMillis" : 0,
  "iterateMillis" : 2488,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 14198625,
  "query" : "--Presto exportedVolume matches IQL\nSELECT \n\tyyyymmdd, \n    hhmm, \n    candidateIds, \n    COUNT(DISTINCT(id)) as id_count\n    \nFROM \n\tdatalake.imhotep.dradisCandidateExportRequest\n\tCROSS JOIN UNNEST(split(candidateIds, ',')) AS t(id)\n    \nWHERE \n\tunixtime BETWEEN IMHOTEP_UNIXTIME('100d') AND IMHOTEP_UNIXTIME('today')\n\tAND advertiserid = 73440070\n    \nGROUP BY \n\tyyyymmdd, hhmm, candidateIds \n    \nORDER BY \n\tyyyymmdd, hhmm\n    \nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.dradiscandidateexportrequest" ],
  "queryIndex" : 89,
  "runStartToQueryComplete" : 607
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 797,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_000258_00015_mzvde): Division by zero",
  "scannedBytes" : 0,
  "query" : "SELECT count(DISTINCT(campaignId)) as distinct_count, COUNT() as total_count, COUNT()/count(DISTINCT(campaignId)) as count_ratio\nFROM\n(\nSELECT SUBSTRING(_key, 1, 23) as campaignId\nFROM datalake.imhotep.campaign_job_mapping\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2d') AND IMHOTEP_UNIXTIME('1d')\n)\n\n\n",
  "queryTables" : [ "datalakehive.imhotep.campaign_job_mapping" ],
  "queryIndex" : 90,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 88105,
  "totalScheduledMillis" : 391163662,
  "cpuMillis" : 51630279,
  "queuedMillis" : 1,
  "executeMillis" : 38034,
  "getResultMillis" : 0,
  "iterateMillis" : 50094,
  "rows" : 54,
  "error" : null,
  "scannedBytes" : 3857432780892,
  "query" : "SELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n, (case when feedid = 50461 then 'Hosted' else 'Indexed' end) as jobProduct\n--, count(DISTINCT(jobId)) as totalJobs\n, count(DISTINCT(case when has_employer_base_pay = 1 then jobId else null end)) as totalExplicitPayJobs\n, count(DISTINCT(case when jlpostal != '' OR jlenhanced = 1 then jobId else null end)) as totalBetterLocationJobs\n, count(DISTINCT(case when contains(applyvisibility, 'mobile') then jobId else null end)) as totalIndeedApplyableJobs\n, count(DISTINCT(case when has_employer_base_pay = 1 AND contains(applyvisibility, 'mobile') AND (jlpostal != '' OR jlenhanced = 1) then jobId else null end)) as totalBetterJobs\n-- logic for the above fields comes from https://link.indeed.tech/YD476T \nFROM datalake.imhotep.searchablejobs\nWHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \nAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\nAND (waldovisibilitylevel in ('organic','jobalert')\n\tOR (waldovisibilitylevel = 'sponsored' AND sponvisibility = 'spon_active'))\nAND jobcountry != 'JP'\nGROUP BY 1,2",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 91,
  "runStartToQueryComplete" : 714
}, {
  "elapsedMillis" : 1248,
  "totalScheduledMillis" : 689,
  "cpuMillis" : 33,
  "queuedMillis" : 1,
  "executeMillis" : 382,
  "getResultMillis" : 0,
  "iterateMillis" : 880,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 4699038,
  "query" : "select rsvp_accountid,\n       rsvp_status,\n       rsvp_uuid,\n       unixtime,\n       disposition,\n       jobseeker_show_time_delta,\n       employer_show_time_delta,\n       event_client_id\nFROM imhotep.interviewrecord\nWHERE day >= '2022-12-01' AND day < '2022-12-02'\n  and event_client_id IN ('api_events')\nLIMIT 100",
  "queryTables" : [ "skipperhive.imhotep.interviewrecord" ],
  "queryIndex" : 92,
  "runStartToQueryComplete" : 637
}, {
  "elapsedMillis" : 7187,
  "totalScheduledMillis" : 707377,
  "cpuMillis" : 89964,
  "queuedMillis" : 0,
  "executeMillis" : 3141,
  "getResultMillis" : 0,
  "iterateMillis" : 4069,
  "rows" : 23,
  "error" : null,
  "scannedBytes" : 10748917508,
  "query" : "SELECT\n    DATE_TRUNC('month', activity_time) AS activity_month,\n    is_ai_outreach,\n    SUM(contact_sent) AS contact_count,\n    SUM(positive_response) AS positive_responses_count,\n    (SUM(positive_response) * 1.00) / SUM(contact_sent) AS pos_resp_rate\nFROM\n    datalake.employer_analytics_platform.xpa_fct_sourcing_engagement\nWHERE \n\tactivity_time >= DATE('2024-01-01')\n    --AND subscription_tier IN ('Standard')\n    AND contact_sent > 0\n    AND advertiser_id = 47908\nGROUP BY 1, 2\nORDER BY 1, 2",
  "queryTables" : [ "datalakehive.employer_analytics_platform.xpa_fct_sourcing_engagement" ],
  "queryIndex" : 93,
  "runStartToQueryComplete" : 647
}, {
  "elapsedMillis" : 5157,
  "totalScheduledMillis" : 157563,
  "cpuMillis" : 33836,
  "queuedMillis" : 0,
  "executeMillis" : 1038,
  "getResultMillis" : 0,
  "iterateMillis" : 4139,
  "rows" : 7,
  "error" : null,
  "scannedBytes" : 3821617499,
  "query" : "SELECT\n    DATE_TRUNC('month', activity_time) AS activity_month,\n    is_ai_outreach,\n    SUM(contact_sent) AS contact_count,\n    SUM(positive_response) AS positive_responses_count,\n    (SUM(positive_response) * 1.00) / SUM(contact_sent) AS pos_resp_rate\nFROM\n    datalake.employer_analytics_platform.xpa_fct_sourcing_engagement\nWHERE \n\tactivity_time >= DATE('2024-08-01')\n    --AND subscription_tier IN ('Standard')\n    AND contact_sent > 0\n    AND advertiser_id = 16775855\nGROUP BY 1, 2\nORDER BY 1, 2",
  "queryTables" : [ "datalakehive.employer_analytics_platform.xpa_fct_sourcing_engagement" ],
  "queryIndex" : 94,
  "runStartToQueryComplete" : 663
}, {
  "elapsedMillis" : 11278,
  "totalScheduledMillis" : 309450,
  "cpuMillis" : 103907,
  "queuedMillis" : 0,
  "executeMillis" : 972,
  "getResultMillis" : 0,
  "iterateMillis" : 10320,
  "rows" : 12,
  "error" : null,
  "scannedBytes" : 7525409025,
  "query" : "SELECT\n    DATE_TRUNC('month', activity_time) AS activity_month,\n    is_ai_outreach,\n    SUM(contact_sent) AS contact_count,\n    SUM(positive_response) AS positive_responses_count,\n    (SUM(positive_response) * 1.00) / SUM(contact_sent) AS pos_resp_rate\nFROM\n    datalake.employer_analytics_platform.xpa_fct_sourcing_engagement\nWHERE \n\tactivity_time >= DATE('2024-04-01')\n    --AND subscription_tier IN ('Standard')\n    AND contact_sent > 0\n    AND advertiser_id = 16775855\nGROUP BY 1, 2\nORDER BY 1, 2",
  "queryTables" : [ "datalakehive.employer_analytics_platform.xpa_fct_sourcing_engagement" ],
  "queryIndex" : 95,
  "runStartToQueryComplete" : 698
}, {
  "elapsedMillis" : 432603,
  "totalScheduledMillis" : 404127,
  "cpuMillis" : 317606,
  "queuedMillis" : 0,
  "executeMillis" : 1108,
  "getResultMillis" : 0,
  "iterateMillis" : 431550,
  "rows" : 305845555,
  "error" : null,
  "scannedBytes" : 420703193,
  "query" : "  SELECT \n  \tadvertiserid as advertiser_id\n    , t.permission as product_perm\n  FROM \n  \tdatalake.imhotep.advertiserusers  \n  \tCROSS JOIN UNNEST (permissionSetNames) as T (permission)\n  WHERE \n  \tunixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n",
  "queryTables" : [ "datalakehive.imhotep.advertiserusers" ],
  "queryIndex" : 96,
  "runStartToQueryComplete" : 1120
}, {
  "elapsedMillis" : 7387,
  "totalScheduledMillis" : 28641,
  "cpuMillis" : 14481,
  "queuedMillis" : 0,
  "executeMillis" : 3995,
  "getResultMillis" : 0,
  "iterateMillis" : 5483,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 78063498,
  "query" : "SELECT *\nFROM logrepo.log.jsapisearch\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-20 11:00:00') AND IMHOTEP_UNIXTIME('2024-10-20 12:00:00')\nLIMIT 10",
  "queryTables" : [ "logrepo.log.jsapisearch" ],
  "queryIndex" : 97,
  "runStartToQueryComplete" : 705
}, {
  "elapsedMillis" : 5466,
  "totalScheduledMillis" : 1460795,
  "cpuMillis" : 151103,
  "queuedMillis" : 1,
  "executeMillis" : 1101,
  "getResultMillis" : 4,
  "iterateMillis" : 4377,
  "rows" : 655,
  "error" : null,
  "scannedBytes" : 8498189747,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('minute', from_unixtime(lp.unixtime)) as date,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-29')\n        and moderationApi='openAIModerations' and moderationType='input'\n    GROUP BY\n        date_trunc('minute', from_unixtime(lp.unixtime)),\n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 240 as double) / cast(30 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30))) as change,\n        sum(1.0) over (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count             \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n\t\traw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n         \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \nchange,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,        \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 240), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 15 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 240)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 98,
  "runStartToQueryComplete" : 703
}, {
  "elapsedMillis" : 6109,
  "totalScheduledMillis" : 1590953,
  "cpuMillis" : 250928,
  "queuedMillis" : 1,
  "executeMillis" : 1153,
  "getResultMillis" : 0,
  "iterateMillis" : 4969,
  "rows" : 1417,
  "error" : null,
  "scannedBytes" : 8801500784,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('minute', from_unixtime(lp.unixtime)) as date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-29')\n        and moderationApi='openAIModerations' and moderationType='input'\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date_trunc('minute', from_unixtime(lp.unixtime)),\n\t\tappName,         \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n\t\tappName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 240 as double) / cast(30 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30))) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n\tappName,     \n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,\n    \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 240), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 15 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 240)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 99,
  "runStartToQueryComplete" : 706
}, {
  "elapsedMillis" : 975,
  "totalScheduledMillis" : 3596,
  "cpuMillis" : 240,
  "queuedMillis" : 1,
  "executeMillis" : 790,
  "getResultMillis" : 0,
  "iterateMillis" : 268,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 1278917,
  "query" : "  SELECT \n  \tadvertiserid as advertiser_id\n    , t.permission as product_perm\n  FROM \n  \tdatalake.imhotep.advertiserusers  \n  \tCROSS JOIN UNNEST (permissionSetNames) as T (permission)\n  WHERE \n  \tunixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 100",
  "queryTables" : [ "datalakehive.imhotep.advertiserusers" ],
  "queryIndex" : 100,
  "runStartToQueryComplete" : 705
}, {
  "elapsedMillis" : 3095,
  "totalScheduledMillis" : 128955,
  "cpuMillis" : 15677,
  "queuedMillis" : 1,
  "executeMillis" : 2027,
  "getResultMillis" : 0,
  "iterateMillis" : 1103,
  "rows" : 73,
  "error" : null,
  "scannedBytes" : 2096292364,
  "query" : "--weekly view of email csat send volume\nSELECT \n    sum(postsuccess) + sum(blockedcount) as total_sends,\n    sum(postsuccess) as successful_sends, \n    sum(opened) as opened_sends, \n    sum(blockedcount) as blocked_sends, \n    sum(postsuccess)/(sum(CAST(postsuccess as DOUBLE)) + sum(CAST(blockedcount as Double))) as send_success_rate,\n    sum(opened)/sum(CAST(postsuccess as DOUBLE)) as open_rate_of_successful_sends, \n    team_market_endq,\n    send_date\nFROM(\n    SELECT a.campaign, a.advertiserid, a.opened, a.postsuccess, a.blockedcount, a.id, b.full_name, b.team_market_endq, b.report_date, a.send_date\n    FROM (\n        SELECT \n        \t    date(FROM_UNIXTIME(unixtime)) as send_date,\n            campaign,\n            optionalidentifier,\n            advertiserid,\n            opened,\n            postsuccess,\n            blockedcount, \n            case\n                when campaign like '%VENDOR%' then json_extract_scalar(optionalidentifier, '$.agent_id')\n                else json_extract_scalar(optionalidentifier, '$.csRepId')\n                \n            end as id\n        FROM datalake.imhotep.ravenemail a\n        WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1w') AND IMHOTEP_UNIXTIME('today')\n        \n        and (campaign like '%SHARED%' or campaign like '%VENDOR%')\n        group by 1,2,3,4,5,6,7\n    ) a\n    LEFT JOIN datalake.sbs_global.sbs_opt_full_roster b\n    ON \n    cast(a.id as INTEGER) = CAST(b.rep_id as INTEGER)\n    WHERE b.report_date = date_add('day', -2, current_date)\n)\nGROUP BY 7,8\nORDER BY 1 desc\n\n",
  "queryTables" : [ "datalakehive.imhotep.ravenemail", "datalakehive.sbs_global.sbs_opt_full_roster" ],
  "queryIndex" : 101,
  "runStartToQueryComplete" : 714
}, {
  "elapsedMillis" : 1364,
  "totalScheduledMillis" : 6958,
  "cpuMillis" : 480,
  "queuedMillis" : 1,
  "executeMillis" : 749,
  "getResultMillis" : 0,
  "iterateMillis" : 1137,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 20735995,
  "query" : "SELECT *\nFROM datalake.imhotep.ruleContentResult_communications\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.rulecontentresult_communications" ],
  "queryIndex" : 102,
  "runStartToQueryComplete" : 723
}, {
  "elapsedMillis" : 6894,
  "totalScheduledMillis" : 202931,
  "cpuMillis" : 60735,
  "queuedMillis" : 0,
  "executeMillis" : 1141,
  "getResultMillis" : 0,
  "iterateMillis" : 5767,
  "rows" : 11,
  "error" : null,
  "scannedBytes" : 6525561285,
  "query" : "SELECT\n    DATE_TRUNC('month', activity_time) AS activity_month,\n    is_ai_outreach,\n    SUM(contact_sent) AS contact_count,\n    SUM(positive_response) AS positive_responses_count,\n    (SUM(positive_response) * 1.00) / SUM(contact_sent) AS pos_resp_rate\nFROM\n    datalake.employer_analytics_platform.xpa_fct_sourcing_engagement\nWHERE \n\tactivity_time >= DATE('2024-05-01')\n    --AND subscription_tier IN ('Standard')\n    AND contact_sent > 0\n    AND advertiser_id = 16775855\nGROUP BY 1, 2\nORDER BY 1, 2",
  "queryTables" : [ "datalakehive.employer_analytics_platform.xpa_fct_sourcing_engagement" ],
  "queryIndex" : 103,
  "runStartToQueryComplete" : 732
}, {
  "elapsedMillis" : 2514,
  "totalScheduledMillis" : 1122827,
  "cpuMillis" : 371312,
  "queuedMillis" : 1,
  "executeMillis" : 1638,
  "getResultMillis" : 0,
  "iterateMillis" : 954,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 2930870466,
  "query" : "with msa_jobs as (\n    select sj.jobid as agg_job_id\n    , jlmsa\n    , trim(jlmsa) as jlmsa_trimmed\n    , sum(case when trim(jlmsa) is not null then 1 else 0 end) AS urban_jobs\n    , sum(case when trim(jlmsa) is null then 1 else 0 end) AS rural_jobs\n    , max_by(trim(jlmsa), hour) as latest_msa\n    from datalake.imhotep.searchablejobs sj\n    where hour between date_format(from_unixtime(to_unixtime(date('2024-10-01'))), '%Y-%m-%d %H:%i:%s') and date_format(from_unixtime(to_unixtime(date('2024-10-02'))), '%Y-%m-%d %H:%i:%s')\n    group by 1,2,3\n)\n\nselect * from msa_jobs\nwhere rural_jobs>0\nlimit 100\n",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 104,
  "runStartToQueryComplete" : 732
}, {
  "elapsedMillis" : 4126,
  "totalScheduledMillis" : 127111,
  "cpuMillis" : 32667,
  "queuedMillis" : 0,
  "executeMillis" : 890,
  "getResultMillis" : 0,
  "iterateMillis" : 3259,
  "rows" : 7,
  "error" : null,
  "scannedBytes" : 3821617499,
  "query" : "SELECT\n    DATE_TRUNC('month', activity_time) AS activity_month,\n    is_ai_outreach,\n    SUM(contact_sent) AS contact_count,\n    SUM(positive_response) AS positive_responses_count,\n    (SUM(positive_response) * 1.00) / SUM(contact_sent) AS pos_resp_rate\nFROM\n    datalake.employer_analytics_platform.xpa_fct_sourcing_engagement\nWHERE \n\tactivity_time >= DATE('2024-08-01')\n    --AND subscription_tier IN ('Standard')\n    AND contact_sent > 0\n    AND advertiser_id = 16775855\nGROUP BY 1, 2\nORDER BY 1, 2",
  "queryTables" : [ "datalakehive.employer_analytics_platform.xpa_fct_sourcing_engagement" ],
  "queryIndex" : 105,
  "runStartToQueryComplete" : 755
}, {
  "elapsedMillis" : 1549,
  "totalScheduledMillis" : 843522,
  "cpuMillis" : 137982,
  "queuedMillis" : 0,
  "executeMillis" : 1563,
  "getResultMillis" : 0,
  "iterateMillis" : 12,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 2932740913,
  "query" : "with msa_jobs as (\n    select sj.jobid as agg_job_id\n    , jlmsa\n    , trim(jlmsa) as jlmsa_trimmed\n    , sum(case when trim(jlmsa) is not null then 1 else 0 end) AS urban_jobs\n    , sum(case when trim(jlmsa) is null then 1 else 0 end) AS rural_jobs\n    , max_by(trim(jlmsa), hour) as latest_msa\n    from datalake.imhotep.searchablejobs sj\n    where hour between date_format(from_unixtime(to_unixtime(date('2024-10-01'))), '%Y-%m-%d %H:%i:%s') and date_format(from_unixtime(to_unixtime(date('2024-10-02'))), '%Y-%m-%d %H:%i:%s')\n    group by 1,2,3\n)\n\nselect * from msa_jobs\nwhere rural_jobs>0 and jlmsa is not null\nlimit 100\n",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 106,
  "runStartToQueryComplete" : 759
}, {
  "elapsedMillis" : 26550,
  "totalScheduledMillis" : 31748048,
  "cpuMillis" : 11487959,
  "queuedMillis" : 0,
  "executeMillis" : 12128,
  "getResultMillis" : 0,
  "iterateMillis" : 14452,
  "rows" : 3,
  "error" : null,
  "scannedBytes" : 491593265902,
  "query" : "/*WITH\n\nsponsor_event as(\n  Select distinct advertiser_id \n  FROM datalake.imhotep.sponsormodulesevents2 \n  WHERE hour >= cast(date_add('day',-21,CURRENT_DATE) as varchar)\n  AND page='sponsorOrganic' \n  AND event='element_click' \n  AND element_name='cpc_sponsorship' \n  AND is_masquerading=1\n),*/\n\nWITH\n\nsbs_reps as (\n\tselect DISTINCT ldap\n\tFROM datalake.imhotep.indeedemployeesnapshot\n\tWHERE sup_org_name LIKE '%SBS%'\n    and day >= cast(date_add('day',-7,CURRENT_DATE) as varchar)\n)\n\nSELECT \ndate_trunc('day',cast(from_unixtime(unixtime) as timestamp)) as date\n, masquerading_user\n, advertiser_id\n, element\n, url_path\n\nFROM datalake.imhotep.employer_user_actions_spark\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  and custom_action_type like '%click%'\n\n  and is_masquerading = 1\n  and ((url_path = '/sponsor/edit/sponsor' and action_name='sponsored-button')\n  or (url_path = '/objective-campaign/create/confirmation' and element like 'save-campaign-button'))\n  \n  and masquerading_user in (Select distinct ldap FROM sbs_reps)\n  --  and advertiser_id = 773008\n\n  --  and ((url_path = '/sponsor/edit/sponsor'and action_name='sponsored-button') or (url_path = '/objective-campaign/create/confirmation' and action_name = 'save-campaign-button'))\n  --  and advertiser_id in (Select distinct advertiser_id FROM sponsor_event)\n\nGROUP BY 1,2,3,4,5\nORDER BY 1 desc",
  "queryTables" : [ "datalakehive.imhotep.employer_user_actions_spark", "datalakehive.imhotep.indeedemployeesnapshot" ],
  "queryIndex" : 107,
  "runStartToQueryComplete" : 787
}, {
  "elapsedMillis" : 17672,
  "totalScheduledMillis" : 4316442,
  "cpuMillis" : 1685057,
  "queuedMillis" : 0,
  "executeMillis" : 11295,
  "getResultMillis" : 0,
  "iterateMillis" : 6429,
  "rows" : 12,
  "error" : null,
  "scannedBytes" : 80368430066,
  "query" : "with convos as (\nselect\n    ce.advertiserId\n   ,eventId\n   ,ce.encryptedConversationId\n   ,(isImported = '1') as isEmail\nFROM conversationEvents ce\njoin convsConversationCreation cc on  ce.encryptedConversationId = cc.encryptedConversationId \n\nWHERE ce.unixtime BETWEEN IMHOTEP_UNIXTIME('181d') AND IMHOTEP_UNIXTIME('1d')\nand   cc.unixtime BETWEEN IMHOTEP_UNIXTIME('195d') AND IMHOTEP_UNIXTIME('1d')\nand ((ce.unixtime - cc.unixtime) < (60*60*24*14))\nand ce.context='APPLICATION'\nand ce.ownerRole != 'JOBSEEKER'\nGROUP BY 1,2,3,4\n)\n\n\n, agg as (\n\nselect advertiserId\n      ,count(*) as totalMessages\n      ,count_if(isEmail) as emailMessages\n      ,count_if(NOT isEmail) as onPlatformMessages \n  from convos\ngroup by 1 \n\n\t\t)\n        \n,advertiserDetails as ( \n\nselect advertiser_id\n   , max_by(parent_company_size_segment, unixtime) as parent_company_size_segment\n   , max_by(type, unixtime) as type\n   , max_by(billing_country, unixtime) as billing_country\n  from  daily_employer2 \n  where unixtime BETWEEN IMHOTEP_UNIXTIME('2d') and IMHOTEP_UNIXTIME('1d')\ngroup by 1\n\n   )\n        \n\n select \n            advertiserDetails.parent_company_size_segment         \n\t\t   , case when emailMessages> 0 and onPlatformMessages=0 then 'Email Only'\n\t\t\t when  emailMessages=0 and onPlatformMessages>0 then 'Chat Only'\n\t\t\t when  emailMessages>0 and onPlatformMessages>0 then 'Email & Chat'\n             end as usageType\n        ,count(distinct advertiserId) as empCount\n        ,100.0*count(distinct advertiserId)/sum(count(distinct advertiserId)) over(partition by parent_company_size_segment) as pctEmpCountWithinSegment\n\n  from agg\nleft join advertiserDetails on agg.advertiserId=advertiserDetails.advertiser_id\nwhere parent_company_size_segment is not null and parent_company_size_segment!='NULL'\ngroup by 1,2\norder by case parent_company_size_segment \n         when 'S' then 1 \n         when 'M' then 2\n         when 'L' then 3 \n         else 4 \n         end asc\n        \n\n\n\n\n",
  "queryTables" : [ "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.convsconversationcreation", "skipperhive.imhotep.daily_employer2" ],
  "queryIndex" : 108,
  "runStartToQueryComplete" : 799
}, {
  "elapsedMillis" : 1088,
  "totalScheduledMillis" : 4517,
  "cpuMillis" : 190,
  "queuedMillis" : 1,
  "executeMillis" : 896,
  "getResultMillis" : 0,
  "iterateMillis" : 290,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 1278917,
  "query" : "  SELECT \n  \tadvertiserid as advertiser_id\n    , t.permission as product_perm\n    , accountid as account_id\n  FROM \n  \tdatalake.imhotep.advertiserusers  \n  \tCROSS JOIN UNNEST (permissionSetNames) as T (permission)\n  WHERE \n  \tunixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 100",
  "queryTables" : [ "datalakehive.imhotep.advertiserusers" ],
  "queryIndex" : 109,
  "runStartToQueryComplete" : 786
}, {
  "elapsedMillis" : 4519,
  "totalScheduledMillis" : 39866,
  "cpuMillis" : 3528,
  "queuedMillis" : 0,
  "executeMillis" : 4090,
  "getResultMillis" : 0,
  "iterateMillis" : 450,
  "rows" : 90,
  "error" : null,
  "scannedBytes" : 188477036,
  "query" : "SELECT SUM((CASE WHEN \"custom_sql_query\".\"sum_sent\" = 0 THEN CAST(NULL AS DOUBLE) ELSE CAST(\"custom_sql_query\".\"sum_resp_any\" AS DOUBLE) / \"custom_sql_query\".\"sum_sent\" END)) AS \"temp_tc___18417798\",\n  DATE_TRUNC('day', CAST(CAST(\"custom_sql_query\".\"day\" AS TIMESTAMP) AS DATE)) AS \"tdy_day_ok\"\nFROM (\n  SELECT \n      day,\n      SUM(resp_any) as sum_resp_any,\n      SUM(sent) as sum_sent\n  FROM imhotep.rsContacts\n  WHERE day BETWEEN CAST(DATE(DATE_ADD('day',-100,DATE_TRUNC('day', CURRENT_TIMESTAMP))) as VARCHAR) AND \n      CAST(DATE(DATE_ADD('day',-10,DATE_TRUNC('day', CURRENT_TIMESTAMP))) as VARCHAR)\n  GROUP BY day\n) \"custom_sql_query\"\nGROUP BY 2",
  "queryTables" : [ "skipperhive.imhotep.rscontacts" ],
  "queryIndex" : 110,
  "runStartToQueryComplete" : 790
}, {
  "elapsedMillis" : 192605,
  "totalScheduledMillis" : 452060761,
  "cpuMillis" : 74598447,
  "queuedMillis" : 1,
  "executeMillis" : 49426,
  "getResultMillis" : 0,
  "iterateMillis" : 143356,
  "rows" : 4,
  "error" : null,
  "scannedBytes" : 504494302301,
  "query" : "WITH \n\nw_asl as (\n\tSELECT source_id, advertiser_id\n\tFROM datalake.tiller.adsystemdb_tbladvertiserjobsource\n\tWHERE status = 'VERIFIED'\n\tGROUP BY 1,2),\n    \nw_jpa as (\n\tSELECT w_asl.advertiser_id as advertiserid\n    , count(DISTINCT(sj.jobId)) as totalJobs\n    FROM datalake.imhotep.searchablejobs sj\n    INNER JOIN w_asl ON sj.sourceid = w_asl.source_id\n    WHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '15' DAY) \n    AND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n    AND jobcountry != 'JP'\n    GROUP BY 1),\n\nw_pjpa as (\n\tSELECT approx_percentile(totalJobs, 0.50) as p50jobsPerAdvId\n\t, approx_percentile(totalJobs, 0.75) as p75jobsPerAdvId\n\t, approx_percentile(totalJobs, 0.90) as p90jobsPerAdvId\n    , approx_percentile(totalJobs, 0.99) as p99jobsPerAdvId\n    FROM w_jpa),\n\nw_upa as (\n\tSELECT cast(au.advertiserId as bigint) as advertiserid\n    , count(distinct(accountId)) as totalUsers\n\tFROM datalake.imhotep.advertiserusers au\n    WHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '15' DAY) \n    AND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n    AND au.advertiserId !='-1'\n    AND au.advertiserId is not null\n    GROUP BY 1),\n    \nw_pjuc as (\n\tSELECT w_upa.advertiserid\n    , sum(case when w_jpa.totalJobs >= (select p50jobsPerAdvId from w_pjpa)\n        then w_upa.totalUsers else null end) as p50JobsUserCount\n    , sum(case when w_jpa.totalJobs >= (select p75jobsPerAdvId from w_pjpa) \n    \tthen w_upa.totalUsers else null end) as p75JobsUserCount\n    , sum(case when w_jpa.totalJobs >= (select p90jobsPerAdvId from w_pjpa) \n    \tthen w_upa.totalUsers else null end) as p90JobsUserCount\n    , sum(case when w_jpa.totalJobs >= (select p99jobsPerAdvId from w_pjpa) \n    \tthen w_upa.totalUsers else null end) as p99JobsUserCount\n    FROM w_upa\n    INNER JOIN w_jpa ON w_upa.advertiserId = w_jpa.advertiserId\n    GROUP BY 1),\n    \nw_final as (\n\tSELECT max(p50jobsPerAdvId) as p50jobsPerAdvId\n    , max(avgUserCountAtp50Jobs) as avgUserCountAtp50Jobs\n    , max(p75jobsPerAdvId) as p75jobsPerAdvId\n    , max(avgUserCountAtp75Jobs) as avgUserCountAtp75Jobs\n    , max(p90jobsPerAdvId) as p90jobsPerAdvId\n    , max(avgUserCountAtp90Jobs) as avgUserCountAtp90Jobs\n    , max(p99jobsPerAdvId) as p99jobsPerAdvId\n    , max(avgUserCountAtp99Jobs) as avgUserCountAtp99Jobs\n    FROM    \n\n        (SELECT avg(w_pjuc.p50JobsUserCount) as avgUserCountAtp50Jobs\n        , avg(w_pjuc.p75JobsUserCount) as avgUserCountAtp75Jobs\n        , avg(w_pjuc.p90JobsUserCount) as avgUserCountAtp90Jobs\n        , avg(w_pjuc.p99JobsUserCount) as avgUserCountAtp99Jobs\n        , 0 as p50jobsPerAdvId\n        , 0 as p75jobsPerAdvId\n        , 0 as p90jobsPerAdvId\n        , 0 as p99jobsPerAdvId\n        FROM w_pjuc\n\n        UNION ALL \n        SELECT 0 as avgUserCountAtp50Jobs\n        , 0 as avgUserCountAtp75Jobs\n        , 0 as avgUserCountAtp90Jobs\n        , 0 as avgUserCountAtp99Jobs\n        , p50jobsPerAdvId\n        , p75jobsPerAdvId\n        , p90jobsPerAdvId\n        , p99jobsPerAdvId\n        FROM w_pjpa))\n\nSELECT 'p50' as percentile, p50jobsPerAdvId as jobsPerAdvId, avgUserCountAtp50Jobs as avgUserCount FROM w_final\nUNION ALL \nSELECT 'p75' as percentile, p75jobsPerAdvId as jobsPerAdvId, avgUserCountAtp75Jobs as avgUserCount FROM w_final\nUNION ALL \nSELECT 'p90' as percentile, p90jobsPerAdvId as jobsPerAdvId, avgUserCountAtp90Jobs as avgUserCount FROM w_final\nUNION ALL \nSELECT 'p99' as percentile, p99jobsPerAdvId as jobsPerAdvId, avgUserCountAtp99Jobs as avgUserCount FROM w_final\n\n",
  "queryTables" : [ "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalakehive.imhotep.advertiserusers", "datalakehive.imhotep.advertiserusers", "datalakehive.imhotep.advertiserusers", "datalakehive.imhotep.advertiserusers", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 111,
  "runStartToQueryComplete" : 979
}, {
  "elapsedMillis" : 11925,
  "totalScheduledMillis" : 14671872,
  "cpuMillis" : 2710113,
  "queuedMillis" : 0,
  "executeMillis" : 3758,
  "getResultMillis" : 0,
  "iterateMillis" : 8230,
  "rows" : 1000,
  "error" : null,
  "scannedBytes" : 109105136176,
  "query" : "with acplh as\n(select adid,\n        cast(max(unixtime) as double) as max_unixtime,\n        cast(sum(mcbpexpendedbudget) as double) / 100000.000000 as expended_budget,\n        cast(sum(case when (status <> 'active' or auctioneligible = 1) then mcbpunderspend else 0 end) as double) / 100000.000000 as underspend\n from datalake.imhotep.adcampaignperformance\n where hour >= '2024-02-02' and\n       hour < '2024-03-01' and\n       country = 'us' and\n       advertisertype not in ('Test', 'Indeed') and\n       isLastHourOfCampaignPeriod = 1 and\n       (auctionEligible = 1 OR mcBPExpendedBudget > 0) and\n       adSubtype <> 'dradis_lifetime' and\n       bidoptadtype like '%monthly%' and\n       mcBPExpendedBudget + case when status <> 'active' OR auctionEligible = 1 then 1 else 0 end * mcBPUnderSpend > 0\n group by 1),\nacpprop as\n(select acp.adid as adid,\n        count(distinct acp.adid) as campaigns,\n        cast(max(case when cast(mcbpexpendedbudget as double) / 100000.000000 / (acplh.expended_budget + acplh.underspend) > 0.9 then acplh.max_unixtime - unixtime else 0 end) as double) / (60 * 60 * 24) as days_early,\n        cast(sum(jobcount) as double) as jobcount,\n        cast(sum(case when islasthourofcampaignperiod = 1 then mcbpexpendedbudget else 0 end) as double) / 100000.000000 as expended_budget,\n        cast(sum(case when (status <> 'active' or auctioneligible = 1) and islasthourofcampaignperiod = 1 then mcbpunderspend else 0 end) as double) / 100000.000000 as underspend\n from datalake.imhotep.adcampaignperformance acp\n inner join acplh\n on acp.adid = acplh.adid\n where hour >= '2024-02-02' and\n       hour < '2024-03-01' and\n       country = 'us' and\n       advertisertype not in ('Test', 'Indeed') and\n       (auctionEligible = 1 OR mcBPExpendedBudget > 0) and\n       adSubtype <> 'dradis_lifetime' and\n       bidoptadtype like 'source_monthly%' and\n       mcBPExpendedBudget + case when status <> 'active' OR auctionEligible = 1 then 1 else 0 end * mcBPUnderSpend > 0\n group by 1)\nselect *\nfrom acpprop\nlimit 1000\n",
  "queryTables" : [ "datalakehive.imhotep.adcampaignperformance", "datalakehive.imhotep.adcampaignperformance" ],
  "queryIndex" : 112,
  "runStartToQueryComplete" : 821
}, {
  "elapsedMillis" : 2299,
  "totalScheduledMillis" : 1706984,
  "cpuMillis" : 190083,
  "queuedMillis" : 1,
  "executeMillis" : 1089,
  "getResultMillis" : 0,
  "iterateMillis" : 1223,
  "rows" : 555,
  "error" : null,
  "scannedBytes" : 10157245056,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('hour', from_unixtime(lp.unixtime)) as date,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-30')\n        and True\n    GROUP BY\n        date_trunc('hour', from_unixtime(lp.unixtime)),\n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 6 as double) / cast(1 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1))) as change,\n        sum(1.0) over (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count             \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n\t\traw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n         \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \nchange,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,        \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 6), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 1 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 6)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 113,
  "runStartToQueryComplete" : 812
}, {
  "elapsedMillis" : 2432,
  "totalScheduledMillis" : 1658570,
  "cpuMillis" : 222760,
  "queuedMillis" : 1,
  "executeMillis" : 1162,
  "getResultMillis" : 0,
  "iterateMillis" : 1320,
  "rows" : 2425,
  "error" : null,
  "scannedBytes" : 10525533472,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('hour', from_unixtime(lp.unixtime)) as date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 1 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('hour', from_unixtime(lp.unixtime)) ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-30')\n        and True\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date_trunc('hour', from_unixtime(lp.unixtime)),\n\t\tappName,         \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n\t\tappName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 6 as double) / cast(1 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (6 * 9.0 / 1)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (6 * 4.5 / 1) as double) / (weekly_request_count + (6 * 9.0 / 1))) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 6 + 1 PRECEDING AND 1 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n\tappName,     \n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,\n    \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 6), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 1 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 6)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 114,
  "runStartToQueryComplete" : 812
}, {
  "elapsedMillis" : 126335,
  "totalScheduledMillis" : 73002727,
  "cpuMillis" : 6601819,
  "queuedMillis" : 0,
  "executeMillis" : 95190,
  "getResultMillis" : 0,
  "iterateMillis" : 31188,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 241433584485,
  "query" : "WITH experiment_data AS (\n\tSELECT \n    \tCASE WHEN ARRAYS_OVERLAP(ndxgrp, \n            ARRAY['#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275']) \n            THEN 'test' ELSE 'control' END AS _group, \n        q, jobid\n    FROM organic -- TABLESAMPLE BERNOULLI (10) -- sampling rate\n    WHERE \n    \tunixtime >= imhotep_unixtime('2024-10-15') AND unixtime < imhotep_unixtime('2024-11-12') -- test start date & end date\n        AND country = 'ca'\n        AND ARRAYS_OVERLAP(ndxgrp, \n        \tARRAY['#b15:idxmatchingcontrolplane_e274','#B15:idxmatchingcontrolplane_e274',\n            \t  '#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275'])\n)\n\nSELECT \n\t_group, \n    COUNT_IF(answer='No') as bad_matches, \n    COUNT() AS all_labels, \n    COUNT_IF(answer='No') / CAST(COUNT() AS DOUBLE) AS BMR\nFROM jobtoqueryrelevance\nINNER JOIN experiment_data ON\n\tjobtoqueryrelevance.query = experiment_data.q\n    AND jobtoqueryrelevance.jobid = experiment_data.jobid\nWHERE \n\tjobtoqueryrelevance.unixtime >= imhotep_unixtime('2024-01-01') AND jobtoqueryrelevance.unixtime < imhotep_unixtime('2024-11-12')\n\tAND jobtoqueryrelevance.country='CA'\nGROUP BY _group",
  "queryTables" : [ "skipperhive.imhotep.jobtoqueryrelevance", "skipperhive.imhotep.organic" ],
  "queryIndex" : 115,
  "runStartToQueryComplete" : 939
}, {
  "elapsedMillis" : 14827,
  "totalScheduledMillis" : 4468636,
  "cpuMillis" : 1162710,
  "queuedMillis" : 0,
  "executeMillis" : 8298,
  "getResultMillis" : 0,
  "iterateMillis" : 6590,
  "rows" : 46734,
  "error" : null,
  "scannedBytes" : 34811955389,
  "query" : "with jobs as (SELECT coalesce(parent_company_id, job_source_id+1000000) as parent_company_id \n, coalesce(max(parent_company_name), max(job_source_name)) as parent_company_name   \n, min(job_source_id) as job_source_id\n, count(distinct agg_job_id) as jobs_l30days \n, sum(organic_apply_starts) as orgAS_l30days \n\nFROM datalake.imhotep.jobactivitymetrics j\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nand job_country_code = 'US' and is_job_searchable > 0 \n--and parent_company_id is not null \nand advertiser_type not in ('Test', 'Indeed')\ngroup by 1)\n\n, rev as (select parent_company_id \n, sum(net_revenue_cents)/100 as spend_lyear \nFROM datalake.imhotep.grdm j \nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1y') AND IMHOTEP_UNIXTIME('today') \ngroup by 1) \n\n, together as (select jobs.* \n, coalesce(spend_lyear, 0) as spend_lyear \n, coalesce(last_any_products, cast('2000-01-01' as date)) as last_revenue_date  -- if no last revenue date, then say they last spend in 2000 \n, date_diff('day', coalesce(last_any_products, cast('2000-01-01' as date)), CURRENT_DATE) as days_since_last_spend \nfrom jobs \nleft join rev on rev.parent_company_id = jobs.parent_company_id \nleft join datalake.core.product_analytics_fct_first_last_activity_dates_parent f \n\ton f.parent_company_id = jobs.parent_company_id\n--where jobs_l30days > 100    \n)\n, together_ordered as (select *, \nntile(10) over (order by jobs_l30days desc) as jobs_ntile -- 1 is most jobs \n, ntile(10) over (order by spend_lyear asc) as spend_ntile -- 1 is lowest spend \n, ntile(10) over (order by days_since_last_spend desc) as days_since_last_spend_ntile -- 1 is never spender\nfrom together )\n\n, together_ordered_fixed as (select parent_company_id  \n, job_source_id\n, parent_company_name \n, orgAS_l30days \n, jobs_l30days\n, days_since_last_spend\n, spend_lyear\n, jobs_ntile \n, case when spend_lyear <= 0 then 1 else spend_ntile end as spend_ntile \n, case when days_since_last_spend > 9000 then 1 else days_since_last_spend_ntile end as days_since_last_spend_ntile\n\nfrom together_ordered )\n\n, rfm as (select * \n, jobs_ntile + spend_ntile + days_since_last_spend_ntile as rfm_unweighted \n, 1*jobs_ntile + 100*spend_ntile + 10*days_since_last_spend_ntile as rfm_weighted -- the higher this number the bigger the penalty\nfrom together_ordered_fixed\norder by 1*jobs_ntile + 100*spend_ntile + 10*days_since_last_spend_ntile desc\n)\n\n, jobs_first_hour as (SELECT sourceid\n, jobid  \n, max(normtitle) as normtitle \n, coalesce(max(jlmsa), 'rural') as msa \n, sum(coalesce(applystarts,0)) as applystarts\n\nFROM datalake.imhotep.searchimpressions j\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nand country in ('us', 'US') and sponsored = 0 and jobageminutes <= 60\ngroup by 1, 2)\n\n, jobs_first_hour_ordered as (SELECT * \n, ntile(10) over (order by applystarts asc) as job_ntile\nFROM jobs_first_hour)\n\n\n, subset as (select case when rfm_weighted > 1000 then 'A_1000+ Biggest Penalty'\nwhen rfm_weighted > 900 then 'B_900'\nwhen rfm_weighted > 800 then 'C_800'\nwhen rfm_weighted > 700 then 'D_700'\nwhen rfm_weighted > 600 then 'E_600'\nwhen rfm_weighted > 500 then 'F_500'\nwhen rfm_weighted > 400 then 'G_400'\nwhen rfm_weighted > 300 then 'H_300'\nwhen rfm_weighted > 200 then 'I_200' else 'J_100 Lowest Penalty' end as rfm_segment \n, rfm.* \n, j.* \n, row_number() over (partition by job_source_id order by applystarts desc) as rn \n\nfrom rfm  \nleft join jobs_first_hour_ordered j on j.sourceid = rfm.job_source_id \nwhere rfm_weighted > 1000 or rfm_weighted < 200\nand j.job_ntile = 10 )\n\nselect * from subset where rn <= 5\n\n\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_fct_first_last_activity_dates_parent", "datalakehive.imhotep.grdm", "datalakehive.imhotep.jobactivitymetrics", "datalakehive.imhotep.searchimpressions" ],
  "queryIndex" : 116,
  "runStartToQueryComplete" : 827
}, {
  "elapsedMillis" : 4378,
  "totalScheduledMillis" : 12214489,
  "cpuMillis" : 3330356,
  "queuedMillis" : 0,
  "executeMillis" : 1324,
  "getResultMillis" : 0,
  "iterateMillis" : 3076,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 38376039821,
  "query" : "SELECT _country, count(distinct(accountid)) as count_js, _featurebuildername, currency\nFROM datalake.mdp_offline_feature_store.jobseeker_feature_mdp_inferred_jobseeker_pay_feature_accountid\nwhere minimumyearlypayminor_v0>0\nand eventtime > TIMESTAMP '2024-05-01 00:00:00' \n    AND eventtime < TIMESTAMP '2024-08-28 00:00:00'\ngroup by _country, _featurebuildername, currency\n",
  "queryTables" : [ "datalake.mdp_offline_feature_store.jobseeker_feature_mdp_inferred_jobseeker_pay_feature_accountid" ],
  "queryIndex" : 117,
  "runStartToQueryComplete" : 817
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 8374,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_000611_00043_mzvde): Division by zero",
  "scannedBytes" : 0,
  "query" : "with \n-- Pull all conversations initiated by employers \ndremrToConvs as (\nselect  unixtime, advertiserId,  eventId, conversationId\n  \t\t, case when starts_with(subEventType, 'INBOUND_EMAIL') then 1 else 0 end as is_email\n        , case when starts_with(source, 'Dremr')  then 1 else 0 end as is_Dremr  \n  from conversationEvents \nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n  and context = 'APPLICATION'\n  and ownerRole != 'JOBSEEKER'\n  and cast(isInitialMessage as int)=1 -- Look at initial messages only, as a proxy for when a conversation is created\ngroup by 1,2,3,4,5,6\n\n              )\n              \n-- Employers with any ATS Integration\n,atsEmployers as (\n\nselect advertiserId  \n  from partnerEmployerMap\n where unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n   and confidenceScore >0\ngroup by  1 \n\n                )\n\n-- data about candidate sync (specify Indeed Apply Sync and One Host ATS Transfer where applicable)\n, atsSync as (\n\nselect advertiser_id, connection_type as ats \n      , agg_job_id\n      ,concat('DRADIS/', cast(advertiser_id as varchar), '-', cast(dradis_candidate_id as varchar)) as conversationId\n\n  from acdcTransferLifecycle\n where unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n   and pipeline_internal_type='INTERNAL_LOCATION_TYPE_EMPLOYER_JOB'\n   and connection_type!= 'h2ia'\n   and driver_type not in ('pipeline-csul', 'pipeline-csul-historical')\ngroup by 1,2,3,4\n            \n            )\n -- Dradis bulk export           \n,dradisBulkExport as (    \t\n\n        SELECT advertiserId\n\t\t\t  ,concat('DRADIS/', cast(advertiserId as varchar), '-', cast(candidateId as varchar)) as conversationId\n        FROM datalake.imhotep.dradisCandidateExportRequest\n        CROSS JOIN UNNEST(split(candidateIds, ',')) as t (candidateId)\n        WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n       group by 1,2\n  )     \n\n\n-- advertiser segment details\n,advertiserDetails as ( \n\nselect advertiser_id\n   , max_by(parent_company_size_segment, unixtime) as parent_company_size_segment\n   , max_by(type, unixtime) as type\n   , max_by(billing_country, unixtime) as billing_country\n  from  daily_employer2 \n  where unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\ngroup by 1\n\n   )\n\n\n -- Aggregate convo counts per query\n, empDataAgg as (\n\n\n select \n\tdremrToConvs.advertiserId\n   ,case when atsEmployers.advertiserId is not null then 1 else 0 end as isAtsIntegrated   \n   ,count(*) as allConvos\n   ,count_if(is_Dremr=1) as dremrConvos\n   --,count_if(is_Dremr=1 and is_email=1) as dremrEmailConvos\n   ,count_if(is_Dremr=1 and atsSync.conversationId is not null) as dremrAtsSyncAll\n   ,count_if(is_Dremr=1 and dradisBulkExport.conversationId is not null) as dremrDradisBulkExport\n   ,count_if(is_Dremr=1 and (atsSync.conversationId is not null or dradisBulkExport.conversationId is not null)) as dremrAtsSyncOrBulkExport\n   \n   \n  from dremrToConvs \n  left join atsEmployers on dremrToConvs.advertiserId=atsEmployers.advertiserId\n  left join atsSync on dremrToConvs.advertiserId=atsSync.advertiser_id\n        and dremrToConvs.conversationId=atsSync.conversationId\n  left join dradisBulkExport on dremrToConvs.advertiserId=dradisBulkExport.advertiserId\n        and dremrToConvs.conversationId=dradisBulkExport.conversationId\n    \n  group by 1,2\n                )\n                \n \n,segConvos as (\nselect\n    ce.advertiserId\n   ,eventId\n   ,ce.encryptedConversationId\n   ,(isImported = '1') as isEmail\nFROM conversationEvents ce\njoin convsConversationCreation cc on  ce.encryptedConversationId = cc.encryptedConversationId \n\nWHERE ce.unixtime BETWEEN IMHOTEP_UNIXTIME('209d') AND IMHOTEP_UNIXTIME('28d')\nand   cc.unixtime BETWEEN IMHOTEP_UNIXTIME('223d') AND IMHOTEP_UNIXTIME('28d')\nand ((ce.unixtime - cc.unixtime) < (60*60*24*14))\nand ce.context='APPLICATION'\nand ce.ownerRole != 'JOBSEEKER'\nGROUP BY 1,2,3,4\n)\n\n\n, segConvosAgg as (\n\nselect advertiserId\n      ,count(*) as totalMessages\n      ,count_if(isEmail) as emailMessages\n      ,count_if(NOT isEmail) as onPlatformMessages \n  from segConvos\ngroup by 1 \n\n\t\t)\n\n,segConvosAgg2 as (\n\n select advertiserId, case when emailMessages> 0 and onPlatformMessages=0 then 'Email Only'\n\t\t\t when  emailMessages=0 and onPlatformMessages>0 then 'Chat Only'\n\t\t\t when  emailMessages>0 and onPlatformMessages>0 then 'Email & Chat'\n             end as usageType\n\n  from segConvosAgg\n                 )\n\n\nselect \n        advertiserDetails.parent_company_size_segment as size\n       ,segConvosAgg2.usageType\n       ,count() as empCount\n       , 100.0*count()/sum(count()) over () pctEmpCountOfAll\n    --  ,count_if(isAtsIntegrated=1) as empCountAtsIntegrated\n      ,100.00* count_if(isAtsIntegrated=1)/count(*) as empPctAtsIntegratedWithinSegment\n      ,sum(allConvos) as ttlConvosInitiated\n\t  ,sum(DremrConvos) as DremrConvosInitiated\n  --    ,sum(dremrConvos) as ttlDremrConvosInitiated\n      ,100.00* sum(dremrConvos)/sum(allConvos) as dremrConvosPctOfAllInitiated\n      \n      ,100.00* sum(dremrAtsSyncAll)/sum(dremrConvos) as pctOfDremrConvosAtsSyncAll\n      ,100.00* sum(dremrDradisBulkExport)/sum(dremrConvos) as pctOfDremrConvosDradisBulkExport\n      ,100.00* sum(dremrAtsSyncOrBulkExport)/sum(dremrConvos) as pctOfDremrConvosAtsSyncOrBulkExport\n     \n from empDataAgg\nleft join advertiserDetails on empDataAgg.advertiserId=advertiserDetails.advertiser_id\nleft join segConvosAgg2     on empDataAgg.advertiserId=segConvosAgg2.advertiserId\nwhere  parent_company_size_segment in ('L', 'XL') and usageType is not null\ngroup by 1,2\n                \n\n                \n\n              \n              \n              ",
  "queryTables" : [ "datalakehive.imhotep.dradiscandidateexportrequest", "skipperhive.imhotep.acdctransferlifecycle", "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.convsconversationcreation", "skipperhive.imhotep.daily_employer2", "skipperhive.imhotep.partneremployermap" ],
  "queryIndex" : 118,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 8293,
  "totalScheduledMillis" : 3956802,
  "cpuMillis" : 1927624,
  "queuedMillis" : 0,
  "executeMillis" : 2645,
  "getResultMillis" : 0,
  "iterateMillis" : 5696,
  "rows" : 3,
  "error" : null,
  "scannedBytes" : 80374705991,
  "query" : "with convos as (\nselect\n    ce.advertiserId\n   ,eventId\n   ,ce.encryptedConversationId\n   ,(isImported = '1') as isEmail\nFROM conversationEvents ce\njoin convsConversationCreation cc on  ce.encryptedConversationId = cc.encryptedConversationId \n\nWHERE ce.unixtime BETWEEN IMHOTEP_UNIXTIME('181d') AND IMHOTEP_UNIXTIME('1d')\nand   cc.unixtime BETWEEN IMHOTEP_UNIXTIME('195d') AND IMHOTEP_UNIXTIME('1d')\nand ((ce.unixtime - cc.unixtime) < (60*60*24*14))\nand ce.context='APPLICATION'\nand ce.ownerRole != 'JOBSEEKER'\nGROUP BY 1,2,3,4\n)\n\n\n, agg as (\n\nselect advertiserId\n      ,count(*) as totalMessages\n      ,count_if(isEmail) as emailMessages\n      ,count_if(NOT isEmail) as onPlatformMessages \n  from convos\ngroup by 1 \n\n\t\t)\n        \n,advertiserDetails as ( \n\nselect advertiser_id\n   , max_by(parent_company_size_segment, unixtime) as parent_company_size_segment\n   , max_by(type, unixtime) as type\n   , max_by(billing_country, unixtime) as billing_country\n  from  daily_employer2 \n  where unixtime BETWEEN IMHOTEP_UNIXTIME('2d') and IMHOTEP_UNIXTIME('1d')\ngroup by 1\n\n   )\n        \n\n select \n            case when emailMessages> 0 and onPlatformMessages=0 then 'Email Only'\n\t\t\t when  emailMessages=0 and onPlatformMessages>0 then 'Chat Only'\n\t\t\t when  emailMessages>0 and onPlatformMessages>0 then 'Email & Chat'\n             end as usageType\n        ,count(distinct advertiserId) as empCountDistinct\n        ,100.0*count(distinct advertiserId)/sum(count(distinct advertiserId)) over() as empCountDistinct\n        ,count() as empCount\n        ,100.0*count()/sum(count()) over() as pctEmpCount\n\n  from agg\nleft join advertiserDetails on agg.advertiserId=advertiserDetails.advertiser_id\nwhere parent_company_size_segment='S'\ngroup by 1\norder by 1\n        \n\n\n\n\n",
  "queryTables" : [ "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.convsconversationcreation", "skipperhive.imhotep.daily_employer2" ],
  "queryIndex" : 119,
  "runStartToQueryComplete" : 823
}, {
  "elapsedMillis" : 1387,
  "totalScheduledMillis" : 10427,
  "cpuMillis" : 1400,
  "queuedMillis" : 0,
  "executeMillis" : 1097,
  "getResultMillis" : 0,
  "iterateMillis" : 861,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 1278917,
  "query" : "  SELECT \n  \tadvertiserid as advertiser_id\n    , t.permission as product_perm\n    , accountid as account_id\n  FROM \n  \tdatalake.imhotep.advertiserusers  \n  \tCROSS JOIN UNNEST (permissionSetNames) as T (permission)\n  WHERE \n  \tunixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n  GROUP BY\n    advertiserid \n    , t.permission \n    , accountid\n\nLIMIT 100",
  "queryTables" : [ "datalakehive.imhotep.advertiserusers" ],
  "queryIndex" : 120,
  "runStartToQueryComplete" : 824
}, {
  "elapsedMillis" : 73472,
  "totalScheduledMillis" : 40676450,
  "cpuMillis" : 8253050,
  "queuedMillis" : 0,
  "executeMillis" : 15900,
  "getResultMillis" : 0,
  "iterateMillis" : 57621,
  "rows" : 139,
  "error" : null,
  "scannedBytes" : 543143858207,
  "query" : "WITH most_recent_starts AS (\n    SELECT appid, env, runId, ROW_NUMBER() OVER(PARTITION BY appid, env ORDER BY unixtime DESC) AS startup_num\n    FROM javaruntimeversions\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n    GROUP BY appid, env, runId, unixtime\n), most_recent_runs AS (\n    SELECT mrs.appid, mrs.env, jrv.unixtime AS most_recent_start_unixtime, jrv.osimage, jrv.osimageversionid, jrv.runid AS runid \n    FROM most_recent_starts mrs\n    LEFT JOIN javaruntimeversions jrv ON mrs.runId=jrv.runId AND mrs.startup_num=1\n    WHERE jrv.unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n), service_data AS (\n    SELECT \n        mrr.appid, mrr.env, srs.applicationname, srs.applicationtype, srs.runtimetype, srs.teamid, \n        srs.lifecycle, srs.state, srs.git_project_url,\n        FROM_UNIXTIME(mrr.most_recent_start_unixtime) AS most_recent_start_time, mrr.runid, mrr.osimage, mrr.osimageversionid\n    FROM most_recent_runs mrr\n    LEFT JOIN serviceregistrysnapshots srs ON mrr.appid=srs.applicationid\n    WHERE srs.unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n), scoped_data AS (\n\tSELECT * FROM service_data sd WHERE sd.lifecycle!='UNAVAILABLE'\n), daemonizer_data AS (\n    SELECT \n        sd.*,\n        jlv.libversion AS daemonizer_version\n    FROM scoped_data sd \n    INNER JOIN javalibversions jlv ON jlv.runid=sd.runid\n    WHERE \n        jlv.unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n        AND jlv.lib='indeed:daemonizer'\n)\n\nSELECT sd.*, dd.daemonizer_version \nFROM scoped_data sd\nLEFT JOIN daemonizer_data dd ON sd.runid=dd.runid\nWHERE dd.teamid LIKE '%'",
  "queryTables" : [ "skipperhive.imhotep.javalibversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.serviceregistrysnapshots", "skipperhive.imhotep.serviceregistrysnapshots" ],
  "queryIndex" : 121,
  "runStartToQueryComplete" : 896
}, {
  "elapsedMillis" : 71096,
  "totalScheduledMillis" : 41677364,
  "cpuMillis" : 8974107,
  "queuedMillis" : 0,
  "executeMillis" : 13582,
  "getResultMillis" : 0,
  "iterateMillis" : 57565,
  "rows" : 8049,
  "error" : null,
  "scannedBytes" : 543143677569,
  "query" : "WITH most_recent_starts AS (\n    SELECT appid, env, runId, ROW_NUMBER() OVER(PARTITION BY appid, env ORDER BY unixtime DESC) AS startup_num\n    FROM javaruntimeversions\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n    GROUP BY appid, env, runId, unixtime\n), most_recent_runs AS (\n    SELECT mrs.appid, mrs.env, jrv.unixtime AS most_recent_start_unixtime, jrv.osimage, jrv.osimageversionid, jrv.runid AS runid \n    FROM most_recent_starts mrs\n    LEFT JOIN javaruntimeversions jrv ON mrs.runId=jrv.runId AND mrs.startup_num=1\n    WHERE jrv.unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n), service_data AS (\n    SELECT \n        mrr.appid, mrr.env, srs.applicationname, srs.applicationtype, srs.runtimetype, srs.teamid, \n        srs.lifecycle, srs.state, srs.git_project_url,\n        FROM_UNIXTIME(mrr.most_recent_start_unixtime) AS most_recent_start_time, mrr.runid, mrr.osimage, mrr.osimageversionid\n    FROM most_recent_runs mrr\n    LEFT JOIN serviceregistrysnapshots srs ON mrr.appid=srs.applicationid\n    WHERE srs.unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n), scoped_data AS (\n\tSELECT * FROM service_data sd WHERE sd.lifecycle!='UNAVAILABLE'\n), boot_data AS (\n    SELECT \n        sd.*,\n        jlv.libversion AS springboot_version\n    FROM scoped_data sd \n    INNER JOIN javalibversions jlv ON jlv.runid=sd.runid\n    WHERE \n        jlv.unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n        AND jlv.lib='org.springframework.boot:spring-boot'\n)\n\nSELECT sd.*, bd.springboot_version FROM scoped_data sd\nLEFT JOIN boot_data bd ON sd.runid=bd.runid",
  "queryTables" : [ "skipperhive.imhotep.javalibversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.serviceregistrysnapshots", "skipperhive.imhotep.serviceregistrysnapshots" ],
  "queryIndex" : 122,
  "runStartToQueryComplete" : 895
}, {
  "elapsedMillis" : 13239,
  "totalScheduledMillis" : 1661636,
  "cpuMillis" : 193493,
  "queuedMillis" : 0,
  "executeMillis" : 1178,
  "getResultMillis" : 0,
  "iterateMillis" : 12074,
  "rows" : 655,
  "error" : null,
  "scannedBytes" : 8789433093,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('minute', from_unixtime(lp.unixtime)) as date,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-29')\n        and appName='ContentGenerationService' and moderationApi='openAIModerations' and moderationType='input'\n    GROUP BY\n        date_trunc('minute', from_unixtime(lp.unixtime)),\n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 240 as double) / cast(30 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30))) as change,\n        sum(1.0) over (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count             \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n\t\traw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n         \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \nchange,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,        \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 240), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 15 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 240)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 123,
  "runStartToQueryComplete" : 853
}, {
  "elapsedMillis" : 52375,
  "totalScheduledMillis" : 27731076,
  "cpuMillis" : 5791947,
  "queuedMillis" : 1,
  "executeMillis" : 1765,
  "getResultMillis" : 0,
  "iterateMillis" : 50640,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 1007512319708,
  "query" : "WITH\n\ninterview_event as(\nselect distinct advertiser_id from datalake.imhotep.employer_user_actions_spark\nwhere hour >= cast(date_add('day',-7,CURRENT_DATE) as varchar)\nAND element = 'schedule-interview-modal-create-new-interview'\nAND custom_action_type = 'click'),\n\nsbs_reps as(\nselect distinct ldap\nfrom datalake.imhotep.indeedemployeesnapshot\nwhere sup_org_name like '%SBS%'\nand day >= cast(date_add('day',-7,CURRENT_DATE) as varchar)\n)\n\nSELECT \ndate_trunc('day',cast(from_unixtime(unixtime) as timestamp)) as date\n, masquerading_user\n, advertiser_id\nfrom datalake.imhotep.employer_user_actions_spark\nWHERE hour >= cast(date_add('day',-7,CURRENT_DATE) as varchar)\nand is_masquerading=1\nAND element = 'schedule-interview-modal-create-new-interview'\nAND custom_action_type = 'click'\nand advertiser_id in (select distinct advertiser_id from interview_event)\nand masquerading_user in (select distinct ldap from sbs_reps)\n\norder by 1 DESC\nlimit 50",
  "queryTables" : [ "datalakehive.imhotep.employer_user_actions_spark", "datalakehive.imhotep.employer_user_actions_spark", "datalakehive.imhotep.indeedemployeesnapshot" ],
  "queryIndex" : 124,
  "runStartToQueryComplete" : 892
}, {
  "elapsedMillis" : 11401,
  "totalScheduledMillis" : 1654272,
  "cpuMillis" : 202590,
  "queuedMillis" : 1,
  "executeMillis" : 1054,
  "getResultMillis" : 0,
  "iterateMillis" : 10359,
  "rows" : 655,
  "error" : null,
  "scannedBytes" : 8797468642,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('minute', from_unixtime(lp.unixtime)) as date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-29')\n        and appName='ContentGenerationService' and moderationApi='openAIModerations' and moderationType='input'\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date_trunc('minute', from_unixtime(lp.unixtime)),\n\t\tappName,         \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n\t\tappName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 240 as double) / cast(30 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30))) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n\tappName,     \n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,\n    \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 240), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 15 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 240)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 125,
  "runStartToQueryComplete" : 852
}, {
  "elapsedMillis" : 6908,
  "totalScheduledMillis" : 305558,
  "cpuMillis" : 30852,
  "queuedMillis" : 1,
  "executeMillis" : 1970,
  "getResultMillis" : 0,
  "iterateMillis" : 4960,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 604195063,
  "query" : "SELECT COUNT(DISTINCT accountid) AS distinct_account_count\nFROM datalake.imhotep.passsigninattempt\nWHERE unixtime > IMHOTEP_UNIXTIME('1month') AND successful = 1",
  "queryTables" : [ "datalakehive.imhotep.passsigninattempt" ],
  "queryIndex" : 126,
  "runStartToQueryComplete" : 851
}, {
  "elapsedMillis" : 61234,
  "totalScheduledMillis" : 4675843,
  "cpuMillis" : 1723027,
  "queuedMillis" : 2,
  "executeMillis" : 5440,
  "getResultMillis" : 0,
  "iterateMillis" : 55832,
  "rows" : 35,
  "error" : null,
  "scannedBytes" : 34670715016,
  "query" : "-- Trigger RB Rules:\n\n-- Rules w/ Payments cat\nWITH rules AS (\n    SELECT rule_id, created_by, rule_state, responses\n    FROM rulebook_snapshot rs\n    WHERE unixtime BETWEEN imhotep_unixtime('1d') AND imhotep_unixtime('today')\n    AND context = 'advertiser'\n    AND (\n        REGEXP_LIKE(LOWER(rule), 'triggeredrulenames') \n        OR REGEXP_LIKE(LOWER(rule), 'triggeredruleids')\n        OR rule_id IN (4800, 4803, 4605, 4853, 4864, 4854, 3161, 3160, 2615, \n                       3257, 3282, 3341, 3346, 3350, 2597, 2610, 4505, 3362, \n                       3267, 4080, 3363, 2616, 3499, 4174, 4183, 4703)\n    \t)\n    AND rule_state = 'ACTIVE'\n    GROUP BY 1,2,3,4\n),\n\n-- Advertisers that matched rules from the CTE above\nrule_matches AS (\n    SELECT ruleid, contentkey\n    FROM rulecontentresult rc\n    INNER JOIN rules r ON r.rule_id = rc.ruleid\n    WHERE unixtime BETWEEN imhotep_unixtime('2024-01-01') AND imhotep_unixtime('7d') --change rule match date here\n    AND match = 1\n    GROUP BY 1,2\n),\n\n-- Advertisers info that matched payments rules\ndmod AS (\n    SELECT ruleid, advertiserid, showhostedjobs, industry\n    FROM dradismod dmod\n    INNER JOIN rule_matches rm ON CAST(dmod.advertiserid AS VARCHAR) = rm.contentkey\n    WHERE unixtime BETWEEN imhotep_unixtime('2d') AND imhotep_unixtime('1d')\n    AND showhostedjobs != 'pending'\n    AND industry != 'Job Seeker'\n    GROUP BY 1,2,3,4\n),\n\nrule_metrics AS (\n    SELECT \n        ruleid,\n        COUNT(DISTINCT CASE WHEN industry = 'Fraud/Scam' THEN advertiserid END) AS fraud_count,\n        COUNT(DISTINCT CASE WHEN showhostedjobs = 'rejected' THEN advertiserid END) AS rejected_count,\n        COUNT(DISTINCT advertiserid) AS all_count, \n        ROUND(COUNT(DISTINCT CASE WHEN industry = 'Fraud/Scam' THEN advertiserid END) / CAST(COUNT(DISTINCT advertiserid) AS DOUBLE) * 100,2) AS fraud_perc,\n        ROUND(COUNT(DISTINCT CASE WHEN showhostedjobs = 'rejected' THEN advertiserid END) / CAST(COUNT(DISTINCT advertiserid) AS DOUBLE) * 100,2) AS rejection_perc\n    FROM dmod\n    GROUP BY 1\n)\n\n-- Left join on the first CTE with the rule_metrics CTE to get an output with all rule values (null values coalesced to zero)\nSELECT\n    rule_id, \n    created_by, \n    rule_state, \n    COALESCE(fraud_count,0) AS fraud_count,\n    COALESCE(rejected_count,0) AS rejected_count,\n    COALESCE(all_count,0) AS all_count,\n    COALESCE(fraud_perc,0) AS fraud_perc,\n    COALESCE(rejection_perc,0) AS rejection_perc\nFROM rules r\nLEFT JOIN rule_metrics rm ON rm.ruleid = r.rule_id\nGROUP BY 1,2,3,4,5,6,7,8\nORDER BY all_count DESC",
  "queryTables" : [ "skipperhive.imhotep.dradismod", "skipperhive.imhotep.rulebook_snapshot", "skipperhive.imhotep.rulebook_snapshot", "skipperhive.imhotep.rulecontentresult" ],
  "queryIndex" : 127,
  "runStartToQueryComplete" : 912
}, {
  "elapsedMillis" : 34766,
  "totalScheduledMillis" : 12149373,
  "cpuMillis" : 2812380,
  "queuedMillis" : 0,
  "executeMillis" : 1826,
  "getResultMillis" : 0,
  "iterateMillis" : 32967,
  "rows" : 3,
  "error" : null,
  "scannedBytes" : 492564294012,
  "query" : "/*WITH\n\nsponsor_event as(\n  Select distinct advertiser_id \n  FROM datalake.imhotep.sponsormodulesevents2 \n  WHERE hour >= cast(date_add('day',-21,CURRENT_DATE) as varchar)\n  AND page='sponsorOrganic' \n  AND event='element_click' \n  AND element_name='cpc_sponsorship' \n  AND is_masquerading=1\n),*/\n\nWITH\n\nsbs_reps as (\n\tselect DISTINCT ldap\n\tFROM datalake.imhotep.indeedemployeesnapshot\n\tWHERE sup_org_name LIKE '%SBS%'\n    and day >= cast(date_add('day',-7,CURRENT_DATE) as varchar)\n)\n\nSELECT \ndate_trunc('day',cast(from_unixtime(unixtime) as timestamp)) as date\n, masquerading_user\n, advertiser_id\n, element\n, url_path\n\nFROM datalake.imhotep.employer_user_actions_spark\n--WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  where hour >= cast(date_add('day',-7,CURRENT_DATE) as varchar)\n  and custom_action_type like '%click%'\n\n  and is_masquerading = 1\n  and ((url_path = '/sponsor/edit/sponsor' and action_name='sponsored-button')\n  or (url_path = '/objective-campaign/create/confirmation' and element like 'save-campaign-button'))\n  \n  and masquerading_user in (Select distinct ldap FROM sbs_reps)\n  --  and advertiser_id = 773008\n\n  --  and ((url_path = '/sponsor/edit/sponsor'and action_name='sponsored-button') or (url_path = '/objective-campaign/create/confirmation' and action_name = 'save-campaign-button'))\n  --  and advertiser_id in (Select distinct advertiser_id FROM sponsor_event)\n\nGROUP BY 1,2,3,4,5\nORDER BY 1 desc",
  "queryTables" : [ "datalakehive.imhotep.employer_user_actions_spark", "datalakehive.imhotep.indeedemployeesnapshot" ],
  "queryIndex" : 128,
  "runStartToQueryComplete" : 886
}, {
  "elapsedMillis" : 7612,
  "totalScheduledMillis" : 443335,
  "cpuMillis" : 50962,
  "queuedMillis" : 1,
  "executeMillis" : 1754,
  "getResultMillis" : 0,
  "iterateMillis" : 5879,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 1201437200,
  "query" : "SELECT COUNT(DISTINCT accountid) AS distinct_account_count\nFROM datalake.imhotep.passsigninattempt\nWHERE unixtime > IMHOTEP_UNIXTIME('2month') AND successful = 1",
  "queryTables" : [ "datalakehive.imhotep.passsigninattempt" ],
  "queryIndex" : 129,
  "runStartToQueryComplete" : 874
}, {
  "elapsedMillis" : 28895,
  "totalScheduledMillis" : 35128248,
  "cpuMillis" : 8625333,
  "queuedMillis" : 1,
  "executeMillis" : 4880,
  "getResultMillis" : 0,
  "iterateMillis" : 24048,
  "rows" : 140,
  "error" : null,
  "scannedBytes" : 543143939486,
  "query" : "WITH most_recent_starts AS (\n    SELECT appid, env, runId, ROW_NUMBER() OVER(PARTITION BY appid, env ORDER BY unixtime DESC) AS startup_num\n    FROM javaruntimeversions\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n    GROUP BY appid, env, runId, unixtime\n), most_recent_runs AS (\n    SELECT mrs.appid, mrs.env, jrv.unixtime AS most_recent_start_unixtime, jrv.osimage, jrv.osimageversionid, jrv.runid AS runid \n    FROM most_recent_starts mrs\n    LEFT JOIN javaruntimeversions jrv ON mrs.runId=jrv.runId AND mrs.startup_num=1\n    WHERE jrv.unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n), service_data AS (\n    SELECT \n        mrr.appid, mrr.env, srs.applicationname, srs.applicationtype, srs.runtimetype, srs.teamid, \n        srs.lifecycle, srs.state, srs.git_project_url,\n        FROM_UNIXTIME(mrr.most_recent_start_unixtime) AS most_recent_start_time, mrr.runid, mrr.osimage, mrr.osimageversionid\n    FROM most_recent_runs mrr\n    LEFT JOIN serviceregistrysnapshots srs ON mrr.appid=srs.applicationid\n    WHERE srs.unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n), scoped_data AS (\n\tSELECT * FROM service_data sd WHERE sd.lifecycle!='UNAVAILABLE'\n), daemonizer_data AS (\n    SELECT \n        sd.*,\n        jlv.libversion AS daemonizer_version\n    FROM scoped_data sd \n    INNER JOIN javalibversions jlv ON jlv.runid=sd.runid\n    WHERE \n        jlv.unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n        AND jlv.lib='indeed:daemonizer'\n)\n\nSELECT sd.*, dd.daemonizer_version \nFROM scoped_data sd\nLEFT JOIN daemonizer_data dd ON sd.runid=dd.runid\nWHERE dd.teamid LIKE '%'",
  "queryTables" : [ "skipperhive.imhotep.javalibversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.serviceregistrysnapshots", "skipperhive.imhotep.serviceregistrysnapshots" ],
  "queryIndex" : 130,
  "runStartToQueryComplete" : 904
}, {
  "elapsedMillis" : 30586,
  "totalScheduledMillis" : 35043261,
  "cpuMillis" : 8694295,
  "queuedMillis" : 0,
  "executeMillis" : 6177,
  "getResultMillis" : 0,
  "iterateMillis" : 24463,
  "rows" : 139,
  "error" : null,
  "scannedBytes" : 543143836319,
  "query" : "WITH most_recent_starts AS (\n    SELECT appid, env, runId, ROW_NUMBER() OVER(PARTITION BY appid, env ORDER BY unixtime DESC) AS startup_num\n    FROM javaruntimeversions\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n    GROUP BY appid, env, runId, unixtime\n), most_recent_runs AS (\n    SELECT mrs.appid, mrs.env, jrv.unixtime AS most_recent_start_unixtime, jrv.osimage, jrv.osimageversionid, jrv.runid AS runid \n    FROM most_recent_starts mrs\n    LEFT JOIN javaruntimeversions jrv ON mrs.runId=jrv.runId AND mrs.startup_num=1\n    WHERE jrv.unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n), service_data AS (\n    SELECT \n        mrr.appid, mrr.env, srs.applicationname, srs.applicationtype, srs.runtimetype, srs.teamid, \n        srs.lifecycle, srs.state, srs.git_project_url,\n        FROM_UNIXTIME(mrr.most_recent_start_unixtime) AS most_recent_start_time, mrr.runid, mrr.osimage, mrr.osimageversionid\n    FROM most_recent_runs mrr\n    LEFT JOIN serviceregistrysnapshots srs ON mrr.appid=srs.applicationid\n    WHERE srs.unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n), scoped_data AS (\n\tSELECT * FROM service_data sd WHERE sd.lifecycle!='UNAVAILABLE'\n), daemonizer_data AS (\n    SELECT \n        sd.*,\n        jlv.libversion AS daemonizer_version\n    FROM scoped_data sd \n    INNER JOIN javalibversions jlv ON jlv.runid=sd.runid\n    WHERE \n        jlv.unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n        AND jlv.lib='indeed:daemonizer'\n)\n\nSELECT sd.*, dd.daemonizer_version \nFROM scoped_data sd\nLEFT JOIN daemonizer_data dd ON sd.runid=dd.runid\nWHERE dd.teamid LIKE '%'",
  "queryTables" : [ "skipperhive.imhotep.javalibversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.serviceregistrysnapshots", "skipperhive.imhotep.serviceregistrysnapshots" ],
  "queryIndex" : 131,
  "runStartToQueryComplete" : 906
}, {
  "elapsedMillis" : 8604,
  "totalScheduledMillis" : 102740,
  "cpuMillis" : 10690,
  "queuedMillis" : 0,
  "executeMillis" : 1500,
  "getResultMillis" : 0,
  "iterateMillis" : 7140,
  "rows" : 9,
  "error" : null,
  "scannedBytes" : 154490078,
  "query" : "WITH opened_daemonizer_mrs AS (\n    SELECT path_with_namespace_plus_mr_iid\n    FROM imhotep.gitlabevents ge \n    WHERE \n        ge.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-04-01') AND IMHOTEP_UNIXTIME('today')\n        AND action='open'\n        AND gitlabhook_objectattributes_title LIKE '%Daemonizer Migration%'\n), flow AS (\n    SELECT \n    \tpath_with_namespace_plus_mr_iid AS merge_request_id, \n        action, \n        CASE \n        \tWHEN action = 'open' THEN FROM_UNIXTIME(CAST(gitlabhook_objectattributes_createdat AS DOUBLE))\n            ELSE FROM_UNIXTIME(gitlabhook_objectattributes_updatedat) \n        END AS action_time,\n        ROW_NUMBER() OVER(PARTITION BY path_with_namespace_plus_mr_iid, action ORDER BY gitlabhook_objectattributes_updatedat ASC) AS step_number\n    FROM imhotep.gitlabevents ge \n    WHERE \n        ge.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-04-01') AND IMHOTEP_UNIXTIME('today')\n        AND ge.path_with_namespace_plus_mr_iid IN (SELECT path_with_namespace_plus_mr_iid FROM opened_daemonizer_mrs)\n        AND action NOT IN ('null')\n    ORDER BY path_with_namespace_plus_mr_iid, gitlabhook_objectattributes_updatedat ASC\n), steps AS (\n\tSELECT merge_request_id, action, action_time\n\tFROM flow \n\tWHERE step_number = 1  -- don't show repeated chains of MR updates\n\tORDER BY merge_request_id, action_time ASC\n), agg_steps AS (\n\tSELECT \n    \tmerge_request_id, \n        ARRAY_JOIN(ARRAY_AGG(action ORDER BY action_time, action), ',') AS steps \n    FROM steps \n    GROUP BY merge_request_id\n)\n\n--SELECT * FROM steps ORDER BY merge_request_id, action_time\n\nSELECT steps, COUNT(merge_request_id) AS num_values FROM agg_steps GROUP BY steps",
  "queryTables" : [ "skipperhive.imhotep.gitlabevents", "skipperhive.imhotep.gitlabevents" ],
  "queryIndex" : 132,
  "runStartToQueryComplete" : 886
}, {
  "elapsedMillis" : 13550,
  "totalScheduledMillis" : 185656,
  "cpuMillis" : 37026,
  "queuedMillis" : 0,
  "executeMillis" : 781,
  "getResultMillis" : 0,
  "iterateMillis" : 12804,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 2644287913,
  "query" : "select * from datalake.tiller.oplin_events_usage order by _h2_inserted desc limit 10",
  "queryTables" : [ "datalake.tiller.oplin_events_usage" ],
  "queryIndex" : 133,
  "runStartToQueryComplete" : 891
}, {
  "elapsedMillis" : 36772,
  "totalScheduledMillis" : 33817203,
  "cpuMillis" : 2775892,
  "queuedMillis" : 1,
  "executeMillis" : 3633,
  "getResultMillis" : 0,
  "iterateMillis" : 33161,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 53280573953,
  "query" : "with msa_jobs as (\n    select sj.jobid as agg_job_id\n    , jlmsa\n    , trim(jlmsa) as jlmsa_trimmed\n    , sum(case when trim(jlmsa) is not null then 1 else 0 end) AS urban_jobs\n    , sum(case when trim(jlmsa) is null then 1 else 0 end) AS rural_jobs\n    , max_by(trim(jlmsa), hour) as latest_msa\n    from datalake.imhotep.searchablejobs sj\n    where hour between date_format(from_unixtime(to_unixtime(date('2024-10-01'))), '%Y-%m-%d %H:%i:%s') and date_format(from_unixtime(to_unixtime(date('2024-10-20'))), '%Y-%m-%d %H:%i:%s')\n    group by 1,2,3\n)\n\nselect * from msa_jobs\nwhere rural_jobs>0 and jlmsa is not null\nlimit 100\n",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 134,
  "runStartToQueryComplete" : 917
}, {
  "elapsedMillis" : 70169,
  "totalScheduledMillis" : 193767123,
  "cpuMillis" : 35039499,
  "queuedMillis" : 1,
  "executeMillis" : 12678,
  "getResultMillis" : 0,
  "iterateMillis" : 57557,
  "rows" : 4,
  "error" : null,
  "scannedBytes" : 2746754941419,
  "query" : "with premium_offer as (\n\tSELECT job_hash,\n        (CASE WHEN (CONTAINS(grp, '#B2:jobmo_sep_2024_premium_mvp1') and CONTAINS(grp, '#A2:premium_package_mvp_sponsor_confirmation1')) THEN 'test1'\n        \tWHEN (CONTAINS(grp, '#B2:jobmo_sep_2024_premium_mvp2') and CONTAINS(grp, '#A2:premium_package_mvp_sponsor_confirmation1')) THEN 'test2'\n            WHEN (CONTAINS(grp, '#B2:jobmo_sep_2024_premium_mvp3') and CONTAINS(grp, '#A2:premium_package_mvp_sponsor_confirmation1')) THEN 'test3'\n        \tELSE 'control' END ) as test_grp\n    FROM datalake.imhotep.sponsormodulesevents2\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-19') AND (IMHOTEP_UNIXTIME('today')) \n        AND (CONTAINS(grp, '#A2:premium_package_mvp_sponsor_confirmation0') OR CONTAINS(grp, '#A2:premium_package_mvp_sponsor_confirmation1'))\n\t\tAND is_privileged=0 AND is_masquerading=0 AND ipcountry='US' AND element_name in ('premium_features_ghost','premium_features') -- AND NOT edit  \n    GROUP BY 1,2\n)\n,\npremium_sponsor as (\n\tSELECT job_hash,\n    \tDATE(from_unixtime(unixtime)) as sponsor_start_date,\n        advertiser_id\n    FROM datalake.imhotep.sponsormodulesevents2 \n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-19') AND (IMHOTEP_UNIXTIME('today')) \n    \tAND is_privileged=0 AND is_masquerading=0 AND ipcountry='US' --AND NOT edit  \n        AND NOT is_free_to_post AND event='page_submission'\n        AND budget_tier in ('PREMIUM','PREMIUM_PACKAGE')\n        AND job_hash IN (SELECT job_hash FROM premium_offer)\n)\n,\nbase_t as (\nSELECT \n\nCASE \n        WHEN CONTAINS(recent_advertiser_test_groups, '#A2:premium_package_mvp_sponsor_confirmation0') THEN 'control'\n        WHEN CONTAINS(recent_advertiser_test_groups, '#A2:premium_package_mvp_sponsor_confirmation1') AND CONTAINS(recent_advertiser_test_groups, '#B2:jobmo_sep_2024_premium_mvp1') THEN 'test1'\n        WHEN CONTAINS(recent_advertiser_test_groups, '#A2:premium_package_mvp_sponsor_confirmation1') AND CONTAINS(recent_advertiser_test_groups, '#B2:jobmo_sep_2024_premium_mvp2') THEN 'test2'\n        WHEN CONTAINS(recent_advertiser_test_groups, '#A2:premium_package_mvp_sponsor_confirmation1') AND CONTAINS(recent_advertiser_test_groups, '#B2:jobmo_sep_2024_premium_mvp3') THEN 'test3'\n        ELSE NULL \n    END AS test_grp,\n            job_hash_underscore, daily_dradis_revenue_cents,\n        job_age_days\n--MAX(CASE WHEN daily_dradis_revenue_cents > 0 THEN job_age_days ELSE NULL END) OVER (PARTITION BY job_hash_underscore) AS last_spending_day\nFROM datalake.imhotep.dradis_job2\nWHERE  unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-19') AND (IMHOTEP_UNIXTIME('today')) \nand job_hash_underscore IN (select job_hash from premium_sponsor)\nand \n(CONTAINS(recent_advertiser_test_groups, '#A2:premium_package_mvp_sponsor_confirmation0') or \nCONTAINS(recent_advertiser_test_groups, '#B2:jobmo_sep_2024_premium_mvp1') or \nCONTAINS(recent_advertiser_test_groups, '#B2:jobmo_sep_2024_premium_mvp2') or \nCONTAINS(recent_advertiser_test_groups,'#B2:jobmo_sep_2024_premium_mvp3') or \nCONTAINS(recent_advertiser_test_groups, '#A2:premium_package_mvp_sponsor_confirmation1')) --filter for proctor grp\n--standard dradis filter\nand advertiser_type not in  ('Job Board','Test','NULL','Sandbox') and claim_version ='NULL' \n  and advertiser_internally_reported_industry not in ('Fraud/Scam','Spam','XO/S/JS','Job Seeker')\n  and advertiser_id not in (0, 43233, 459913, 2225717, 3507986, 3879560, 3879585, 3942867, 4205377, 4841581, 5947263, 5948211, 6284290, 6302066, 6384297, 6982716, 6997812, 7249199, 12405367, 16613338, 18552332, 25627469, 26108352, 26108717, 26108792, 26108845, 26387297, 28108792, 34801098, 35492727, 35602484, 38931109, 42045174, 54191120, 57836609, 58047475, 59025892, 59249532, 59272371, 59648098, 60474994, 61299917, 61412142, 61412820, 61413952, 61494280, 61633373, 62478551, 62758988, 62759012, 63508117, 63844048, 63850326, 65323046, 65789593, 66801582, 67462127, 67463298, 67463472, 67463586, 67463657)\n --newly created job\nand date_created>20240919\nand date_created<20241019\nand country IN ('us', 'US') \nand trailing_90_days_dradis_hack_campaign_revenue_cents = 0 --not dradis hack campaign\n--frontend filters\n  and   job_hash_underscore IN (select job_hash from premium_offer)\n  )\n  ,\n  last_spending_day_t as (\n  select test_grp,\n    job_hash_underscore,\n    MAX(CASE WHEN daily_dradis_revenue_cents > 0 THEN job_age_days ELSE NULL END) AS last_spending_day \n    from base_t\n    group by 1,2)\n    \n    \nselect test_grp as dataset, round(avg(last_spending_day),2) as avg_last_spending_day,\napprox_percentile(last_spending_day,0.5) as median_last_spending_day,\napprox_percentile(last_spending_day,0.10) as pct10_last_spending_day, \napprox_percentile(last_spending_day,0.25) as pct25_last_spending_day,\napprox_percentile(last_spending_day,0.75) as pct75_last_spending_day,\napprox_percentile(last_spending_day,0.90) as pct90_last_spending_day,\napprox_percentile(last_spending_day,0.95) as pct95_last_spending_day,\napprox_percentile(last_spending_day,0.99) as pct99_last_spending_day--,count(distinct job_hash_underscore) as jobs \nfrom last_spending_day_t\ngroup by 1\nhaving test_grp is not NULL",
  "queryTables" : [ "datalakehive.imhotep.dradis_job2", "datalakehive.imhotep.sponsormodulesevents2", "datalakehive.imhotep.sponsormodulesevents2", "datalakehive.imhotep.sponsormodulesevents2" ],
  "queryIndex" : 135,
  "runStartToQueryComplete" : 952
}, {
  "elapsedMillis" : 3788,
  "totalScheduledMillis" : 336426,
  "cpuMillis" : 23700,
  "queuedMillis" : 1,
  "executeMillis" : 1647,
  "getResultMillis" : 0,
  "iterateMillis" : 2304,
  "rows" : 5,
  "error" : null,
  "scannedBytes" : 750787135,
  "query" : "select fcc_id, COUNT(*) as unique_visitors\nfrom datalake.glassdoor.unique_visitors\nwhere day between '2024-09-01' and '2024-10-01'\nGROUP BY fcc_id\nlimit 5",
  "queryTables" : [ "datalakehive.glassdoor.unique_visitors" ],
  "queryIndex" : 136,
  "runStartToQueryComplete" : 899
}, {
  "elapsedMillis" : 4725,
  "totalScheduledMillis" : 657035,
  "cpuMillis" : 81010,
  "queuedMillis" : 0,
  "executeMillis" : 2550,
  "getResultMillis" : 0,
  "iterateMillis" : 2193,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 1869083176,
  "query" : "SELECT COUNT(DISTINCT accountid) AS distinct_account_count\nFROM datalake.imhotep.passsigninattempt\nWHERE unixtime > IMHOTEP_UNIXTIME('3month') AND successful = 1",
  "queryTables" : [ "datalakehive.imhotep.passsigninattempt" ],
  "queryIndex" : 137,
  "runStartToQueryComplete" : 904
}, {
  "elapsedMillis" : 21614,
  "totalScheduledMillis" : 35473879,
  "cpuMillis" : 9290693,
  "queuedMillis" : 1,
  "executeMillis" : 2916,
  "getResultMillis" : 0,
  "iterateMillis" : 18730,
  "rows" : 8049,
  "error" : null,
  "scannedBytes" : 543141437285,
  "query" : "WITH most_recent_starts AS (\n    SELECT appid, env, runId, ROW_NUMBER() OVER(PARTITION BY appid, env ORDER BY unixtime DESC) AS startup_num\n    FROM javaruntimeversions\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n    GROUP BY appid, env, runId, unixtime\n), most_recent_runs AS (\n    SELECT mrs.appid, mrs.env, jrv.unixtime AS most_recent_start_unixtime, jrv.osimage, jrv.osimageversionid, jrv.runid AS runid \n    FROM most_recent_starts mrs\n    LEFT JOIN javaruntimeversions jrv ON mrs.runId=jrv.runId AND mrs.startup_num=1\n    WHERE jrv.unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n), service_data AS (\n    SELECT \n        mrr.appid, mrr.env, srs.applicationname, srs.applicationtype, srs.runtimetype, srs.teamid, \n        srs.lifecycle, srs.state, srs.git_project_url,\n        FROM_UNIXTIME(mrr.most_recent_start_unixtime) AS most_recent_start_time, mrr.runid, mrr.osimage, mrr.osimageversionid\n    FROM most_recent_runs mrr\n    LEFT JOIN serviceregistrysnapshots srs ON mrr.appid=srs.applicationid\n    WHERE srs.unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n), scoped_data AS (\n\tSELECT * FROM service_data sd WHERE sd.lifecycle!='UNAVAILABLE'\n), boot_data AS (\n    SELECT \n        sd.*,\n        jlv.libversion AS springboot_version\n    FROM scoped_data sd \n    INNER JOIN javalibversions jlv ON jlv.runid=sd.runid\n    WHERE \n        jlv.unixtime BETWEEN IMHOTEP_UNIXTIME('4y') AND IMHOTEP_UNIXTIME('today')\n        AND jlv.lib='org.springframework.boot:spring-boot'\n)\n\nSELECT sd.*, bd.springboot_version FROM scoped_data sd\nLEFT JOIN boot_data bd ON sd.runid=bd.runid",
  "queryTables" : [ "skipperhive.imhotep.javalibversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.javaruntimeversions", "skipperhive.imhotep.serviceregistrysnapshots", "skipperhive.imhotep.serviceregistrysnapshots" ],
  "queryIndex" : 138,
  "runStartToQueryComplete" : 924
}, {
  "elapsedMillis" : 47188,
  "totalScheduledMillis" : 105260789,
  "cpuMillis" : 5321749,
  "queuedMillis" : 0,
  "executeMillis" : 12411,
  "getResultMillis" : 0,
  "iterateMillis" : 34805,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 190946979115,
  "query" : "SELECT \n\tCOUNT(*) as total_profile_views, \n   \tSUM(r.sent) as outreaches,\n    SUM(r.resp_pos) as resp_pos,\n    COUNT(js.account_id) as profile_with_linkedIn,\n    ROUND(CAST(SUM(r.sent) AS DOUBLE) * 100 / COUNT(*), 1) as outreach_rate\nFROM \n\tdatalake.imhotep.rsresumeview v\n\nLEFT JOIN\n\tdatalake.imhotep.rscontacts r\nON\n\tv.recruiterAccountId = r.senderAccountId\n    AND v.resumeAccountId = r.recipientAccountId\n    AND r.unixtime BETWEEN IMHOTEP_UNIXTIME('29d') AND IMHOTEP_UNIXTIME('1d')\n\nLEFT JOIN \n\tdatalake.imhotep.js_fact_store_snapshot_prod js\nON\n\tv.resumeAccountId = js.account_id\n\tAND js.resume_links_size > 0\n\tAND CARDINALITY(FILTER(js.resume_links_array, x -> x LIKE '%linkedin%')) > 0\n    AND js.unixtime BETWEEN IMHOTEP_UNIXTIME('29d') AND IMHOTEP_UNIXTIME('1d')\n\nWHERE \n\tv.unixtime BETWEEN IMHOTEP_UNIXTIME('29d') AND IMHOTEP_UNIXTIME('1d')\n    AND js.account_id IS NOT NULL -- with LinkedIn link\n--    AND js.account_id IS NULL -- exclude linkedIn users\n--    AND r.batchSize = 1",
  "queryTables" : [ "datalakehive.imhotep.js_fact_store_snapshot_prod", "datalakehive.imhotep.rscontacts", "datalakehive.imhotep.rsresumeview" ],
  "queryIndex" : 139,
  "runStartToQueryComplete" : 952
}, {
  "elapsedMillis" : 130307,
  "totalScheduledMillis" : 416322940,
  "cpuMillis" : 58815538,
  "queuedMillis" : 1,
  "executeMillis" : 42190,
  "getResultMillis" : 0,
  "iterateMillis" : 88145,
  "rows" : 54,
  "error" : null,
  "scannedBytes" : 3857427759470,
  "query" : "SELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n, (case when feedid = 50461 then 'Hosted' else 'Indexed' end) as jobProduct\n--, count(DISTINCT(jobId)) as unutilizedJobs\n, count(DISTINCT(case when has_employer_base_pay = 1 then jobId else null end)) as unutilizedExplicitPayJobs\n, count(DISTINCT(case when jlpostal != '' OR jlenhanced = 1 then jobId else null end)) as unutilizedBetterLocationJobs\n, count(DISTINCT(case when contains(applyvisibility, 'mobile') then jobId else null end)) as unutilizedIndeedApplyableJobs\n, count(DISTINCT(case when has_employer_base_pay = 1 AND contains(applyvisibility, 'mobile') AND (jlpostal != '' OR jlenhanced = 1) then jobId else null end)) as unutilizedBetterJobs\n-- logic for the above fields comes from https://link.indeed.tech/YD476T \nFROM datalake.imhotep.searchablejobs\nWHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \nAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\nAND (waldoVisibilityLevel not in ('organic','jobalert')\n\tAND NOT(waldoVisibilityLevel = 'sponsored' AND sponVisibility = 'spon_active'))\nAND jobcountry != 'JP'\nAND dupestatus != 1 \nGROUP BY 1,2",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 140,
  "runStartToQueryComplete" : 1048
}, {
  "elapsedMillis" : 8152,
  "totalScheduledMillis" : 406500,
  "cpuMillis" : 38701,
  "queuedMillis" : 0,
  "executeMillis" : 4243,
  "getResultMillis" : 0,
  "iterateMillis" : 3940,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 2671892264,
  "query" : "WITH \n\nsubscription_attributes AS (\n\n\tSELECT subscription_id, DATE(purchased_at) AS last_purchase_date\n    FROM datalake.core.product_analytics_dim_resume_subscription_current\n    \n    \t\n),\n\nrsContacts AS (\n\n    SELECT\t\n    \tDATE(FROM_UNIXTIME(unixtime)) AS activity_date,\n        subscriptionid AS subscription_id,\n        advertiserid AS advertiser_id,\n        IF(source = 'MATCHES', 'Match', 'Search') AS product,\n        IF(subscriptiontier = 'PROFESSIONAL' \n        \tAND (last_purchase_date < DATE('2024-04-02') OR last_purchase_date IS NULL)\n            AND source != 'MATCHES', 'Legacy Search', 'Smart Sourcing') AS subscription_tier,\n        SUM(sent) AS contacts_sent,\n        SUM(resp_any) AS all_responses,\n        SUM(resp_pos) AS positive_responses\n    FROM \n    \tdatalake.imhotep.rsContacts rsContacts\n    LEFT JOIN\n        subscription_attributes \n        \tON rsContacts.subscriptionid = subscription_attributes.subscription_id\n    WHERE \n    \tunixtime BETWEEN IMHOTEP_UNIXTIME('2024-04-02') AND IMHOTEP_UNIXTIME('today') \n\tGROUP BY 1, 2, 3, 4, 5\n\n)\n\nSELECT\n\tsubscription_tier,\n    COUNT(DISTINCT advertiser_id) AS num_advertisers,\n    COUNT(DISTINCT subscription_id) AS num_subscriptions,\n    SUM(contacts_sent) AS contacts_sent,\n    SUM(all_responses) AS all_responses,\n    (SUM(all_responses) * 1.0000) / SUM(contacts_sent) AS all_response_rate,\n    SUM(positive_responses) AS positive_responses,\n    (SUM(positive_responses) * 1.0000) / SUM(contacts_sent) AS positive_response_rate\nFROM \n\trsContacts\nWHERE advertiser_id = 2489298\n\nGROUP BY 1\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_dim_resume_subscription_current", "datalakehive.imhotep.rscontacts" ],
  "queryIndex" : 141,
  "runStartToQueryComplete" : 927
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 1247,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_000802_00067_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "-- What are the largest clusters which exactly match between legacy and mvp?\n-- Do the bin files in the jobindex artifact exactly match for those jobs?\n\nwith\n--lets try to collect the clusters into sets, a special case for adding the original into the set also\ncluster_job_array_prep as\n(   select modelId,\n           concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           originalJobId as originalRepostId,\n           array_distinct(array_agg(jobCopyJobId)) as copyJobIds\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and modelId in ('Legacy RepostDetector', 'legacyRepostModel-2.2')\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n    group by 1,2,3\n),\nlegacy_cluster_job_arrays as\n(   select model,\n           originalRepostId,\n           array_min(array_union(copyJobIds, array [originalRepostId])) as newRepostId,\n           array_sort(array_distinct(array_union(copyJobIds, array [originalRepostId]))) as jobIds\n    from cluster_job_array_prep\n    where modelId='Legacy RepostDetector'\n),\nmvp_cluster_job_arrays as\n(   select model,\n           originalRepostId,\n           array_min(array_union(copyJobIds, array [originalRepostId])) as newRepostId,\n           array_sort(array_distinct(array_union(copyJobIds, array [originalRepostId]))) as jobIds\n    from cluster_job_array_prep\n    where modelId='legacyRepostModel-2.2'\n)\nselect lcja.originalRepostId,\n       --lcja.newRepostId,\n       cardinality(coalesce(lcja.jobIds, mcja.jobIds)) as cluster_size\nfrom            legacy_cluster_job_arrays lcja\nfull outer join mvp_cluster_job_arrays    mcja on (    lcja.newRepostId = mcja.newRepostId\n                                                   and lcja.originalRepostId = mcja.originalRepostId\n                                                   and lcja.jobIds = mcja.jobIds)\nwhere lcja.newRepostId is not null\nand   mcja.newRepostId is not null\norder by cluster_size desc\nlimit 1000",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 142,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 13262,
  "totalScheduledMillis" : 6527499,
  "cpuMillis" : 2049539,
  "queuedMillis" : 0,
  "executeMillis" : 2436,
  "getResultMillis" : 0,
  "iterateMillis" : 10869,
  "rows" : 3,
  "error" : null,
  "scannedBytes" : 80374058151,
  "query" : "with convos as (\nselect\n    ce.advertiserId\n   ,eventId\n   ,ce.encryptedConversationId\n   ,(isImported = '1') as isEmail\nFROM conversationEvents ce\njoin convsConversationCreation cc on  ce.encryptedConversationId = cc.encryptedConversationId \n\nWHERE ce.unixtime BETWEEN IMHOTEP_UNIXTIME('181d') AND IMHOTEP_UNIXTIME('1d')\nand   cc.unixtime BETWEEN IMHOTEP_UNIXTIME('195d') AND IMHOTEP_UNIXTIME('1d')\nand ((ce.unixtime - cc.unixtime) < (60*60*24*14))\nand ce.context='APPLICATION'\nand ce.ownerRole != 'JOBSEEKER'\nGROUP BY 1,2,3,4\n)\n\n\n, agg as (\n\nselect advertiserId\n      ,count(*) as totalMessages\n      ,count_if(isEmail) as emailMessages\n      ,count_if(NOT isEmail) as onPlatformMessages \n  from convos\ngroup by 1 \n\n\t\t)\n        \n,advertiserDetails as ( \n\nselect advertiser_id\n   , max_by(parent_company_size_segment, unixtime) as parent_company_size_segment\n   , max_by(type, unixtime) as type\n   , max_by(billing_country, unixtime) as billing_country\n  from  daily_employer2 \n  where unixtime BETWEEN IMHOTEP_UNIXTIME('2d') and IMHOTEP_UNIXTIME('1d')\ngroup by 1\n\n   )\n        \n\n select \n            case when emailMessages> 0 and onPlatformMessages=0 then 'Email Only'\n\t\t\t when  emailMessages=0 and onPlatformMessages>0 then 'Chat Only'\n\t\t\t when  emailMessages>0 and onPlatformMessages>0 then 'Email & Chat'\n             end as usageType\n        ,count(distinct advertiserId) as empCountDistinct\n        ,100.0*count(distinct advertiserId)/sum(count(distinct advertiserId)) over() as empCountDistinct\n        ,count() as empCount\n        ,100.0*count()/sum(count()) over() as pctEmpCount\n\n  from agg\nleft join advertiserDetails on agg.advertiserId=advertiserDetails.advertiser_id\nwhere parent_company_size_segment in ('L', 'XL')\ngroup by 1\norder by 1\n        \n\n\n\n\n",
  "queryTables" : [ "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.convsconversationcreation", "skipperhive.imhotep.daily_employer2" ],
  "queryIndex" : 143,
  "runStartToQueryComplete" : 943
}, {
  "elapsedMillis" : 4112,
  "totalScheduledMillis" : 64081,
  "cpuMillis" : 11646,
  "queuedMillis" : 0,
  "executeMillis" : 2472,
  "getResultMillis" : 0,
  "iterateMillis" : 1675,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 921919405,
  "query" : "WITH \n\nsubscription_attributes AS (\n\n\tSELECT subscription_id, DATE(purchased_at) AS last_purchase_date\n    FROM datalake.core.product_analytics_dim_resume_subscription_current\n    \n    \t\n),\n\nrsContacts AS (\n\n    SELECT\t\n    \tDATE(FROM_UNIXTIME(unixtime)) AS activity_date,\n        subscriptionid AS subscription_id,\n        advertiserid AS advertiser_id,\n        IF(source = 'MATCHES', 'Match', 'Search') AS product,\n        IF(subscriptiontier = 'PROFESSIONAL' \n        \tAND (last_purchase_date < DATE('2024-04-02') OR last_purchase_date IS NULL)\n            AND source != 'MATCHES', 'Legacy Search', 'Smart Sourcing') AS subscription_tier,\n        SUM(sent) AS contacts_sent,\n        SUM(resp_any) AS all_responses,\n        SUM(resp_pos) AS positive_responses\n    FROM \n    \tdatalake.imhotep.rsContacts rsContacts\n    LEFT JOIN\n        subscription_attributes \n        \tON rsContacts.subscriptionid = subscription_attributes.subscription_id\n    WHERE \n    \tunixtime BETWEEN IMHOTEP_UNIXTIME('2024-08-01') AND IMHOTEP_UNIXTIME('today') \n\tGROUP BY 1, 2, 3, 4, 5\n\n)\n\nSELECT\n\tsubscription_tier,\n    COUNT(DISTINCT advertiser_id) AS num_advertisers,\n    COUNT(DISTINCT subscription_id) AS num_subscriptions,\n    SUM(contacts_sent) AS contacts_sent,\n    SUM(all_responses) AS all_responses,\n    (SUM(all_responses) * 1.0000) / SUM(contacts_sent) AS all_response_rate,\n    SUM(positive_responses) AS positive_responses,\n    (SUM(positive_responses) * 1.0000) / SUM(contacts_sent) AS positive_response_rate\nFROM \n\trsContacts\nWHERE advertiser_id = 16775855\n\nGROUP BY 1\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_dim_resume_subscription_current", "datalakehive.imhotep.rscontacts" ],
  "queryIndex" : 144,
  "runStartToQueryComplete" : 954
}, {
  "elapsedMillis" : 18950,
  "totalScheduledMillis" : 1098916,
  "cpuMillis" : 206293,
  "queuedMillis" : 0,
  "executeMillis" : 7815,
  "getResultMillis" : 0,
  "iterateMillis" : 11171,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 2142269502,
  "query" : "with cjs as (\nSELECT \n    dcr.job_hash,\n    sum(cast(dcr.numhiresMade as int)) as numhiresMade,\n    sum(cast(dcr.numHiresMadeIndeed as int)) as numHiresMadeIndeed,\n    sum(cast(dcr.numHiresMadeExternal as int)) as numHiresMadeExternal\nFROM datalake.imhotep.dradisClosedJobReason as dcr\nWHERE dcr.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-04-01') AND IMHOTEP_UNIXTIME('2024-10-24')\n\tand dcr.numhiresMade> '0'\n    and closedJobReason = 'notIndeed'\ngroup by 1\n)\nselect count(distinct cjs.job_hash) as jobs_w_eh,\ncount(distinct hs.jobhash) as jobs_w_eh_n_ih\nfrom cjs \nleft join datalake.imhotep.hiredsignal hs \n\ton hs.unixtime BETWEEN IMHOTEP_UNIXTIME('2022-04-01') AND IMHOTEP_UNIXTIME('2024-10-24') \n    and hs.jobhash = cjs.job_hash \n\n",
  "queryTables" : [ "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.hiredsignal" ],
  "queryIndex" : 145,
  "runStartToQueryComplete" : 972
}, {
  "elapsedMillis" : 226688,
  "totalScheduledMillis" : 5620223,
  "cpuMillis" : 2872814,
  "queuedMillis" : 0,
  "executeMillis" : 5637,
  "getResultMillis" : 0,
  "iterateMillis" : 221063,
  "rows" : 3,
  "error" : null,
  "scannedBytes" : 4426796,
  "query" : "SELECT passwd\nFROM logrepo.log.rozNavTiming\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1000d') AND IMHOTEP_UNIXTIME('1d') \nGROUP BY passwd\nLIMIT 10",
  "queryTables" : [ "logrepo.log.roznavtiming" ],
  "queryIndex" : 146,
  "runStartToQueryComplete" : 1195
}, {
  "elapsedMillis" : 11095,
  "totalScheduledMillis" : 466240,
  "cpuMillis" : 283416,
  "queuedMillis" : 0,
  "executeMillis" : 5082,
  "getResultMillis" : 0,
  "iterateMillis" : 6034,
  "rows" : 4,
  "error" : null,
  "scannedBytes" : 4116152412,
  "query" : "with classifiedData AS (\nSELECT\n    regexp_replace(timberlakeclassificationevent.name, '^datalake-prod', 'datalake') as tableName,\n    timberlakeclassificationevent.fieldname,\n    timberlakeclassificationevent.classification,\n    bool_or(timberlakeclassificationevent.elementsexamined > 0 AND timberlakeclassificationevent.elementsmatching > 0) as contentsMatched\nFROM datalake.imhotep.timberlakeclassificationevent\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-01') AND IMHOTEP_UNIXTIME('2024-10-15')\nAND classificationtype = 'FIELD'\nAND regexp_like(timberlakeclassificationevent.name, '^datalake-prod')\nGROUP BY\n    name, fieldname, classification\nHAVING\n    bool_or(timberlakeclassificationevent.elementsexamined > 0)\n)\nSELECT\n    classification,\n    COUNT(*) AS total,\n    COUNT(CASE WHEN contentsMatched THEN 1 END) AS matched_true,\n    ROUND(100.0 * COUNT(CASE WHEN contentsMatched THEN 1 END) / COUNT(*), 2) AS percentage_true\nFROM\n    classifiedData\nGROUP BY\n    classification",
  "queryTables" : [ "datalakehive.imhotep.timberlakeclassificationevent" ],
  "queryIndex" : 147,
  "runStartToQueryComplete" : 984
}, {
  "elapsedMillis" : 18496,
  "totalScheduledMillis" : 505017,
  "cpuMillis" : 184485,
  "queuedMillis" : 0,
  "executeMillis" : 15414,
  "getResultMillis" : 0,
  "iterateMillis" : 3110,
  "rows" : 4,
  "error" : null,
  "scannedBytes" : 5423913890,
  "query" : "WITH last_active AS\n\t(\n\t\tSELECT  \n            company_id, \n            MAX(CASE WHEN product = 'indeedplus' THEN FROM_UNIXTIME(unixtime) END) AS ppc_last_active,\n            MAX(CASE WHEN COALESCE(product, '') <> 'indeedplus' THEN FROM_UNIXTIME(unixtime) END) AS ppp_last_active\n\t\tFROM \n        \tdatalake.imhotep.recruit_company_jobs\n\t\tWHERE \n        \tunixtime BETWEEN IMHOTEP_UNIXTIME('2022-01-01') AND IMHOTEP_UNIXTIME('today')\n\t\tGROUP BY\n        \t1\n\t)\nSELECT \n\tCASE \n        WHEN ppc_last_active IS NULL THEN 'PPP - Yet to Migrate'\n        WHEN DATE_TRUNC('week', ppp_last_active) < DATE_TRUNC('week', ppc_last_active) THEN 'PPC Migration started - only active on PPC'\n        WHEN DATE_TRUNC('week', ppp_last_active) >= DATE_TRUNC('week', ppc_last_active) THEN 'PPC Migration started - still active on PPP'\n        WHEN ppp_last_active IS NULL THEN 'PPC - Acquired on PPC no prior PPP'\n\tEND AS migration_status, \n\tCOUNT(company_id) AS companies\nFROM \n\tlast_active\nGROUP BY \n\t1",
  "queryTables" : [ "datalakehive.imhotep.recruit_company_jobs" ],
  "queryIndex" : 148,
  "runStartToQueryComplete" : 993
}, {
  "elapsedMillis" : 26109,
  "totalScheduledMillis" : 11270174,
  "cpuMillis" : 2621457,
  "queuedMillis" : 0,
  "executeMillis" : 13006,
  "getResultMillis" : 0,
  "iterateMillis" : 13226,
  "rows" : 48,
  "error" : null,
  "scannedBytes" : 43407092912,
  "query" : "WITH migration_date AS\n\t(\n\t\tSELECT  \n            company_id, \n            MIN(CASE WHEN product = 'indeedplus' THEN FROM_UNIXTIME(unixtime) END) AS ppc_migration_date,\n\t\t\tMAX(CASE WHEN COALESCE(product, '') <> 'indeedplus' THEN FROM_UNIXTIME(unixtime) END) AS ppp_last_date\n\t\tFROM \n        \tdatalake.imhotep.recruit_company_jobs\n\t\tWHERE \n        \tunixtime BETWEEN IMHOTEP_UNIXTIME('2022-01-01') AND IMHOTEP_UNIXTIME('today')\n\t\tGROUP BY\n        \t1\n\t),\nweekly_rev AS\n\t(\n        SELECT \n            r.company_id, \n            DATE_TRUNC('week', FROM_UNIXTIME(unixtime)) AS week,\n            ppc_migration_date,\n            SUM(rev) AS rev,\n            COUNT(DISTINCT DATE_TRUNC('day', FROM_UNIXTIME(unixtime))) AS active_days\n        FROM \n            datalake.imhotep.recruit_company_jobs r\n        INNER JOIN\n            migration_date m\n            ON r.company_id = m.company_id\n        WHERE\n            unixtime BETWEEN IMHOTEP_UNIXTIME('2022-01-01') AND IMHOTEP_UNIXTIME('today')\n            --Only include migrated companies\n            AND ppc_migration_date IS NOT NULL\n            AND ppp_last_date IS NOT NULL\n        GROUP BY \n            1, 2, 3\n        HAVING\n            SUM(rev) > 0\n    ),\npre_post_active_days AS\n\t(\n    \tSELECT\n    \t\tcompany_id,\n            SUM(CASE WHEN week < ppc_migration_date THEN active_days END) AS pre_migration_active_days,\n            SUM(CASE WHEN week >= ppc_migration_date THEN active_days END) AS post_migration_active_days\n\t\tFROM weekly_rev\n        GROUP BY\n        \t1    \n    ),\npre_migration_weekly_baseline AS\n\t(\n        SELECT \n            company_id,\n            1e0*SUM(rev)/COUNT(week) AS pre_migration_wkly_baseline\n        FROM\n            weekly_rev\n        WHERE\n            week < ppc_migration_date\n            AND week >= DATE('2023-10-01')\n        GROUP BY\n            1\n    ),\nweekly_vs_baseline AS\n\t(\n        SELECT \n            r.company_id, \n            r.week, \n            r.rev, \n            pre_migration_wkly_baseline, \n            100*((1e0*r.rev/pre_migration_wkly_baseline)-1) AS pct_change_vs_ppp\n        FROM\n        \tweekly_rev r\n        INNER JOIN \n        \tpre_migration_weekly_baseline b\n        \tON r.company_id = b.company_id\n\t\tINNER JOIN\n      \t\tpre_post_active_days d\n            ON r.company_id = d.company_id\n\t\tWHERE\n\t\t\t--At least 30 days of pre and post migration activity under each company for reasonable evaluation of behavior\n            pre_migration_active_days >= 30\n        \tAND post_migration_active_days >= 30\n            --Show dates up to latest completely populated week in dataset\n            AND WEEK <= (SELECT MAX(week) FROM weekly_rev WHERE active_days = 7)\n\t)\nSELECT \n\tweek,\n    APPROX_PERCENTILE(pct_change_vs_ppp, 0.5) AS median_pct_change_vs_pre_migration_baseline\nFROM \n\tweekly_vs_baseline\nWHERE\n\tweek > DATE('2024-02-01')\nGROUP BY \n\t1\nORDER BY\n\t1",
  "queryTables" : [ "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs" ],
  "queryIndex" : 149,
  "runStartToQueryComplete" : 1017
}, {
  "elapsedMillis" : 21038,
  "totalScheduledMillis" : 93092,
  "cpuMillis" : 11206,
  "queuedMillis" : 0,
  "executeMillis" : 16099,
  "getResultMillis" : 0,
  "iterateMillis" : 4972,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 921748421,
  "query" : "WITH \n\nsubscription_attributes AS (\n\n\tSELECT subscription_id, DATE(purchased_at) AS last_purchase_date\n    FROM datalake.core.product_analytics_dim_resume_subscription_current\n    \n    \t\n),\n\nrsContacts AS (\n\n    SELECT\t\n    \tDATE(FROM_UNIXTIME(unixtime)) AS activity_date,\n        subscriptionid AS subscription_id,\n        advertiserid AS advertiser_id,\n        IF(source = 'MATCHES', 'Match', 'Search') AS product,\n        IF(subscriptiontier = 'PROFESSIONAL' \n        \tAND (last_purchase_date < DATE('2024-04-02') OR last_purchase_date IS NULL)\n            AND source != 'MATCHES', 'Legacy Search', 'Smart Sourcing') AS subscription_tier,\n        SUM(sent) AS contacts_sent,\n        SUM(resp_any) AS all_responses,\n        SUM(resp_pos) AS positive_responses\n    FROM \n    \tdatalake.imhotep.rsContacts rsContacts\n    LEFT JOIN\n        subscription_attributes \n        \tON rsContacts.subscriptionid = subscription_attributes.subscription_id\n    WHERE \n    \tunixtime BETWEEN IMHOTEP_UNIXTIME('2024-08-01') AND IMHOTEP_UNIXTIME('today') \n\tGROUP BY 1, 2, 3, 4, 5\n\n)\n\nSELECT\n\tsubscription_tier,\n    COUNT(DISTINCT advertiser_id) AS num_advertisers,\n    COUNT(DISTINCT subscription_id) AS num_subscriptions,\n    SUM(contacts_sent) AS contacts_sent,\n    SUM(all_responses) AS all_responses,\n    (SUM(all_responses) * 1.0000) / SUM(contacts_sent) AS all_response_rate,\n    SUM(positive_responses) AS positive_responses,\n    (SUM(positive_responses) * 1.0000) / SUM(contacts_sent) AS positive_response_rate\nFROM \n\trsContacts\nWHERE advertiser_id = 16775855\n\nGROUP BY 1\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_dim_resume_subscription_current", "datalakehive.imhotep.rscontacts" ],
  "queryIndex" : 150,
  "runStartToQueryComplete" : 1012
}, {
  "elapsedMillis" : 12785,
  "totalScheduledMillis" : 1014766,
  "cpuMillis" : 97488,
  "queuedMillis" : 0,
  "executeMillis" : 4606,
  "getResultMillis" : 0,
  "iterateMillis" : 8216,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 2142269502,
  "query" : "with cjs as (\nSELECT \n    dcr.job_hash,\n    sum(cast(dcr.numhiresMade as int)) as numhiresMade,\n    sum(cast(dcr.numHiresMadeIndeed as int)) as numHiresMadeIndeed,\n    sum(cast(dcr.numHiresMadeExternal as int)) as numHiresMadeExternal\nFROM datalake.imhotep.dradisClosedJobReason as dcr\nWHERE dcr.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-04-01') AND IMHOTEP_UNIXTIME('2024-10-24')\n\tand dcr.numhiresMade> '0'\n    and closedJobReason = 'notIndeed'\ngroup by 1\n)\nselect count(distinct cjs.job_hash) as jobs_w_eh,\ncount(distinct hs.jobhash) as jobs_w_eh_n_ih\nfrom cjs \nleft join datalake.imhotep.hiredsignal hs \n\ton hs.unixtime BETWEEN IMHOTEP_UNIXTIME('2022-04-01') AND IMHOTEP_UNIXTIME('2024-10-24') \n    and hs.jobhash = cjs.job_hash \n\n",
  "queryTables" : [ "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.hiredsignal" ],
  "queryIndex" : 151,
  "runStartToQueryComplete" : 1012
}, {
  "elapsedMillis" : 19639,
  "totalScheduledMillis" : 7243878,
  "cpuMillis" : 1826844,
  "queuedMillis" : 1,
  "executeMillis" : 7609,
  "getResultMillis" : 0,
  "iterateMillis" : 12129,
  "rows" : 70,
  "error" : null,
  "scannedBytes" : 28898978352,
  "query" : "WITH migration_date AS\n\t(\n\t\tSELECT  \n            company_id, \n            MIN(CASE WHEN product = 'indeedplus' THEN FROM_UNIXTIME(unixtime) END) AS ppc_migration_date,\n\t\t\tMAX(CASE WHEN COALESCE(product, '') <> 'indeedplus' THEN FROM_UNIXTIME(unixtime) END) AS ppp_last_date\n\t\tFROM \n        \tdatalake.imhotep.recruit_company_jobs\n\t\tWHERE \n        \tunixtime BETWEEN IMHOTEP_UNIXTIME('2022-01-01') AND IMHOTEP_UNIXTIME('today')\n\t\tGROUP BY\n        \t1\n\t),\nmonthly_companies AS\n\t(\n\t\tSELECT  \n        \tc.company_id, \n\t\t\t(DATE_TRUNC('month', FROM_UNIXTIME(unixtime))) AS month,\n            (DATE_TRUNC('month', ppc_migration_date)) as migration_month,\n\t\t\tSUM(CASE WHEN COALESCE(product, '') <> 'indeedplus' THEN 1 END) AS ppp_count,\n            SUM(CASE WHEN product = 'indeedplus' THEN 1 END) AS ppc_count,\n            --Identify PPP activity after PPC migration has already started within the same month\n            SUM(CASE WHEN \n            \tCOALESCE(product, '') <> 'indeedplus' \n                AND FROM_UNIXTIME(unixtime) > ppc_migration_date \n\t\t\tTHEN 1 END) AS ppp_after_ppc_count\n\t\tFROM \n        \tdatalake.imhotep.recruit_company_jobs c\n        LEFT JOIN\n        \tmigration_date m\n            ON c.company_id = m.company_id    \n\t\tWHERE \n        \tunixtime BETWEEN IMHOTEP_UNIXTIME('2023-03-01') AND IMHOTEP_UNIXTIME('today')\n\t\tGROUP BY \n        \t1, 2, 3\n\t),\nmonthly_companies_labelled AS\n\t(\n\t\tSELECT  \n        \tc.*,\n\t\t\tCASE \n            \tWHEN (ppc_migration_date IS NULL OR month < migration_month) THEN 'PPP - Yet to migrate'\n\t\t\t\tWHEN ppp_last_DATE IS NULL THEN 'PPC - Acquired on PPC no prior PPP'\n                WHEN \n                \t(COALESCE(ppp_count, 0) > 0\n                    AND ppc_migration_date IS NOT NULL\n                    AND month > migration_month)\n\t\t\t\t\tOR ((COALESCE(ppp_after_ppc_count, 0)) > 0 AND month = migration_month)\n\t\t\t\tTHEN 'PPC Migration started - still active on PPP'\n                WHEN \n                \t(COALESCE(ppp_count, 0) = 0 \n                    AND ppc_migration_date IS NOT NULL\n                    AND month > migration_month\n                    )\n                    OR ((COALESCE(ppp_after_ppc_count, 0)) = 0 AND month = migration_month)\n\t\t\t\tTHEN 'PPC Migration started - only active on PPC'\n\t\t\tEND AS cohort\n\t\tFROM \n        \tmonthly_companies c\n        LEFT JOIN\n        \tmigration_date m\n            ON c.company_id = m.company_id    \n    )\nSELECT\n\tCAST(base_mth.month + INTERVAL '1' MONTH AS DATE) AS month, \n    base_mth.cohort,\n    COUNT(DISTINCT base_mth.company_id) AS companies, \n    COUNT(DISTINCT next_mth.company_id) AS retained_companies,\n\t100*(1e0*COUNT(DISTINCT next_mth.company_id)/COUNT(DISTINCT base_mth.company_id)) AS retention_rate,\n\t100-(100*(1e0*COUNT(DISTINCT next_mth.company_id)/COUNT(DISTINCT base_mth.company_id))) AS churn_rate\nFROM \n\tmonthly_companies_labelled AS base_mth\nLEFT JOIN\n\tmonthly_companies_labelled AS next_mth\n\tON base_mth.company_id = next_mth.company_id\n\tAND base_mth.cohort = next_mth.cohort\n\tAND base_mth.month + INTERVAL '1' MONTH = next_mth.month\nWHERE \n\tbase_mth.month + INTERVAL '1' MONTH <= DATE_TRUNC('month', CURRENT_TIMESTAMP)\nGROUP BY\n\t1, 2\nORDER BY\n\t1",
  "queryTables" : [ "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs" ],
  "queryIndex" : 152,
  "runStartToQueryComplete" : 1020
}, {
  "elapsedMillis" : 17114,
  "totalScheduledMillis" : 10833185,
  "cpuMillis" : 2459839,
  "queuedMillis" : 0,
  "executeMillis" : 3213,
  "getResultMillis" : 0,
  "iterateMillis" : 14020,
  "rows" : 48,
  "error" : null,
  "scannedBytes" : 43407092912,
  "query" : "WITH migration_date AS\n\t(\n\t\tSELECT  \n            company_id, \n            MIN(CASE WHEN product = 'indeedplus' THEN FROM_UNIXTIME(unixtime) END) AS ppc_migration_date,\n\t\t\tMAX(CASE WHEN COALESCE(product, '') <> 'indeedplus' THEN FROM_UNIXTIME(unixtime) END) AS ppp_last_date\n\t\tFROM \n        \tdatalake.imhotep.recruit_company_jobs\n\t\tWHERE \n        \tunixtime BETWEEN IMHOTEP_UNIXTIME('2022-01-01') AND IMHOTEP_UNIXTIME('today')\n\t\tGROUP BY\n        \t1\n\t),\nweekly_rev AS\n\t(\n        SELECT \n            r.company_id, \n            DATE_TRUNC('week', FROM_UNIXTIME(unixtime)) AS week,\n            ppc_migration_date,\n            SUM(rev) AS rev,\n            COUNT(DISTINCT DATE_TRUNC('day', FROM_UNIXTIME(unixtime))) AS active_days\n        FROM \n            datalake.imhotep.recruit_company_jobs r\n        INNER JOIN\n            migration_date m\n            ON r.company_id = m.company_id\n        WHERE\n            unixtime BETWEEN IMHOTEP_UNIXTIME('2022-01-01') AND IMHOTEP_UNIXTIME('today')\n            --Only include migrated companies\n            AND ppc_migration_date IS NOT NULL\n            AND ppp_last_date IS NOT NULL\n        GROUP BY \n            1, 2, 3\n        HAVING\n            SUM(rev) > 0\n    ),\npre_post_active_days AS\n\t(\n    \tSELECT\n    \t\tcompany_id,\n            SUM(CASE WHEN week < ppc_migration_date THEN active_days END) AS pre_migration_active_days,\n            SUM(CASE WHEN week >= ppc_migration_date THEN active_days END) AS post_migration_active_days\n\t\tFROM weekly_rev\n        GROUP BY\n        \t1    \n    ),\npre_migration_weekly_baseline AS\n\t(\n        SELECT \n            company_id,\n            1e0*SUM(rev)/COUNT(week) AS pre_migration_wkly_baseline\n        FROM\n            weekly_rev\n        WHERE\n            week < ppc_migration_date\n            AND week >= DATE('2024-01-01')\n        GROUP BY\n            1\n    ),\nweekly_vs_baseline AS\n\t(\n        SELECT \n            r.company_id, \n            r.week, \n            r.rev, \n            pre_migration_wkly_baseline, \n            100*((1e0*r.rev/pre_migration_wkly_baseline)-1) AS pct_change_vs_ppp\n        FROM\n        \tweekly_rev r\n        INNER JOIN \n        \tpre_migration_weekly_baseline b\n        \tON r.company_id = b.company_id\n\t\tINNER JOIN\n      \t\tpre_post_active_days d\n            ON r.company_id = d.company_id\n\t\tWHERE\n        \t--At least 30 days of pre and post migration activity under each company for reasonable evaluation of behavior\n            pre_migration_active_days >= 30\n        \tAND post_migration_active_days >= 30\n            --Show dates up to latest completely populated week in dataset\n            AND WEEK <= (SELECT MAX(week) FROM weekly_rev WHERE active_days = 7)\n\t)\nSELECT \n\tweek,\n    APPROX_PERCENTILE(pct_change_vs_ppp, 0.5) AS median_pct_change_vs_pre_migration_baseline\nFROM \n\tweekly_vs_baseline\nWHERE\n\tweek > DATE('2024-02-01')\nGROUP BY \n\t1\nORDER BY\n\t1",
  "queryTables" : [ "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs" ],
  "queryIndex" : 153,
  "runStartToQueryComplete" : 1019
}, {
  "elapsedMillis" : 10034,
  "totalScheduledMillis" : 397615,
  "cpuMillis" : 29934,
  "queuedMillis" : 0,
  "executeMillis" : 2950,
  "getResultMillis" : 0,
  "iterateMillis" : 7121,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 2697087613,
  "query" : "WITH \n\nsubscription_attributes AS (\n\n\tSELECT subscription_id, DATE(purchased_at) AS last_purchase_date\n    FROM datalake.core.product_analytics_dim_resume_subscription_current\n    \n    \t\n),\n\nrsContacts AS (\n\n    SELECT\t\n    \tDATE(FROM_UNIXTIME(unixtime)) AS activity_date,\n        subscriptionid AS subscription_id,\n        advertiserid AS advertiser_id,\n        IF(source = 'MATCHES', 'Match', 'Search') AS product,\n        IF(subscriptiontier = 'PROFESSIONAL' \n        \tAND (last_purchase_date < DATE('2024-04-02') OR last_purchase_date IS NULL)\n            AND source != 'MATCHES', 'Legacy Search', 'Smart Sourcing') AS subscription_tier,\n        SUM(sent) AS contacts_sent,\n        SUM(resp_any) AS all_responses,\n        SUM(resp_pos) AS positive_responses\n    FROM \n    \tdatalake.imhotep.rsContacts rsContacts\n    LEFT JOIN\n        subscription_attributes \n        \tON rsContacts.subscriptionid = subscription_attributes.subscription_id\n    WHERE \n    \tunixtime BETWEEN IMHOTEP_UNIXTIME('2024-01-01') AND IMHOTEP_UNIXTIME('today') \n\tGROUP BY 1, 2, 3, 4, 5\n\n)\n\nSELECT\n\tsubscription_tier,\n    COUNT(DISTINCT advertiser_id) AS num_advertisers,\n    COUNT(DISTINCT subscription_id) AS num_subscriptions,\n    SUM(contacts_sent) AS contacts_sent,\n    SUM(all_responses) AS all_responses,\n    (SUM(all_responses) * 1.0000) / SUM(contacts_sent) AS all_response_rate,\n    SUM(positive_responses) AS positive_responses,\n    (SUM(positive_responses) * 1.0000) / SUM(contacts_sent) AS positive_response_rate\nFROM \n\trsContacts\nWHERE advertiser_id = 16775855\n\nGROUP BY 1\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_dim_resume_subscription_current", "datalakehive.imhotep.rscontacts" ],
  "queryIndex" : 154,
  "runStartToQueryComplete" : 1012
}, {
  "elapsedMillis" : 4882,
  "totalScheduledMillis" : 6660,
  "cpuMillis" : 1408,
  "queuedMillis" : 0,
  "executeMillis" : 3962,
  "getResultMillis" : 0,
  "iterateMillis" : 1004,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 41494655,
  "query" : "WITH phone_8x8_surveys AS (\n\tSELECT \n      'Live Phone' as channel,\n      'Phone CSAT' as survey_name,\n      CAST(rep_id AS VARCHAR) as rep_id,\n      cs_rep_full_name as rep_full_name,\n      cs_rep_ldap,\n      cs_rep_sup_org_name,\n      cs_rep_team,\n      final_agent_id,\n      final_agent_email,\n      call_datetime as response_date,\n      call_date as survey_date,\n      transaction_id as response_id,\n      call_id as survey_instance_id,\n      'Live Phone' as interaction_type,\n      queue_name,\n      TRY_CAST(q1_response as double) as csat_score,\n      q2_response as indeed_score,\n      q3_response as csat_feedback\n\tFROM datalake.analysis.surveys_8x8_post_call_survey\n),\nsalesforce_case_data AS (\n    SELECT \n      casenumber,\n      status,\n      closeddate,\n      type_cs__c,\n      sub_type_cs__c,\n      product__c,\n      owner_name__c,\n      campaign_type__c,\n      subject\n    FROM datalake.salesforce.\"case\" \n),\nsalesforce_user_data AS (\n\tSELECT\n    \tadc_user_id__c, \n        name, \n        username, \n        email, \n        alias, \n        department, \n        lastname\n\tFROM datalake.salesforce.\"user\"\n)\nSELECT \n\tphone_8x8_surveys.*,\n    salesforce_user_data.name,\n    salesforce_user_data.email,\n    salesforce_user_data.alias,\n    salesforce_user_data.department,\n    salesforce_user_data.lastname\nFROM phone_8x8_surveys\nLEFT JOIN salesforce_user_data\nON phone_8x8_surveys.rep_id = salesforce_user_data.adc_user_id__c\nWHERE csat_score IS NOT NULL AND csat_score  > 0.0\nLIMIT 100",
  "queryTables" : [ "datalakehive.analysis.surveys_8x8_post_call_survey", "datalakehive.salesforce.user" ],
  "queryIndex" : 155,
  "runStartToQueryComplete" : 1007
}, {
  "elapsedMillis" : 6152,
  "totalScheduledMillis" : 1522588,
  "cpuMillis" : 158796,
  "queuedMillis" : 0,
  "executeMillis" : 1106,
  "getResultMillis" : 0,
  "iterateMillis" : 5067,
  "rows" : 655,
  "error" : null,
  "scannedBytes" : 8487557839,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('minute', from_unixtime(lp.unixtime)) as date,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-29')\n        and moderationApi='openAIModerations' and moderationType='input'\n    GROUP BY\n        date_trunc('minute', from_unixtime(lp.unixtime)),\n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 240 as double) / cast(30 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30))) as change,\n        sum(1.0) over (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count             \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n\t\traw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n         \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \nchange,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,        \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 240), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 15 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 240)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 156,
  "runStartToQueryComplete" : 1010
}, {
  "elapsedMillis" : 8085,
  "totalScheduledMillis" : 1507278,
  "cpuMillis" : 205126,
  "queuedMillis" : 0,
  "executeMillis" : 1661,
  "getResultMillis" : 0,
  "iterateMillis" : 6466,
  "rows" : 1414,
  "error" : null,
  "scannedBytes" : 8789187843,
  "query" : "WITH daily_data AS (\n    SELECT\n        date_trunc('minute', from_unixtime(lp.unixtime)) as date,\n        appName,\n        moderationApi, \n        moderationType, \n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 30 PRECEDING AND current row) AS request_count,\n\t\tSUM(SUM(CAST(lp.flagged  AS DOUBLE))) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_block_count,\n\t\tSUM(COUNT(*)) OVER (partition by appName, moderationApi, moderationType ORDER BY date_trunc('minute', from_unixtime(lp.unixtime)) ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS weekly_request_count\n    FROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.flagged IS NOT NULL\n        AND lp.metric = 'moderation-text'\n        AND unixtime BETWEEN imhotep_unixtime(timestamp '2024-10-24') AND imhotep_unixtime(timestamp '2024-10-29')\n        and moderationApi='openAIModerations' and moderationType='input'\n        -- see rate_change query for filters by appName etc.\n    GROUP BY\n        date_trunc('minute', from_unixtime(lp.unixtime)),\n\t\tappName,         \n        moderationApi, \n        moderationType \n),\nblock_rate AS (\n    SELECT\n        date,\n\t\tappName, \n        moderationApi, \n        moderationType,\n        request_count,\n        weekly_request_count,\n        cast(request_count * 240 as double) / cast(30 * weekly_request_count as double) as relative_request_rate,\n        cast(block_count as double) / request_count as raw_block_rate,\n        -- We adjust the proportion estimates by adding z^2/2 successes and z^2/2 failures (z^2 obs total) to the counts.\n        -- This can be thought of as a Bayesian prior, or as a correction to the Normal approximation for rates close to 0 or 1.\n        -- For large values of request_count this converges to the raw proportion (block_rate ~ raw_block_rate).\n        -- Using z=3.0, in other words, we're going to require a \"3-sigma\" detection.\n        cast(block_count + 4.5 as double) / (request_count + 9.0) as block_rate, -- corrected/smoothed\n        cast(block_count as double) / (request_count + 9.0) as block_rate_zc, -- zero-centered corrected/smoothed rate (only adding to denominator)\n        cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate, -- corrected/smoothed\n        cast(weekly_block_count as double) / (weekly_request_count + (240 * 9.0 / 30)) as  weekly_block_rate_zc, -- corrected/smoothed        \n        cast(weekly_block_count as double) /  weekly_request_count as raw_weekly_block_rate,  \n        (cast(block_count + 4.5 as double) / (request_count + 9.0)) - (cast(weekly_block_count + (240 * 4.5 / 30) as double) / (weekly_request_count + (240 * 9.0 / 30))) as change,\n        sum(1.0) over (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 1 PRECEDING) as previous_dates_count\n    FROM\n        daily_data\n),\n\nblock_rate_change AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        request_count,\n\t\trelative_request_rate,        \n        weekly_request_count,\n        STDDEV(block_rate-weekly_block_rate) OVER (partition by appName, moderationApi, moderationType ORDER BY date ROWS BETWEEN 240 + 30 PRECEDING AND 30 PRECEDING) AS stddev_obs,\n        SQRT((block_rate * (1.0 - block_rate) / request_count) + ((weekly_block_rate * (1.0 - weekly_block_rate) / weekly_request_count))) as stddev_approx,\n        raw_block_rate,  \n        block_rate,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        raw_weekly_block_rate,  \n        weekly_block_rate, -- corrected/smoothed\n        change,\n        case weekly_block_rate_zc\n           when 0 then 0\n           else (block_rate_zc - weekly_block_rate_zc) \n        end as change_zc,\n\t\tprevious_dates_count        \n    FROM\n        block_rate\n),\nconfidence_interval AS (\n    SELECT\n        date,\n\t\tappName,         \n        moderationApi, \n        moderationType,\n        stddev_obs,\n\t\tstddev_approx,\n        greatest(stddev_approx, stddev_obs) as stddev,\n        block_rate,\n        block_rate - 6 * greatest(stddev_approx, stddev_obs)  AS block_rate_lower_bound,\n        block_rate + 6 * greatest(stddev_approx, stddev_obs) AS block_rate_upper_bound,          \n        weekly_block_rate,\n\t\tweekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,        \n        change,\n        change_zc,\n        block_rate_zc,\n        weekly_block_rate_zc,\n        request_count,\n\t\trelative_request_rate,                \n        change - 6 * greatest(stddev_approx, stddev_obs) AS change_lower_bound,\n        change + 6 * greatest(stddev_approx, stddev_obs) AS change_upper_bound,\n        previous_dates_count,\n        row_number() OVER (partition by moderationApi, moderationType ORDER BY date) as row_n \n    FROM\n        block_rate_change\n    --WHERE\n    --\tweekly_request_count > 100\n    --    and request_count > 100\n)\nSELECT\n    date,\n\tappName,     \n    moderationApi, \n    moderationType,\n    request_count,\n\trelative_request_rate,            \n    stddev_obs,\n    stddev_approx,\n    stddev,\n    block_rate,\n\tblock_rate_lower_bound,\n    block_rate_upper_bound,\n    weekly_block_rate,\n    weekly_request_count,\n        raw_block_rate,        \n        raw_weekly_block_rate,    \n    change,\n    change / weekly_block_rate as relative_change,\n    change_zc / weekly_block_rate_zc as relative_change_zc,\n    change_zc,\n    block_rate_zc,\n    weekly_block_rate_zc,\n    \n    change_lower_bound,\n    change_upper_bound,\n    change_lower_bound * 1.0 / weekly_block_rate as relative_change_lower_bound,\n    change_upper_bound * 1.0 / weekly_block_rate as relative_change_upper_bound,\n    greatest(change_lower_bound / weekly_block_rate, 0) as relative_change_lower_bound_sig,\n    least(change_upper_bound / weekly_block_rate, 0) as relative_change_upper_bound_sig,     \n    IF((change_lower_bound > 0 or change_upper_bound<0) and (previous_dates_count > 240), 1, 0) as is_sig_change,\n    IF((change_lower_bound > 0 or change_upper_bound<0) and (date = (max(date) over ())) and (previous_dates_count > 6), 1, 0) as is_latest_date_sig_change,\n    change/stddev as z_score,\n    IF(date = (max(date) over ()), change/stddev, NULL) as latest_date_z_score, \n    max(date) over () as latest_data_date,\n    previous_dates_count            \nFROM\n    confidence_interval -- Note: return values for all dates unlike with the alerting query\nWHERE\n    (change/stddev) IS NOT NULL\n    and ((row_n % 15 = 0)  or (abs(change/stddev) > 6))\n    and previous_dates_count > (0.9 * 240)\nORDER BY\n    date DESC, moderationApi ASC, moderationType DESC",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 157,
  "runStartToQueryComplete" : 1012
}, {
  "elapsedMillis" : 10993,
  "totalScheduledMillis" : 2026588,
  "cpuMillis" : 511273,
  "queuedMillis" : 0,
  "executeMillis" : 3302,
  "getResultMillis" : 0,
  "iterateMillis" : 7719,
  "rows" : 37,
  "error" : null,
  "scannedBytes" : 8857199088,
  "query" : "WITH migration_date AS\n\t(\n\t\tSELECT  \n            company_id, \n\t\t\tMIN(CASE WHEN COALESCE(product, '') = 'indeedplus' THEN FROM_UNIXTIME(unixtime) END) AS ppc_migration_date,\n\t\t\tMAX(CASE WHEN COALESCE(product, '') <> 'indeedplus' THEN FROM_UNIXTIME(unixtime) END) AS ppp_last_date,\n            --Last activity date regardless of whether PPC or PPP\n            MAX(FROM_UNIXTIME(unixtime)) AS all_last_date\n\t\tFROM \n        \tdatalake.imhotep.recruit_company_jobs\n\t\tWHERE \n        \tunixtime BETWEEN IMHOTEP_UNIXTIME('2022-01-01') AND IMHOTEP_UNIXTIME('today')\n\t\tGROUP BY\n        \t1\n\t),\nmonthly_companies AS\n\t(\n\t\tSELECT  \n        \tc.company_id, \n\t\t\tCAST(DATE_TRUNC('month', FROM_UNIXTIME(unixtime)) AS DATE) AS month\n\t\tFROM \n        \tdatalake.imhotep.recruit_company_jobs c\n\t\tWHERE \n        \tunixtime BETWEEN IMHOTEP_UNIXTIME('2023-04-01') AND IMHOTEP_UNIXTIME('today')\n\t\tGROUP BY \n        \t1, 2\n\t),\nmonthly_companies_labelled AS\n\t(\n\t\tSELECT\n\t\t\tc.company_id,\n            month,\n            CASE\n            \tWHEN month = CAST(DATE_TRUNC('month', all_last_date) AS DATE) THEN 1 \n\t\t\tEND AS final_active_month_flag,\n            CASE \n            \tWHEN \n                    ppc_migration_date IS NULL \n                    --Retain yet to migrate label for months prior to migration starting\n                    OR month < CAST(DATE_TRUNC('month', ppc_migration_date) AS DATE) \n                THEN 'Yet to start PPC migration'\n\t\t\t\tWHEN\n                \t--Assign migration started label from first month with PPC activity onwards\n                \tCAST(DATE_TRUNC('month', ppc_migration_date) AS DATE) <= month \n\t\t\t\tTHEN 'PPC Migration started'\n\t\t\tEND AS cohort\n\t\tFROM \n        \tmonthly_companies c\n        LEFT JOIN\n        \tmigration_date m\n            ON c.company_id = m.company_id \n\t)\nSELECT\n\tmonth, \n    cohort, \n    COUNT(DISTINCT company_id) AS companies, \n    SUM(final_active_month_flag) AS fully_churned_companies,\n\t100*(1e0*SUM(final_active_month_flag)/COUNT(DISTINCT company_id)) AS fully_churned_rate\nFROM \n\tmonthly_companies_labelled\nWHERE \n\tmonth < DATE_TRUNC('month', CURRENT_DATE)\nGROUP BY \n\t1, 2\nORDER BY\n\t1",
  "queryTables" : [ "datalakehive.imhotep.recruit_company_jobs", "datalakehive.imhotep.recruit_company_jobs" ],
  "queryIndex" : 158,
  "runStartToQueryComplete" : 1015
}, {
  "elapsedMillis" : 25868,
  "totalScheduledMillis" : 54488184,
  "cpuMillis" : 17084080,
  "queuedMillis" : 1,
  "executeMillis" : 5260,
  "getResultMillis" : 0,
  "iterateMillis" : 20634,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 353644287833,
  "query" : "WITH date_range AS (\n    SELECT\n        IMHOTEP_UNIXTIME('2month') AS start_date,\n        IMHOTEP_UNIXTIME('today') AS end_date\n)\nSELECT\n    COUNT(DISTINCT signin.accountid)\nFROM\n    datalake.imhotep.passdailysnapshot snapshot\nJOIN\n    datalake.imhotep.passsigninattempt signin ON snapshot.accountid = signin.accountid\nJOIN\n    date_range dr ON snapshot.unixtime BETWEEN dr.start_date AND dr.end_date\n    AND signin.unixtime BETWEEN dr.start_date AND dr.end_date\nWHERE\n    snapshot.deleted = 0\n    AND signin.successful = 1",
  "queryTables" : [ "datalakehive.imhotep.passdailysnapshot", "datalakehive.imhotep.passsigninattempt" ],
  "queryIndex" : 159,
  "runStartToQueryComplete" : 1048
}, {
  "elapsedMillis" : 8029,
  "totalScheduledMillis" : 1736892,
  "cpuMillis" : 89784,
  "queuedMillis" : 0,
  "executeMillis" : 3417,
  "getResultMillis" : 0,
  "iterateMillis" : 4639,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 5508504082,
  "query" : "WITH \n\nsubscription_attributes AS (\n\n\tSELECT subscription_id, DATE(purchased_at) AS last_purchase_date\n    FROM datalake.core.product_analytics_dim_resume_subscription_current\n    \n    \t\n),\n\nrsContacts AS (\n\n    SELECT\t\n    \tDATE(FROM_UNIXTIME(unixtime)) AS activity_date,\n        subscriptionid AS subscription_id,\n        advertiserid AS advertiser_id,\n        IF(source = 'MATCHES', 'Match', 'Search') AS product,\n        IF(subscriptiontier = 'PROFESSIONAL' \n        \tAND (last_purchase_date < DATE('2023-01-02') OR last_purchase_date IS NULL)\n            AND source != 'MATCHES', 'Legacy Search', 'Smart Sourcing') AS subscription_tier,\n        SUM(sent) AS contacts_sent,\n        SUM(resp_any) AS all_responses,\n        SUM(resp_pos) AS positive_responses\n    FROM \n    \tdatalake.imhotep.rsContacts rsContacts\n    LEFT JOIN\n        subscription_attributes \n        \tON rsContacts.subscriptionid = subscription_attributes.subscription_id\n    WHERE \n    \tunixtime BETWEEN IMHOTEP_UNIXTIME('2023-01-01') AND IMHOTEP_UNIXTIME('today') \n\tGROUP BY 1, 2, 3, 4, 5\n\n)\n\nSELECT\n\tsubscription_tier,\n    COUNT(DISTINCT advertiser_id) AS num_advertisers,\n    COUNT(DISTINCT subscription_id) AS num_subscriptions,\n    SUM(contacts_sent) AS contacts_sent,\n    SUM(all_responses) AS all_responses,\n    (SUM(all_responses) * 1.0000) / SUM(contacts_sent) AS all_response_rate,\n    SUM(positive_responses) AS positive_responses,\n    (SUM(positive_responses) * 1.0000) / SUM(contacts_sent) AS positive_response_rate\nFROM \n\trsContacts\nWHERE advertiser_id = 16775855\n\nGROUP BY 1\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_dim_resume_subscription_current", "datalakehive.imhotep.rscontacts" ],
  "queryIndex" : 160,
  "runStartToQueryComplete" : 1034
}, {
  "elapsedMillis" : 3307,
  "totalScheduledMillis" : 53643,
  "cpuMillis" : 7871,
  "queuedMillis" : 0,
  "executeMillis" : 2081,
  "getResultMillis" : 0,
  "iterateMillis" : 1252,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 917662605,
  "query" : "WITH \n\nsubscription_attributes AS (\n\n\tSELECT subscription_id, DATE(purchased_at) AS last_purchase_date\n    FROM datalake.core.product_analytics_dim_resume_subscription_current\n    \n    \t\n),\n\nrsContacts AS (\n\n    SELECT\t\n    \tDATE(FROM_UNIXTIME(unixtime)) AS activity_date,\n        subscriptionid AS subscription_id,\n        advertiserid AS advertiser_id,\n        IF(source = 'MATCHES', 'Match', 'Search') AS product,\n        IF(subscriptiontier = 'PROFESSIONAL' \n        \tAND (last_purchase_date < DATE('2024-08-02') OR last_purchase_date IS NULL)\n            AND source != 'MATCHES', 'Legacy Search', 'Smart Sourcing') AS subscription_tier,\n        SUM(sent) AS contacts_sent,\n        SUM(resp_any) AS all_responses,\n        SUM(resp_pos) AS positive_responses\n    FROM \n    \tdatalake.imhotep.rsContacts rsContacts\n    LEFT JOIN\n        subscription_attributes \n        \tON rsContacts.subscriptionid = subscription_attributes.subscription_id\n    WHERE \n    \tunixtime BETWEEN IMHOTEP_UNIXTIME('2024-08-02') AND IMHOTEP_UNIXTIME('today') \n\tGROUP BY 1, 2, 3, 4, 5\n\n)\n\nSELECT\n\tsubscription_tier,\n    COUNT(DISTINCT advertiser_id) AS num_advertisers,\n    COUNT(DISTINCT subscription_id) AS num_subscriptions,\n    SUM(contacts_sent) AS contacts_sent,\n    SUM(all_responses) AS all_responses,\n    (SUM(all_responses) * 1.0000) / SUM(contacts_sent) AS all_response_rate,\n    SUM(positive_responses) AS positive_responses,\n    (SUM(positive_responses) * 1.0000) / SUM(contacts_sent) AS positive_response_rate\nFROM \n\trsContacts\nWHERE advertiser_id = 16775855\n\nGROUP BY 1\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_dim_resume_subscription_current", "datalakehive.imhotep.rscontacts" ],
  "queryIndex" : 161,
  "runStartToQueryComplete" : 1061
}, {
  "elapsedMillis" : 156734,
  "totalScheduledMillis" : 150491936,
  "cpuMillis" : 129192151,
  "queuedMillis" : 1,
  "executeMillis" : 2601,
  "getResultMillis" : 0,
  "iterateMillis" : 154158,
  "rows" : 94,
  "error" : null,
  "scannedBytes" : 12457920451405,
  "query" : "SELECT \n\tELEMENT_AT(data, 'rum_context_element') as element,\n    ELEMENT_AT(data, 'rum_context_module') as module\nFROM \n\tlogrepo.raw_log.one_host_rum_action\nWHERE \n\tunixtime > IMHOTEP_UNIXTIME('yesterday') AND\n    ELEMENT_AT(data, 'rum_view_name') = '/jobs' AND\n    ELEMENT_AT(data, 'rum_context_scope') = 'jobman-feature-modules' AND\n    ELEMENT_AT(data, 'rum_context_customActionType') = 'click' AND\n    ELEMENT_AT(data, 'rum_view_url') LIKE '%?%' AND\n    ELEMENT_AT(data, 'rum_context_additional_searchTkUuid') IS NOT NULL AND\n    ELEMENT_AT(data, 'rum_context_additional_clickDepthIndex') IS NOT NULL AND\n    ELEMENT_AT(data, 'rum_context_additional_clickDepthPage') IS NOT NULL\n--    CARDINALITY(SPLIT(ELEMENT_AT(data, 'rum_context_additional_clickDepthIndex'), ',')) > 1 AND\n--    CARDINALITY(SPLIT(ELEMENT_AT(data, 'rum_context_additional_clickDepthPage'), ',')) > 1\nGROUP BY\n\tELEMENT_AT(data, 'rum_context_element'),\n    ELEMENT_AT(data, 'rum_context_module')",
  "queryTables" : [ "logrepo.raw_log.one_host_rum_action" ],
  "queryIndex" : 162,
  "runStartToQueryComplete" : 1222
}, {
  "elapsedMillis" : 3178,
  "totalScheduledMillis" : 60455,
  "cpuMillis" : 12706,
  "queuedMillis" : 0,
  "executeMillis" : 1749,
  "getResultMillis" : 0,
  "iterateMillis" : 1473,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 917504775,
  "query" : "WITH \n\nsubscription_attributes AS (\n\n\tSELECT subscription_id, DATE(purchased_at) AS last_purchase_date\n    FROM datalake.core.product_analytics_dim_resume_subscription_current\n    \n    \t\n),\n\nrsContacts AS (\n\n    SELECT\t\n    \tDATE(FROM_UNIXTIME(unixtime)) AS activity_date,\n        subscriptionid AS subscription_id,\n        advertiserid AS advertiser_id,\n        IF(source = 'MATCHES', 'Match', 'Search') AS product,\n        IF(subscriptiontier = 'PROFESSIONAL' \n        \tAND (last_purchase_date < DATE('2024-08-02') OR last_purchase_date IS NULL)\n            AND source != 'MATCHES', 'Legacy Search', 'Smart Sourcing') AS subscription_tier,\n        SUM(sent) AS contacts_sent,\n        SUM(resp_any) AS all_responses,\n        SUM(resp_pos) AS positive_responses\n    FROM \n    \tdatalake.imhotep.rsContacts rsContacts\n    LEFT JOIN\n        subscription_attributes \n        \tON rsContacts.subscriptionid = subscription_attributes.subscription_id\n    WHERE \n    \tunixtime BETWEEN IMHOTEP_UNIXTIME('2024-08-02') AND IMHOTEP_UNIXTIME('today') \n\tGROUP BY 1, 2, 3, 4, 5\n\n)\n\nSELECT\n\tsubscription_tier,\n    COUNT(DISTINCT advertiser_id) AS num_advertisers,\n    COUNT(DISTINCT subscription_id) AS num_subscriptions,\n    SUM(contacts_sent) AS contacts_sent,\n    SUM(all_responses) AS all_responses,\n    (SUM(all_responses) * 1.0000) / SUM(contacts_sent) AS all_response_rate,\n    SUM(positive_responses) AS positive_responses,\n    (SUM(positive_responses) * 1.0000) / SUM(contacts_sent) AS positive_response_rate\nFROM \n\trsContacts\nWHERE advertiser_id = 16775855\n\nGROUP BY 1\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_dim_resume_subscription_current", "datalakehive.imhotep.rscontacts" ],
  "queryIndex" : 163,
  "runStartToQueryComplete" : 1072
}, {
  "elapsedMillis" : 3228,
  "totalScheduledMillis" : 50496,
  "cpuMillis" : 33703,
  "queuedMillis" : 0,
  "executeMillis" : 2311,
  "getResultMillis" : 0,
  "iterateMillis" : 1025,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 103004183,
  "query" : "WITH test_adids AS (\n    SELECT DISTINCT adid\n    FROM datalake.imhotep.adCampaignPerformanceTest51\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-26 00:00:00') \n                      AND IMHOTEP_UNIXTIME('2024-09-26 08:00:00')\n),\nverification_adids AS (\n    SELECT DISTINCT adid\n    FROM datalake.imhotep.adCampaignPerformanceVerification\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-26 00:00:00') \n                      AND IMHOTEP_UNIXTIME('2024-09-26 08:00:00')\n),\nadids_in_test_not_in_verification AS (\n    SELECT adid\n    FROM test_adids\n    EXCEPT\n    SELECT adid\n    FROM verification_adids\n),\nadids_in_verification_not_in_test AS (\n    SELECT adid\n    FROM verification_adids\n    EXCEPT\n    SELECT adid\n    FROM test_adids\n)\nSELECT\n    'adids_in_test_not_in_verification' AS difference_type,\n    ARRAY_AGG(adid) AS adid_list\nFROM (\n    SELECT adid\n    FROM adids_in_test_not_in_verification\n    LIMIT 10\n) t",
  "queryTables" : [ "datalakehive.imhotep.adcampaignperformancetest51", "datalakehive.imhotep.adcampaignperformanceverification" ],
  "queryIndex" : 164,
  "runStartToQueryComplete" : 1073
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 0,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001052_00090_mzvde): Access Denied: Cannot select from columns [billing_country, advertiser_name, type, parent_company_id, advertiser_id] in table or view core: User adevore is not authorized to query Table: core.client_attributes_dim_advertiser_attributes_current in Catalog: datalakehive. Rejected columns: {datalake.core.client_attributes_dim_advertiser_attributes_current=[billing_country, advertiser_name, type, parent_company_id, advertiser_id]} ",
  "scannedBytes" : 0,
  "query" : "  WITH raw_qual_response_base AS (\n      SELECT\n        *,\n        ROW_NUMBER() OVER (PARTITION BY id ORDER BY s3_load_timestamp DESC) AS priority_recent\n      FROM \"datalake\".\"qualtrics_survey_data_raw\".\"qual_responses_base\"\n  ),\n  raw_sbs_survey as (\n    SELECT \n    CAST(sbs.response_id AS VARCHAR) as response_id,\n    CAST(campaign_name AS VARCHAR) as campaign_name, \n    CAST(service_rep_first_name AS VARCHAR) as rep_first_name,\n    CAST(service_rep_last_name AS VARCHAR) as rep_last_name,\n    CAST(email_sent_at AS VARCHAR) as sent_date,\n    CAST(response_at AS DATE) as response_date,\n    CAST(JSON_EXTRACT_SCALAR(values_json, '$.x_variable') AS VARCHAR) as x_variable,\n    CAST(interaction_type AS VARCHAR) as interaction_type,\n    case_id,\n    '' as case_number,\n    CAST(rep_csat_score AS VARCHAR) as csat_score, \n    CAST(rep_csat_feedback AS VARCHAR) as csat_feedback,\n    CAST(indeed_score AS VARCHAR) as indeed_score,\n    CAST(indeed_feedback AS VARCHAR) as indeed_feedback,\n    CAST(advertiser_id AS VARCHAR) as advertiser_id,\n    CAST(cs_rep_id AS VARCHAR) as cs_rep_id\n    FROM datalake.analysis.surveys_qualtrics_sbs_shared_csat_survey sbs\n      JOIN raw_qual_response_base qrbase \n    ON sbs.response_id = qrbase.response_id AND priority_recent = 1\n  ),\n  raw_vendor_survey as (\n    SELECT\n    CAST(qualtrics_response_id AS VARCHAR) as response_id,\n    CAST(campaign_name AS VARCHAR) as campaign_name,\n    CAST(emp_first_name AS VARCHAR) as rep_first_name,\n    '' as rep_last_name,\n    CAST(sent_at AS VARCHAR) as sent_date,\n    CAST(response_at AS DATE) as response_date,\n    CAST(x_variable AS VARCHAR) as x_variable,\n    CAST(interaction_type AS VARCHAR) as interaction_type,\n    salesforce_case_id as case_id,\n    CAST(salesforce_case_number AS VARCHAR) as case_number,\n    CAST(rep_csat AS VARCHAR) as csat_score,\n    CAST(rep_feedback AS VARCHAR) as csat_feedback,\n    CAST(indeed_csat AS VARCHAR) AS indeed_score,\n    CAST(indeed_feedback AS VARCHAR) as indeed_feedback,\n    CAST(advertiser_id AS VARCHAR) as advertiser_id,\n    CAST(COALESCE(raven_agent_id, CAST(zendesk_agent_id AS VARCHAR)) AS VARCHAR) as cs_rep_id\n      FROM datalake.analysis.surveys_qualtrics_vendor_csat qcsat\n      JOIN raw_qual_response_base qrbase \n      ON qcsat.qualtrics_response_id = qrbase.response_id AND priority_recent = 1\n  ),\n  raw_ded_survey as (\n    SELECT \n    CAST(ded.response_id AS VARCHAR) as response_id, \n    CAST(campaign_name AS VARCHAR) as campaign_name, \n    CAST(service_rep_first_name AS VARCHAR) as rep_first_name, \n    CAST(service_rep_last_name AS VARCHAR) as rep_last_name, \n    CAST(email_sent_at AS VARCHAR) as sent_date,\n    CAST(response_at AS DATE) as response_date,\n    CAST(JSON_EXTRACT_SCALAR(values_json, '$.x_variable') AS VARCHAR) as x_variable, \n    CAST(interaction_type AS VARCHAR) as interaction_type, \n    case_id, \n    CAST(case_number AS VARCHAR) as case_number,\n    CAST(rep_csat_score AS VARCHAR) as csat_score, \n    CAST(rep_csat_feedback AS VARCHAR) as csat_feedback, \n    CAST(indeed_score AS VARCHAR) AS indeed_score,\n    CAST(indeed_feedback AS VARCHAR) as indeed_feedback,\n    CAST(advertiser_id AS VARCHAR) as advertiser_id, \n    CAST(cs_rep_id AS VARCHAR) as cs_rep_id\n    FROM datalake.analysis.surveys_dedicated_agency_csat_survey ded\n    JOIN raw_qual_response_base qrbase \n    ON ded.response_id = qrbase.response_id AND priority_recent = 1\n  ),\n  joined_surveys as (\n    SELECT * FROM raw_sbs_survey\n    UNION ALL \n    SELECT * FROM raw_vendor_survey\n    UNION ALL \n    SELECT * FROM raw_ded_survey\n  ),\n  salesforce_case_data as (\n    SELECT DISTINCT\n      casenumber,\n      status,\n        closeddate,\n        type_cs__c,\n        sub_type_cs__c,\n        product__c,\n        owner_name__c,\n        campaign_type__c,\n        subject\n    FROM datalake.salesforce.\"case\" \n ),\n  salesforce_user_data as (\n\tSELECT DISTINCT \n    \tadc_user_id__c, \n        name, \n        username, \n        email, \n        alias, \n        department, \n        lastname\n\tFROM datalake.salesforce.\"user\"\n ),\n advertiser_data as (\n    select distinct \n      a.advertiser_id , \n        a.advertiser_name , \n        a.type as advertiser_type , \n        a.billing_country , \n        p.parent_company_id , \n        p.parent_company_name , \n        p.company_size_segment as parent_company_size_segment \n    from datalake.core.client_attributes_dim_advertiser_attributes_current a \n    left join datalake.core.client_attributes_dim_parent_attributes_current p \n    on a.parent_company_id = p.parent_company_id\n  ),\n  surveys_advertiser as (\n    SELECT DISTINCT\n        (SELECT survey_id FROM \"datalake\".\"qualtrics_survey_data_raw\".\"qual_responses_base\" \n            WHERE response_id = js.response_id ORDER BY s3_load_timestamp DESC LIMIT 1) AS survey_id, \n        js.*,\n        ad.advertiser_name,\n        ad.advertiser_type,\n        ad.billing_country,\n        ad.parent_company_id,\n        ad.parent_company_name,\n        ad.parent_company_size_segment\n      FROM joined_surveys js\n      LEFT JOIN advertiser_data ad\n      ON js.advertiser_id = CAST(ad.advertiser_id AS VARCHAR(256))\n  ),\n  surveys_salesforce as (\n  \tSELECT \n    \tsa.*, \n        sc.status,\n        sc.closeddate,\n        sc.type_cs__c,\n        sc.sub_type_cs__c,\n        sc.product__c,\n        sc.owner_name__c,\n        sc.campaign_type__c,\n        sc.subject,\n        su.name, \n        su.username, \n        su.email, \n        su.alias, \n        su.department, \n        su.lastname\n    FROM surveys_advertiser sa\n    LEFT JOIN salesforce_case_data sc\n    ON sa.case_number = CAST(sc.casenumber AS VARCHAR(256))\n    LEFT JOIN salesforce_user_data su\n    ON sa.cs_rep_id = CAST(su.adc_user_id__c AS VARCHAR(256))\n  ),\n web_surveys as (\n\tSELECT DISTINCT\n  \t\t'Web' as channel,\n    \tNULL as queue,\n        sd.survey_name as survey_name,\n    \tssa.* \n\tFROM surveys_salesforce ssa\n    LEFT JOIN datalake.qualtrics_survey_data_raw.qual_survey_base sd\n    ON CAST(sd.survey_id AS varchar) = CAST(ssa.survey_id as varchar)\n )\nSELECT count(*) FROM web_surveys\n ",
  "queryTables" : [ "datalakehive.analysis.surveys_dedicated_agency_csat_survey", "datalakehive.analysis.surveys_qualtrics_sbs_shared_csat_survey", "datalakehive.analysis.surveys_qualtrics_vendor_csat", "datalakehive.core.client_attributes_dim_advertiser_attributes_current", "datalakehive.core.client_attributes_dim_parent_attributes_current", "datalakehive.qualtrics_survey_data_raw.qual_responses_base", "datalakehive.qualtrics_survey_data_raw.qual_responses_base", "datalakehive.qualtrics_survey_data_raw.qual_responses_base", "datalakehive.qualtrics_survey_data_raw.qual_responses_base", "datalakehive.qualtrics_survey_data_raw.qual_survey_base", "datalakehive.salesforce.case", "datalakehive.salesforce.user" ],
  "queryIndex" : 165,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 44965,
  "totalScheduledMillis" : 96174254,
  "cpuMillis" : 20898107,
  "queuedMillis" : 1,
  "executeMillis" : 5506,
  "getResultMillis" : 0,
  "iterateMillis" : 39521,
  "rows" : 9,
  "error" : null,
  "scannedBytes" : 423063693660,
  "query" : "with cjs as (\nSELECT \n    dcr.job_hash,\n    sum(cast(dcr.numhiresMade as int)) as numhiresMade,\n    sum(cast(dcr.numHiresMadeIndeed as int)) as numHiresMadeIndeed,\n    sum(cast(dcr.numHiresMadeExternal as int)) as numHiresMadeExternal\nFROM datalake.imhotep.dradisClosedJobReason as dcr\nWHERE dcr.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-04-01') AND IMHOTEP_UNIXTIME('2024-10-24')\n\tand dcr.numhiresMade> '0'\ngroup by 1\n)\n,job_id_job_hash_mapping as (\nSELECT \n\tjobhash_underscore, max(agg_job_id) as agg_job_id\nFROM datalake.imhotep.job_id_job_hash_mapping\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-23') AND IMHOTEP_UNIXTIME('2024-10-24')\n\tand jobhash_underscore in (select distinct job_hash from cjs)\ngroup by 1 \n)\n,jam as (\nselect \n\tagg_job_id,\n    max(job_product) as job_product,\n    max(job_country_code) as job_country_code,\n    sum(clicks) as clicks,\n    sum(apply_starts) as apply_starts,\n    sum(applies) as applies\nfrom datalake.imhotep.jobactivitymetrics\nwhere day between '2022-04-01' and 'today'\t\n\tand agg_job_id in (select distinct agg_job_id from job_id_job_hash_mapping)\ngroup by 1 \n)\n\n,view as (\nselect \n\tjob_id_job_hash_mapping.agg_job_id,\n    coalesce(applies,0) as applies,\n    coalesce(apply_starts,0) as apply_starts,\n    coalesce(clicks,0) as clicks,\n    numhiresMade,\n    numHiresMadeIndeed,\n    numHiresMadeExternal,\n    job_product,\n    job_country_code\nfrom cjs \njoin job_id_job_hash_mapping on job_id_job_hash_mapping.jobhash_underscore = cjs.job_hash\nleft join jam\n\ton job_id_job_hash_mapping.agg_job_id = jam.agg_job_id\n)\n\nselect \n   case when job_country_code in ('US', 'JP') then job_country_code else 'RoW' end as job_country,\n   case when numHiresMadeExternal>0 and numHiresMadeExternal = numhiresMade then 'Only External'\n        when numHiresMadeExternal>0 and numHiresMadeIndeed>0 then 'External + Internal'\n        else 'Only Internal' end as cohort, \n   sum(numhiresMade) as hires,\n   1.00*count(distinct case when applies>0 then agg_job_id end)/count(distinct agg_job_id) as pct_job_w_apply,\n   1.00*count(distinct case when applies<10 then agg_job_id end)/count(distinct agg_job_id) as pct_job_w_lessthan10applies,\n   avg(applies) as avg_applies,\n   avg(apply_starts) as avg_applystarts,\n   avg(clicks) as avg_clicks\nfrom view \ngroup by 1,2",
  "queryTables" : [ "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.job_id_job_hash_mapping", "datalakehive.imhotep.job_id_job_hash_mapping", "datalakehive.imhotep.jobactivitymetrics" ],
  "queryIndex" : 166,
  "runStartToQueryComplete" : 1141
}, {
  "elapsedMillis" : 3836,
  "totalScheduledMillis" : 13745,
  "cpuMillis" : 2053,
  "queuedMillis" : 1,
  "executeMillis" : 2842,
  "getResultMillis" : 0,
  "iterateMillis" : 1019,
  "rows" : 12804,
  "error" : null,
  "scannedBytes" : 1708682,
  "query" : "WITH lib_base AS (\n\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      lib.teamId,\n      'lib' AS resource_type,\n      'n/a' AS env,\n      languageRuntime AS languageRuntime,\n      libraryName AS resource_name,\n      CAST(highestVersionPublishAgeDays AS BIGINT) AS age\n  FROM libraryFreshnessSnapshot lib\n  LEFT JOIN teamwrksSnapshot tm\n  ON lib.teamid = tm.teamid\n  WHERE lib.unixtime >= IMHOTEP_UNIXTIME('2d') AND lib.unixtime < IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime >= IMHOTEP_UNIXTIME('2d') AND tm.unixtime < IMHOTEP_UNIXTIME('1d')\n),\n\napp_base AS (\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      app.teamId,\n      'app' AS resource_type,\n      env,\n      applicationRuntime AS languageRuntime,\n      applicationName AS resource_name,\n      lockdownAge AS age\n  FROM applicationFreshnessSnapshot AS app\n  LEFT JOIN teamwrksSnapshot tm\n  ON app.teamid = tm.teamid\n  WHERE app.unixtime >= IMHOTEP_UNIXTIME('2d') AND app.unixtime < IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime >= IMHOTEP_UNIXTIME('2d') AND tm.unixtime < IMHOTEP_UNIXTIME('1d')\n)\n\n\nSELECT *\nFROM (\n  SELECT *\n  FROM app_base\n  UNION ALL\n  SELECT *\n  FROM lib_base\n)\nWHERE env != 'MISSING'",
  "queryTables" : [ "skipperhive.imhotep.applicationfreshnesssnapshot", "skipperhive.imhotep.libraryfreshnesssnapshot", "skipperhive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 167,
  "runStartToQueryComplete" : 1113
}, {
  "elapsedMillis" : 3138,
  "totalScheduledMillis" : 10728,
  "cpuMillis" : 1806,
  "queuedMillis" : 0,
  "executeMillis" : 2385,
  "getResultMillis" : 0,
  "iterateMillis" : 775,
  "rows" : 12804,
  "error" : null,
  "scannedBytes" : 1708682,
  "query" : "WITH lib_base AS (\n\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      lib.teamId,\n      'lib' AS resource_type,\n      'n/a' AS env,\n      languageRuntime AS languageRuntime,\n      libraryName AS resource_name,\n      CAST(highestVersionPublishAgeDays AS BIGINT) AS age\n  FROM libraryFreshnessSnapshot lib\n  LEFT JOIN teamwrksSnapshot tm\n  ON lib.teamid = tm.teamid\n  WHERE lib.unixtime >= IMHOTEP_UNIXTIME('2d') AND lib.unixtime < IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime >= IMHOTEP_UNIXTIME('2d') AND tm.unixtime < IMHOTEP_UNIXTIME('1d')\n),\n\napp_base AS (\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      app.teamId,\n      'app' AS resource_type,\n      env,\n      applicationRuntime AS languageRuntime,\n      applicationName AS resource_name,\n      lockdownAge AS age\n  FROM applicationFreshnessSnapshot AS app\n  LEFT JOIN teamwrksSnapshot tm\n  ON app.teamid = tm.teamid\n  WHERE app.unixtime >= IMHOTEP_UNIXTIME('2d') AND app.unixtime < IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime >= IMHOTEP_UNIXTIME('2d') AND tm.unixtime < IMHOTEP_UNIXTIME('1d')\n)\n\n\nSELECT *\nFROM (\n  SELECT *\n  FROM app_base\n  UNION ALL\n  SELECT *\n  FROM lib_base\n)\nWHERE env != 'MISSING'",
  "queryTables" : [ "skipperhive.imhotep.applicationfreshnesssnapshot", "skipperhive.imhotep.libraryfreshnesssnapshot", "skipperhive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 168,
  "runStartToQueryComplete" : 1115
}, {
  "elapsedMillis" : 7963,
  "totalScheduledMillis" : 96596,
  "cpuMillis" : 21646,
  "queuedMillis" : 0,
  "executeMillis" : 6324,
  "getResultMillis" : 0,
  "iterateMillis" : 1693,
  "rows" : 59,
  "error" : null,
  "scannedBytes" : 101480338,
  "query" : "WITH lib_base AS (\n\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      lib.teamId,\n      'lib' AS resource_type,\n      'n/a' AS env,\n      languageRuntime AS languageRuntime,\n      libraryName AS resource_name,\n      CAST(highestVersionPublishAgeDays AS BIGINT) AS age,\n      lib.day\n  FROM libraryFreshnessSnapshot lib\n  LEFT JOIN teamwrksSnapshot tm\n  ON lib.teamid = tm.teamid\n  AND lib.day = DATE_FORMAT(CAST(tm.day AS TIMESTAMP), '%Y-%m-%d')\n  WHERE lib.unixtime BETWEEN IMHOTEP_UNIXTIME('60d') AND IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime BETWEEN IMHOTEP_UNIXTIME('60d') AND IMHOTEP_UNIXTIME('1d')\n),\n\napp_base AS (\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      app.teamId,\n      'app' AS resource_type,\n      env,\n      applicationRuntime AS languageRuntime,\n      applicationName AS resource_name,\n      lockdownAge AS age,\n      app.day\n  FROM applicationFreshnessSnapshot AS app\n  LEFT JOIN teamwrksSnapshot tm\n  ON app.teamid = tm.teamid\n  AND app.day = DATE_FORMAT(CAST(tm.day AS TIMESTAMP), '%Y-%m-%d')\n  WHERE app.unixtime BETWEEN IMHOTEP_UNIXTIME('60d') AND IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime BETWEEN IMHOTEP_UNIXTIME('60d') AND IMHOTEP_UNIXTIME('1d')\n)\n\n\nSELECT\n\tday,\n    100.0 * COUNT_IF(age < 90)/COUNT(*) AS freshness_percentage\nFROM (\n  SELECT *\n  FROM app_base\n  UNION ALL\n  SELECT *\n  FROM lib_base\n)\nWHERE ('' = '' OR teamId like '')\n  AND ('Indeed' = '' OR tier_0 like 'Indeed')\n  AND ('' = '' OR tier_1 like '')\n  AND ('' = '' OR tier_2 like '')\n  AND ('' = '' OR tier_3 like '')\n  AND ('' = '' OR tier_4 like '')\n  AND ('' = '' OR tier_5 like '')\n  AND ('' = '' OR tier_6 like '')\n  AND ('' = '' OR tier_7 like '')\n  AND ('' = '' OR tier_8 like '')\nGROUP BY day",
  "queryTables" : [ "skipperhive.imhotep.applicationfreshnesssnapshot", "skipperhive.imhotep.libraryfreshnesssnapshot", "skipperhive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 169,
  "runStartToQueryComplete" : 1119
}, {
  "elapsedMillis" : 2326,
  "totalScheduledMillis" : 968343,
  "cpuMillis" : 91751,
  "queuedMillis" : 0,
  "executeMillis" : 1505,
  "getResultMillis" : 0,
  "iterateMillis" : 993,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 2933427535,
  "query" : "with msa_jobs as (\n    select sj.jobid as agg_job_id\n    , jlmsa\n    , trim(jlmsa) as jlmsa_trimmed\n--    , sum(case when jlmsa!='' then 1 else 0 end) AS urban_jobs\n--    , sum(case when jlmsa='' then 1 else 0 end) AS rural_jobs\n    , max_by(jlmsa, hour) as latest_msa\n    from datalake.imhotep.searchablejobs sj\n    where hour between date_format(from_unixtime(to_unixtime(date('2024-10-01'))), '%Y-%m-%d %H:%i:%s') and date_format(from_unixtime(to_unixtime(date('2024-10-02'))), '%Y-%m-%d %H:%i:%s')\n    group by 1,2,3\n)\n\nselect * from msa_jobs\nwhere jlmsa=''\nlimit 100\n",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 170,
  "runStartToQueryComplete" : 1116
}, {
  "elapsedMillis" : 2811,
  "totalScheduledMillis" : 8853,
  "cpuMillis" : 1774,
  "queuedMillis" : 0,
  "executeMillis" : 2456,
  "getResultMillis" : 0,
  "iterateMillis" : 384,
  "rows" : 1584,
  "error" : null,
  "scannedBytes" : 940980,
  "query" : "WITH golden_amis AS (\n    SELECT name, id, created_on, day\n    FROM datalake.imhotep.aws_ami\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND REGEXP_LIKE(name, '^indeed_(ubuntu|(dsp_ecs_)?centos_7|amzn2|AL2023).*')\n)\n, ec2 AS (\n\tSELECT name, image_id, id, account_id, launched_on\n    FROM datalake.imhotep.aws_ec2\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND state='running'\n    AND (emr_role='' OR emr_role IS NULL)\n)\nSELECT DISTINCT(e.id) AS ec2_id, e.name AS ec2_name, e.image_id, e.account_id, e.launched_on, g.name as ami_name\n\t, day(current_timestamp - from_iso8601_timestamp(e.launched_on)) as ec2_age_days\n    , day(current_timestamp - from_iso8601_timestamp(g.created_on)) as ami_age_days\n    /*\n    Cases:\n    \tlaunched_on > 90 days ago = not patched\n        launched_on < 90 days ago\n        \tcreated_on > 120 days ago = not patched -- Giving 30 day buffer given the golden AMI rebuild schedule. \n            created_on <= 120 days ago = patched \n    This does not flag instances that are not picking up the newest image on rebuild but that image is still less than 120 days old. Technically still patched, but on track to not be patched. \n    */\n    , CASE WHEN \n        day(current_timestamp - from_iso8601_timestamp(e.launched_on)) <= 90 -- Must update at least every 90 days\n        \tAND day(current_timestamp - from_iso8601_timestamp(g.created_on)) <= 120 -- If the image it built from is over 120 days old, it's not following the golden image releases\n        THEN 1\n        ELSE 0\n      END AS patched\n    , 1 AS count\nfrom ec2 e\nINNER JOIN golden_amis g\nON e.image_id=g.id\n\n",
  "queryTables" : [ "datalakehive.imhotep.aws_ami", "datalakehive.imhotep.aws_ec2" ],
  "queryIndex" : 171,
  "runStartToQueryComplete" : 1131
}, {
  "elapsedMillis" : 7855,
  "totalScheduledMillis" : 75416,
  "cpuMillis" : 37195,
  "queuedMillis" : 1,
  "executeMillis" : 6497,
  "getResultMillis" : 0,
  "iterateMillis" : 1386,
  "rows" : 3876,
  "error" : null,
  "scannedBytes" : 145514085,
  "query" : "WITH ec2_instances AS (\n  SELECT name, id, image_id, account_id, launched_on\n  FROM datalake.imhotep.aws_ec2\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND state='running' \n  AND (emr_role is NULL or emr_role = '') -- Don't pull in EMR. Not Centos, centrally managed mostly, and has lots of churn. \n)\n, golden_name_amis AS (\n  SELECT id, name, display_name\n  FROM datalake.imhotep.aws_ami\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND LOWER(display_name) LIKE '%golden%'\n)\n, amis AS (\n  SELECT id, name, display_name\n  FROM datalake.imhotep.aws_ami\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n)\n, aws_accounts AS (\n  SELECT \"key\" AS account_id, accountablePartyId AS teamId, accountablePartyType AS teamType\n  FROM datalake.imhotep.ownershipsnapshot\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n  AND namespace='AWS_ACCOUNT' AND statusflagvalue='ACTIVE'\n)\n, all_hosts AS (\n\tSELECT resource_id\n    FROM aws_inspector_findings\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n)\n, ownership AS (\n  SELECT teamid, displayname, intakejira, tier_0, tier_1, tier_2, tier_3, tier_4, tier_5, tier_6, email, engineeringleadldap, productleadldap, programleadldap, technologyleadldap, usersslackchannel\n  FROM datalake.imhotep.teamwrkssnapshot \n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n)\n\nSELECT DISTINCT(e.id), e.name AS ec2_name, e.account_id, o.teamid, o.tier_0, o.tier_1, o.tier_2, o.tier_3, o.tier_4, o.tier_5, o.tier_6, ch.resource_id, amis.name AS ami_name, amis.display_name AS ami_display_name\n\t, CASE WHEN ch.resource_id IS NOT NULL AND LOWER(amis.display_name) LIKE '%golden%' THEN 1 ELSE 0 END AS is_golden\n    , 1 as count\nFROM ec2_instances e\nLEFT OUTER JOIN aws_accounts a ON e.account_id= a.account_id\nLEFT OUTER JOIN all_hosts ch ON ch.resource_id = e.id\nLEFT OUTER JOIN amis ON amis.id = e.image_id\nLEFT OUTER JOIN ownership o ON a.teamId=o.teamid\nWHERE \n\t(''='' OR o.tier_0 = '')\n    AND (''='' OR o.tier_1 = '')\n    AND (''='' OR o.tier_2 = '')\n    AND (''='' OR o.tier_3 = '')\n    AND (''='' OR o.tier_4 = '')\n    AND o.tier_6 != 'Cluster'",
  "queryTables" : [ "datalakehive.imhotep.aws_ami", "datalakehive.imhotep.aws_ec2", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.aws_inspector_findings" ],
  "queryIndex" : 172,
  "runStartToQueryComplete" : 1142
}, {
  "elapsedMillis" : 1776,
  "totalScheduledMillis" : 1362163,
  "cpuMillis" : 267831,
  "queuedMillis" : 0,
  "executeMillis" : 1744,
  "getResultMillis" : 0,
  "iterateMillis" : 67,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 2894333333,
  "query" : "with msa_jobs as (\n    select sj.jobid as agg_job_id\n    , jlmsa\n    , trim(jlmsa) as jlmsa_trimmed\n--    , sum(case when jlmsa!='' then 1 else 0 end) AS urban_jobs\n--    , sum(case when jlmsa='' then 1 else 0 end) AS rural_jobs\n    , max_by(jlmsa, hour) as latest_msa\n    from datalake.imhotep.searchablejobs sj\n    where hour between date_format(from_unixtime(to_unixtime(date('2024-10-01'))), '%Y-%m-%d %H:%i:%s') and date_format(from_unixtime(to_unixtime(date('2024-10-02'))), '%Y-%m-%d %H:%i:%s')\n    group by 1,2,3\n)\n\nselect * from msa_jobs\nwhere jlmsa is null\nlimit 100\n",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 173,
  "runStartToQueryComplete" : 1140
}, {
  "elapsedMillis" : 240987,
  "totalScheduledMillis" : 411750579,
  "cpuMillis" : 15617138,
  "queuedMillis" : 0,
  "executeMillis" : 131222,
  "getResultMillis" : 0,
  "iterateMillis" : 109789,
  "rows" : 186072,
  "error" : null,
  "scannedBytes" : 1186364850896,
  "query" : "with jobs as (SELECT coalesce(parent_company_id, job_source_id+1000000) as parent_company_id \n, coalesce(max(parent_company_name), max(job_source_name)) as parent_company_name   \n, min(job_source_id) as job_source_id\n, count(distinct agg_job_id) as jobs_l30days \n, sum(organic_apply_starts) as orgAS_l30days \n\nFROM datalake.imhotep.jobactivitymetrics j\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nand job_country_code = 'US' and is_job_searchable > 0 \n--and parent_company_id is not null \nand advertiser_type not in ('Test', 'Indeed')\ngroup by 1)\n\n, rev as (select parent_company_id \n, sum(net_revenue_cents)/100 as spend_lyear \nFROM datalake.imhotep.grdm j \nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1y') AND IMHOTEP_UNIXTIME('today') \ngroup by 1) \n\n, together as (select jobs.* \n, coalesce(spend_lyear, 0) as spend_lyear \n, coalesce(last_any_products, cast('2000-01-01' as date)) as last_revenue_date  -- if no last revenue date, then say they last spend in 2000 \n, date_diff('day', coalesce(last_any_products, cast('2000-01-01' as date)), CURRENT_DATE) as days_since_last_spend \nfrom jobs \nleft join rev on rev.parent_company_id = jobs.parent_company_id \nleft join datalake.core.product_analytics_fct_first_last_activity_dates_parent f \n\ton f.parent_company_id = jobs.parent_company_id\n--where jobs_l30days > 100    \n)\n, together_ordered as (select *, \nntile(10) over (order by jobs_l30days desc) as jobs_ntile -- 1 is most jobs \n, ntile(10) over (order by spend_lyear asc) as spend_ntile -- 1 is lowest spend \n, ntile(10) over (order by days_since_last_spend desc) as days_since_last_spend_ntile -- 1 is never spender\nfrom together )\n\n, together_ordered_fixed as (select parent_company_id  \n, job_source_id\n, parent_company_name \n, orgAS_l30days \n, jobs_l30days\n, days_since_last_spend\n, spend_lyear\n, jobs_ntile \n, case when spend_lyear <= 0 then 1 else spend_ntile end as spend_ntile \n, case when days_since_last_spend > 9000 then 1 else days_since_last_spend_ntile end as days_since_last_spend_ntile\n\nfrom together_ordered )\n\n, rfm as (select * \n, jobs_ntile + spend_ntile + days_since_last_spend_ntile as rfm_unweighted \n, 1*jobs_ntile + 100*spend_ntile + 10*days_since_last_spend_ntile as rfm_weighted -- the higher this number the bigger the penalty\nfrom together_ordered_fixed\norder by 1*jobs_ntile + 100*spend_ntile + 10*days_since_last_spend_ntile desc\n)\n\n, jobs_first_hour as (SELECT sourceid\n, jobid  \n, max(normtitle) as normtitle \n, coalesce(max(jlmsa), 'rural') as msa \n, sum(coalesce(applystarts,0)) as applystarts\n\nFROM datalake.imhotep.mobileorganic j\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nand country in ('us', 'US') and jobageminutes <= 60\ngroup by 1, 2)\n\n, jobs_first_hour_ordered as (SELECT * \n, ntile(10) over (order by applystarts asc) as job_ntile\nFROM jobs_first_hour)\n\n\n, subset as (select case when rfm_weighted > 1000 then 'A_1000+ Biggest Penalty'\nwhen rfm_weighted > 900 then 'B_900'\nwhen rfm_weighted > 800 then 'C_800'\nwhen rfm_weighted > 700 then 'D_700'\nwhen rfm_weighted > 600 then 'E_600'\nwhen rfm_weighted > 500 then 'F_500'\nwhen rfm_weighted > 400 then 'G_400'\nwhen rfm_weighted > 300 then 'H_300'\nwhen rfm_weighted > 200 then 'I_200' else 'J_100 Lowest Penalty' end as rfm_segment \n, rfm.* \n, j.* \n, row_number() over (partition by job_source_id order by applystarts desc) as rn \n\nfrom rfm  \nleft join jobs_first_hour_ordered j on j.sourceid = rfm.job_source_id \nwhere rfm_weighted > 1000 or rfm_weighted < 200\nand j.job_ntile = 10 )\n\nselect * from subset where rn <= 5\n\n\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_fct_first_last_activity_dates_parent", "datalakehive.imhotep.grdm", "datalakehive.imhotep.jobactivitymetrics", "datalakehive.imhotep.mobileorganic" ],
  "queryIndex" : 174,
  "runStartToQueryComplete" : 1383
}, {
  "elapsedMillis" : 24923,
  "totalScheduledMillis" : 1954716,
  "cpuMillis" : 1327484,
  "queuedMillis" : 0,
  "executeMillis" : 19890,
  "getResultMillis" : 0,
  "iterateMillis" : 5092,
  "rows" : 2495,
  "error" : null,
  "scannedBytes" : 8749976323,
  "query" : "with app_status as(SELECT ats,apply_id, disposition_status, max(unixtime) as unixtime\nFROM signal_back_disposition a\nWHERE \n  -- remove apply_id that existed 1y nefore april\n  NOT EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition b\n      WHERE unixtime between imhotep_unixtime('2023-04-01') and imhotep_unixtime('2024-04-01')\n        AND a.apply_id=b.apply_id\n  )\n  -- apply_id first appears in april\n  AND EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition c\n      WHERE unixtime between imhotep_unixtime('2024-04-01') and imhotep_unixtime('2024-05-01')\n        AND a.apply_id=c.apply_id\n  )\n  AND unixtime between imhotep_unixtime('2024-04-01') and imhotep_unixtime('2024-10-01')\n  \n  GROUP BY 1,2,3\n)\n,max_times as (\nSELECT ats\n, apply_id\n, disposition_status\n, unixtime\n, ROW_NUMBER() OVER(PARTITION BY apply_id ORDER BY unixtime ASC) as status_number\nFROM app_status)\n\nSELECT ats, status_number, disposition_status, count(distinct apply_id) as apps\nFROM max_times\nGROUP BY 1,2,3",
  "queryTables" : [ "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition" ],
  "queryIndex" : 175,
  "runStartToQueryComplete" : 1167
}, {
  "elapsedMillis" : 18965,
  "totalScheduledMillis" : 10596931,
  "cpuMillis" : 2475583,
  "queuedMillis" : 1,
  "executeMillis" : 17083,
  "getResultMillis" : 0,
  "iterateMillis" : 1914,
  "rows" : 500,
  "error" : null,
  "scannedBytes" : 27898796430,
  "query" : "with examples as (\nSELECT\n\t\tmoderationtext,\n        flagged,\n        flaggedcategories,\n        flaggedwords,\n \t\tappName,\n        moderationApi, \n        moderationType,\n        FROM_UNIXTIME(unixtime) as request_date,\n        requestid       \nFROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n    \tflagged = '1'\n        and lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and ('ContentGenerationService' = '' or appName='ContentGenerationService')\n        and ('openAIModerations' = '' or moderationApi='openAIModerations')\n        and ('' = '' or moderationType='')\nlimit 500\n)\n\nSELECT\n        m.*,\n        r.model,\n        r.moderationHeader,\n        r.locale        \nFROM datalake.imhotep.llmproxy r\n        join examples m on m.requestid = r.requestid -- switching to imhotep for speed even though there is a 1D data lag\n    WHERE\n        r.unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and r.metric = 'llm-proxy'",
  "queryTables" : [ "datalakehive.imhotep.llmproxy", "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 176,
  "runStartToQueryComplete" : 1164
}, {
  "elapsedMillis" : 3075,
  "totalScheduledMillis" : 5290027,
  "cpuMillis" : 300717,
  "queuedMillis" : 0,
  "executeMillis" : 2114,
  "getResultMillis" : 0,
  "iterateMillis" : 1018,
  "rows" : 37,
  "error" : null,
  "scannedBytes" : 16189187960,
  "query" : "SELECT\n \t\tappName,\n        moderationApi, \n        moderationType, \n        count(*) as request_count,\n        COUNT_IF(flagged='1') as flagged_count,\n        COUNT_IF(flagged='1')  * 100.0 / count(*) as block_rate_pct,\n        min(FROM_UNIXTIME(unixtime)) as start_date,\n\t\tmax(FROM_UNIXTIME(unixtime)) as end_date,\n        now() as query_current_date,\n        DATE_DIFF('hour', max(FROM_UNIXTIME(unixtime)), now()) as data_lag_hours\nFROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\ngroup by 1, 2, 3",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 177,
  "runStartToQueryComplete" : 1148
}, {
  "elapsedMillis" : 1942,
  "totalScheduledMillis" : 2590978,
  "cpuMillis" : 467443,
  "queuedMillis" : 1,
  "executeMillis" : 897,
  "getResultMillis" : 0,
  "iterateMillis" : 1067,
  "rows" : 35,
  "error" : null,
  "scannedBytes" : 15940040868,
  "query" : "SELECT\n \t\tappName,\n        moderationApi, \n        moderationType,\n        flaggedwords,\n        count(*) as request_count,\n        sum(count(*)) over (partition by appName, moderationApi, moderationType) as app_request_count,\n        count(*) * 100.0 * 100.0 / sum(count(*)) over (partition by appName, moderationApi, moderationType) as bps_of_app_requests\nFROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and ('ContentGenerationService' = '' or appName='ContentGenerationService')\n        and moderationApi = 'mcmurdoDenylist'\n        and ('' = '' or moderationType='')\ngroup by 1, 2, 3,4\norder by 5 DESC\nlimit 500",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 178,
  "runStartToQueryComplete" : 1152
}, {
  "elapsedMillis" : 15339,
  "totalScheduledMillis" : 3125564,
  "cpuMillis" : 1046885,
  "queuedMillis" : 1,
  "executeMillis" : 11449,
  "getResultMillis" : 0,
  "iterateMillis" : 3901,
  "rows" : 5000,
  "error" : null,
  "scannedBytes" : 34812767560,
  "query" : "with jobs as (SELECT coalesce(parent_company_id, job_source_id+1000000) as parent_company_id\n, coalesce(max(parent_company_name), max(job_source_name)) as parent_company_name  \n, count(distinct agg_job_id) as jobs_l30days \n, sum(organic_apply_starts) as orgAS_l30days \n\nFROM datalake.imhotep.jobactivitymetrics j\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nand job_country_code = 'US' and is_job_searchable > 0 \n--and parent_company_id is not null \nand advertiser_type not in ('Test', 'Indeed')\ngroup by 1)\n\n, rev as (select parent_company_id \n, sum(net_revenue_cents)/100 as spend_lyear \nFROM datalake.imhotep.grdm j \nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1y') AND IMHOTEP_UNIXTIME('today') \ngroup by 1) \n\n, together as (select jobs.* \n, coalesce(spend_lyear, 0) as spend_lyear \n, coalesce(last_any_products, cast('2000-01-01' as date)) as last_revenue_date  -- if no last revenue date, then say they last spend in 2000 \n, date_diff('day', coalesce(last_any_products, cast('2000-01-01' as date)), CURRENT_DATE) as days_since_last_spend \nfrom jobs \nleft join rev on rev.parent_company_id = jobs.parent_company_id \nleft join datalake.core.product_analytics_fct_first_last_activity_dates_parent f \n\ton f.parent_company_id = jobs.parent_company_id\n--where jobs_l30days > 100    \n)\n\n, together_ordered as (select *, \nntile(10) over (order by jobs_l30days desc) as jobs_ntile -- 1 is most jobs \n, ntile(10) over (order by spend_lyear asc) as spend_ntile -- 1 is lowest spend \n, ntile(10) over (order by days_since_last_spend desc) as days_since_last_spend_ntile -- 1 is never spender\nfrom together )\n\n, together_ordered_fixed as (select parent_company_id \n, parent_company_name \n, orgAS_l30days \n, jobs_l30days\n, days_since_last_spend\n, spend_lyear\n, jobs_ntile \n, case when spend_lyear <= 0 then 1 else spend_ntile end as spend_ntile \n, case when days_since_last_spend > 9000 then 1 else days_since_last_spend_ntile end as days_since_last_spend_ntile\n\nfrom together_ordered )\n\n, rfm as (select * \n, jobs_ntile + spend_ntile + days_since_last_spend_ntile as rfm_unweighted \n, 1*jobs_ntile + 100*spend_ntile + 10*days_since_last_spend_ntile as rfm_weighted -- the higher this number the bigger the penalty\nfrom together_ordered_fixed\norder by 1*jobs_ntile + 100*spend_ntile + 10*days_since_last_spend_ntile desc\n)\n\nselect *\n\nfrom rfm \n--WHERE REGEXP_LIKE(lower(parent_company_name), 'google|walmart|amazon|bank of america|cvs|walgreens|facebook|tesla')\nORDER BY orgAS_l30days desc \nLIMIT 5000\n\n\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_fct_first_last_activity_dates_parent", "datalakehive.imhotep.grdm", "datalakehive.imhotep.jobactivitymetrics" ],
  "queryIndex" : 179,
  "runStartToQueryComplete" : 1169
}, {
  "elapsedMillis" : 8898,
  "totalScheduledMillis" : 2255880,
  "cpuMillis" : 930090,
  "queuedMillis" : 0,
  "executeMillis" : 4911,
  "getResultMillis" : 0,
  "iterateMillis" : 4010,
  "rows" : 80347,
  "error" : null,
  "scannedBytes" : 34813741272,
  "query" : "with jobs as (SELECT coalesce(parent_company_id, job_source_id+1000000) as parent_company_id \n, coalesce(max(parent_company_name), max(job_source_name)) as parent_company_name   \n, min(job_source_id) as job_source_id\n, count(distinct agg_job_id) as jobs_l30days \n, sum(organic_apply_starts) as orgAS_l30days \n\nFROM datalake.imhotep.jobactivitymetrics j\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nand job_country_code = 'US' and is_job_searchable > 0 \n--and parent_company_id is not null \nand advertiser_type not in ('Test', 'Indeed')\ngroup by 1)\n\n, rev as (select parent_company_id \n, sum(net_revenue_cents)/100 as spend_lyear \nFROM datalake.imhotep.grdm j \nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1y') AND IMHOTEP_UNIXTIME('today') \ngroup by 1) \n\n, together as (select jobs.* \n, coalesce(spend_lyear, 0) as spend_lyear \n, coalesce(last_any_products, cast('2000-01-01' as date)) as last_revenue_date  -- if no last revenue date, then say they last spend in 2000 \n, date_diff('day', coalesce(last_any_products, cast('2000-01-01' as date)), CURRENT_DATE) as days_since_last_spend \nfrom jobs \nleft join rev on rev.parent_company_id = jobs.parent_company_id \nleft join datalake.core.product_analytics_fct_first_last_activity_dates_parent f \n\ton f.parent_company_id = jobs.parent_company_id\n--where jobs_l30days > 100    \n)\n, together_ordered as (select *, \nntile(10) over (order by jobs_l30days desc) as jobs_ntile -- 1 is most jobs \n, ntile(10) over (order by spend_lyear asc) as spend_ntile -- 1 is lowest spend \n, ntile(10) over (order by days_since_last_spend desc) as days_since_last_spend_ntile -- 1 is never spender\nfrom together )\n\n, together_ordered_fixed as (select parent_company_id  \n, job_source_id\n, parent_company_name \n, orgAS_l30days \n, jobs_l30days\n, days_since_last_spend\n, spend_lyear\n, jobs_ntile \n, case when spend_lyear <= 0 then 1 else spend_ntile end as spend_ntile \n, case when days_since_last_spend > 9000 then 1 else days_since_last_spend_ntile end as days_since_last_spend_ntile\n\nfrom together_ordered )\n\n, rfm as (select * \n, jobs_ntile + spend_ntile + days_since_last_spend_ntile as rfm_unweighted \n, 1*jobs_ntile + 100*spend_ntile + 10*days_since_last_spend_ntile as rfm_weighted -- the higher this number the bigger the penalty\nfrom together_ordered_fixed\norder by 1*jobs_ntile + 100*spend_ntile + 10*days_since_last_spend_ntile desc\n)\n\n, jobs_first_hour as (SELECT sourceid\n, jobid  \n, max(normtitle) as normtitle \n, coalesce(max(jlmsa), 'rural') as msa \n, sum(coalesce(applystarts,0)) as applystarts\n\nFROM datalake.imhotep.searchimpressions j\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nand country in ('us', 'US') and sponsored = 0 and jobageminutes <= 60\ngroup by 1, 2)\n\n, jobs_first_hour_ordered as (SELECT * \n, ntile(10) over (order by applystarts asc) as job_ntile\nFROM jobs_first_hour)\n\n\n, subset as (select case when rfm_weighted > 1000 then 'A_1000+ Biggest Penalty'\nwhen rfm_weighted > 900 then 'B_900'\nwhen rfm_weighted > 800 then 'C_800'\nwhen rfm_weighted > 700 then 'D_700'\nwhen rfm_weighted > 600 then 'E_600'\nwhen rfm_weighted > 500 then 'F_500'\nwhen rfm_weighted > 400 then 'G_400'\nwhen rfm_weighted > 300 then 'H_300'\nwhen rfm_weighted > 200 then 'I_200' else 'J_100 Lowest Penalty' end as rfm_segment \n, rfm.* \n, j.* \n, row_number() over (partition by job_source_id order by applystarts desc) as rn \n\nfrom rfm  \nleft join jobs_first_hour_ordered j on j.sourceid = rfm.job_source_id \nwhere rfm_weighted > 1000 or rfm_weighted < 200\nand j.job_ntile = 10 )\n\nselect * from subset where rn <= 5\n\n\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_fct_first_last_activity_dates_parent", "datalakehive.imhotep.grdm", "datalakehive.imhotep.jobactivitymetrics", "datalakehive.imhotep.searchimpressions" ],
  "queryIndex" : 180,
  "runStartToQueryComplete" : 1182
}, {
  "elapsedMillis" : 1551,
  "totalScheduledMillis" : 1072032,
  "cpuMillis" : 353802,
  "queuedMillis" : 0,
  "executeMillis" : 930,
  "getResultMillis" : 0,
  "iterateMillis" : 676,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 2939659228,
  "query" : "with msa_jobs as (\n    select sj.jobid as agg_job_id\n    , jlmsa\n    , trim(jlmsa) as jlmsa_trimmed\n    , sum(case when jlmsa!='' then 1 else 0 end) AS urban_jobs\n    , sum(case when jlmsa='' then 1 else 0 end) AS rural_jobs\n    , max_by(jlmsa, hour) as latest_msa\n    from datalake.imhotep.searchablejobs sj\n    where hour between date_format(from_unixtime(to_unixtime(date('2024-10-01'))), '%Y-%m-%d %H:%i:%s') and date_format(from_unixtime(to_unixtime(date('2024-10-02'))), '%Y-%m-%d %H:%i:%s')\n    group by 1,2,3\n)\n\nselect * from msa_jobs\nlimit 100\n",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 181,
  "runStartToQueryComplete" : 1191
}, {
  "elapsedMillis" : 2355,
  "totalScheduledMillis" : 2137,
  "cpuMillis" : 1483,
  "queuedMillis" : 0,
  "executeMillis" : 2184,
  "getResultMillis" : 0,
  "iterateMillis" : 217,
  "rows" : 12804,
  "error" : null,
  "scannedBytes" : 1708682,
  "query" : "WITH lib_base AS (\n\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      lib.teamId,\n      'lib' AS resource_type,\n      'n/a' AS env,\n      languageRuntime AS languageRuntime,\n      libraryName AS resource_name,\n      CAST(highestVersionPublishAgeDays AS BIGINT) AS age\n  FROM libraryFreshnessSnapshot lib\n  LEFT JOIN teamwrksSnapshot tm\n  ON lib.teamid = tm.teamid\n  WHERE lib.unixtime >= IMHOTEP_UNIXTIME('2d') AND lib.unixtime < IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime >= IMHOTEP_UNIXTIME('2d') AND tm.unixtime < IMHOTEP_UNIXTIME('1d')\n),\n\napp_base AS (\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      app.teamId,\n      'app' AS resource_type,\n      env,\n      applicationRuntime AS languageRuntime,\n      applicationName AS resource_name,\n      lockdownAge AS age\n  FROM applicationFreshnessSnapshot AS app\n  LEFT JOIN teamwrksSnapshot tm\n  ON app.teamid = tm.teamid\n  WHERE app.unixtime >= IMHOTEP_UNIXTIME('2d') AND app.unixtime < IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime >= IMHOTEP_UNIXTIME('2d') AND tm.unixtime < IMHOTEP_UNIXTIME('1d')\n)\n\n\nSELECT *\nFROM (\n  SELECT *\n  FROM app_base\n  UNION ALL\n  SELECT *\n  FROM lib_base\n)\nWHERE env != 'MISSING'",
  "queryTables" : [ "skipperhive.imhotep.applicationfreshnesssnapshot", "skipperhive.imhotep.libraryfreshnesssnapshot", "skipperhive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 182,
  "runStartToQueryComplete" : 1194
}, {
  "elapsedMillis" : 11486,
  "totalScheduledMillis" : 1600,
  "cpuMillis" : 1112,
  "queuedMillis" : 0,
  "executeMillis" : 11329,
  "getResultMillis" : 0,
  "iterateMillis" : 174,
  "rows" : 12804,
  "error" : null,
  "scannedBytes" : 1708682,
  "query" : "WITH lib_base AS (\n\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      lib.teamId,\n      'lib' AS resource_type,\n      'n/a' AS env,\n      languageRuntime AS languageRuntime,\n      libraryName AS resource_name,\n      CAST(highestVersionPublishAgeDays AS BIGINT) AS age\n  FROM libraryFreshnessSnapshot lib\n  LEFT JOIN teamwrksSnapshot tm\n  ON lib.teamid = tm.teamid\n  WHERE lib.unixtime >= IMHOTEP_UNIXTIME('2d') AND lib.unixtime < IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime >= IMHOTEP_UNIXTIME('2d') AND tm.unixtime < IMHOTEP_UNIXTIME('1d')\n),\n\napp_base AS (\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      app.teamId,\n      'app' AS resource_type,\n      env,\n      applicationRuntime AS languageRuntime,\n      applicationName AS resource_name,\n      lockdownAge AS age\n  FROM applicationFreshnessSnapshot AS app\n  LEFT JOIN teamwrksSnapshot tm\n  ON app.teamid = tm.teamid\n  WHERE app.unixtime >= IMHOTEP_UNIXTIME('2d') AND app.unixtime < IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime >= IMHOTEP_UNIXTIME('2d') AND tm.unixtime < IMHOTEP_UNIXTIME('1d')\n)\n\n\nSELECT *\nFROM (\n  SELECT *\n  FROM app_base\n  UNION ALL\n  SELECT *\n  FROM lib_base\n)\nWHERE env != 'MISSING'",
  "queryTables" : [ "skipperhive.imhotep.applicationfreshnesssnapshot", "skipperhive.imhotep.libraryfreshnesssnapshot", "skipperhive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 183,
  "runStartToQueryComplete" : 1204
}, {
  "elapsedMillis" : 6976,
  "totalScheduledMillis" : 38071,
  "cpuMillis" : 12219,
  "queuedMillis" : 0,
  "executeMillis" : 6703,
  "getResultMillis" : 0,
  "iterateMillis" : 298,
  "rows" : 59,
  "error" : null,
  "scannedBytes" : 101480338,
  "query" : "WITH lib_base AS (\n\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      lib.teamId,\n      'lib' AS resource_type,\n      'n/a' AS env,\n      languageRuntime AS languageRuntime,\n      libraryName AS resource_name,\n      CAST(highestVersionPublishAgeDays AS BIGINT) AS age,\n      lib.day\n  FROM libraryFreshnessSnapshot lib\n  LEFT JOIN teamwrksSnapshot tm\n  ON lib.teamid = tm.teamid\n  AND lib.day = DATE_FORMAT(CAST(tm.day AS TIMESTAMP), '%Y-%m-%d')\n  WHERE lib.unixtime BETWEEN IMHOTEP_UNIXTIME('60d') AND IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime BETWEEN IMHOTEP_UNIXTIME('60d') AND IMHOTEP_UNIXTIME('1d')\n),\n\napp_base AS (\n  SELECT\n      tier_0,\n      tier_1,\n      tier_2,\n      tier_3,\n      tier_4,\n      tier_5,\n      tier_6,\n      tier_7,\n      tier_8,\n      app.teamId,\n      'app' AS resource_type,\n      env,\n      applicationRuntime AS languageRuntime,\n      applicationName AS resource_name,\n      lockdownAge AS age,\n      app.day\n  FROM applicationFreshnessSnapshot AS app\n  LEFT JOIN teamwrksSnapshot tm\n  ON app.teamid = tm.teamid\n  AND app.day = DATE_FORMAT(CAST(tm.day AS TIMESTAMP), '%Y-%m-%d')\n  WHERE app.unixtime BETWEEN IMHOTEP_UNIXTIME('60d') AND IMHOTEP_UNIXTIME('1d')\n  AND tm.unixtime BETWEEN IMHOTEP_UNIXTIME('60d') AND IMHOTEP_UNIXTIME('1d')\n)\n\n\nSELECT\n\tday,\n    100.0 * COUNT_IF(age < 90)/COUNT(*) AS freshness_percentage\nFROM (\n  SELECT *\n  FROM app_base\n  UNION ALL\n  SELECT *\n  FROM lib_base\n)\nWHERE ('' = '' OR teamId like '')\n  AND ('Indeed' = '' OR tier_0 like 'Indeed')\n  AND ('' = '' OR tier_1 like '')\n  AND ('' = '' OR tier_2 like '')\n  AND ('' = '' OR tier_3 like '')\n  AND ('' = '' OR tier_4 like '')\n  AND ('' = '' OR tier_5 like '')\n  AND ('' = '' OR tier_6 like '')\n  AND ('' = '' OR tier_7 like '')\n  AND ('' = '' OR tier_8 like '')\nGROUP BY day",
  "queryTables" : [ "skipperhive.imhotep.applicationfreshnesssnapshot", "skipperhive.imhotep.libraryfreshnesssnapshot", "skipperhive.imhotep.teamwrkssnapshot", "skipperhive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 184,
  "runStartToQueryComplete" : 1204
}, {
  "elapsedMillis" : 471866,
  "totalScheduledMillis" : 316216767,
  "cpuMillis" : 40101994,
  "queuedMillis" : 0,
  "executeMillis" : 60871,
  "getResultMillis" : 0,
  "iterateMillis" : 411022,
  "rows" : 27,
  "error" : null,
  "scannedBytes" : 358026571253,
  "query" : "WITH\n\nw_va as (\n\tSELECT source_id, advertiser_id\n    FROM datalake.tiller.adsystemdb_tbladvertiserjobsource\n    WHERE status = 'VERIFIED'\n    GROUP BY 1,2),\n\nw_sj as (\n\tSELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(sj.unixtime)),'%Y-%m-%d') as weekStart\n\t, w_va.advertiser_id\n    FROM datalake.imhotep.searchablejobs sj\n\tINNER JOIN w_va ON sj.sourceid = w_va.source_id\n    WHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \n\tAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n\tAND jobcountry != 'JP'\n\tGROUP BY 1,2),\n    \nw_s as (\n\tSELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n\t, CAST(advertiserid as BIGINT) as advertiser_id\n    , COUNT(DISTINCT(requestTk)) as searches\n    , COUNT(DISTINCT(CASE WHEN totalhitcount>0 \n    \tTHEN requestTk ELSE NULL END)) as searchesWithResults\n    , COUNT(DISTINCT(CASE WHEN totalhitcount>0 AND clickdepth_csv IS NOT NULL \n    \tTHEN requestTk ELSE NULL END)) as searchesWithResultsAndClick\n\tFROM datalake.imhotep.employerjobsearch\n    WHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \n\t\tAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n\tAND lower(gqlOperationName) not like '%facet%' \n    AND lower(gqlOperationName) not like '%count%' \n    AND onehostmodule = './SimpleJobPicker' \n    /* ^^ Only inlcudes SJP until logging fix (EJSEARCH-1895) completes */\n\tAND country != 'JP'\n    AND coalesce(is_privileged,0) != 1\n    GROUP BY 1,2)\n       \nSELECT weekStart\n, searches\n, (searches-searchesWithResults) as searchesWithZeroResults\n, searchesWithResultsAndClick\n, format('%.2f%%', (cast(searchesWithResultsAndClick as double)/cast(searches as double)*100)) as netClickRate\nFROM\n    (SELECT w_s.weekStart\n    , SUM(w_s.searches) as searches\n    , SUM(w_s.searchesWithResults) as searchesWithResults\n    , SUM(w_s.searchesWithResultsAndClick) as searchesWithResultsAndClick\n    FROM w_s \n    INNER JOIN w_sj\n        ON w_s.advertiser_id = w_sj.advertiser_id \n        AND w_s.weekstart = w_sj.weekstart\n    GROUP BY 1)",
  "queryTables" : [ "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalakehive.imhotep.employerjobsearch", "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 185,
  "runStartToQueryComplete" : 1686
}, {
  "elapsedMillis" : 30571,
  "totalScheduledMillis" : 68021376,
  "cpuMillis" : 14367750,
  "queuedMillis" : 1,
  "executeMillis" : 6855,
  "getResultMillis" : 0,
  "iterateMillis" : 23742,
  "rows" : 934,
  "error" : null,
  "scannedBytes" : 814160302773,
  "query" : "WITH snapshot AS (\n    SELECT \n\t    acctid,\n        resume.resumetype as resumetype,\n        date_trunc('hour', from_unixtime(resume.datecreated)) AS hour_created\n    FROM datalake.jssdi.profile_snapshot\n    CROSS JOIN UNNEST(slice(array_sort(resumes, (r1, r2) -> IF(r1.datecreated<r2.datecreated, 1, IF(r1.datecreated=r2.datecreated, 0, -1))), 1, cardinality(resumes)-1)) AS resume -- get all resumes except first created\n    WHERE \n\t        day = '2024-10-24'\n        AND CARDINALITY(resumes) > 1\n), create_events AS (\n\tSELECT accountid, client, DATE_TRUNC('hour', FROM_UNIXTIME(unixtime)) AS hour_created\n    FROM datalake.imhotep.profileevents\n    WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-10') AND IMHOTEP_UNIXTIME('2024-10-17')\n    \tAND (eventtype = 'RESUME_CREATED' OR eventtype = 'PROFILE_CREATED')\n)\nSELECT t1.hour_created, resumetype, t2.client, count(*) as count\nFROM snapshot t1\nLEFT JOIN create_events t2\n\tON t1.acctid = t2.accountid AND t1.hour_created = t2.hour_created\nWHERE t1.hour_created BETWEEN DATE_PARSE('2024-10-10', '%Y-%m-%d') AND DATE_PARSE('2024-10-17', '%Y-%m-%d')\nGROUP BY t1.hour_created, resumetype, t2.client",
  "queryTables" : [ "datalakehive.imhotep.profileevents", "datalakehive.jssdi.profile_snapshot" ],
  "queryIndex" : 186,
  "runStartToQueryComplete" : 1254
}, {
  "elapsedMillis" : 155550,
  "totalScheduledMillis" : 120724949,
  "cpuMillis" : 9740657,
  "queuedMillis" : 0,
  "executeMillis" : 88095,
  "getResultMillis" : 0,
  "iterateMillis" : 67486,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 450717949686,
  "query" : "WITH experiment_data AS (\n\tSELECT \n    \tCASE WHEN ARRAYS_OVERLAP(ndxgrp, \n            ARRAY['#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275']) \n            THEN 'test' ELSE 'control' END AS _group, \n        q, jobid\n    FROM sponsored -- TABLESAMPLE BERNOULLI (10) -- sampling rate\n    WHERE \n    \tunixtime >= imhotep_unixtime('2024-10-15') AND unixtime < imhotep_unixtime('2024-11-12') -- test start date & end date\n        AND country = 'ca'\n        AND ARRAYS_OVERLAP(ndxgrp, \n        \tARRAY['#b15:idxmatchingcontrolplane_e274','#B15:idxmatchingcontrolplane_e274',\n            \t  '#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275'])\n)\n\nSELECT \n\t_group, \n    COUNT_IF(answer='No') as bad_matches, \n    COUNT() AS all_labels, \n    COUNT_IF(answer='No') / CAST(COUNT() AS DOUBLE) AS BMR\nFROM jobtoqueryrelevance\nINNER JOIN experiment_data ON\n\tjobtoqueryrelevance.query = experiment_data.q\n    AND jobtoqueryrelevance.jobid = experiment_data.jobid\nWHERE \n\tjobtoqueryrelevance.unixtime >= imhotep_unixtime('2024-01-01') AND jobtoqueryrelevance.unixtime < imhotep_unixtime('2024-11-12')\n\tAND jobtoqueryrelevance.country='CA'\nGROUP BY _group",
  "queryTables" : [ "skipperhive.imhotep.jobtoqueryrelevance", "skipperhive.imhotep.sponsored" ],
  "queryIndex" : 187,
  "runStartToQueryComplete" : 1403
}, {
  "elapsedMillis" : 1910,
  "totalScheduledMillis" : 1220425,
  "cpuMillis" : 281113,
  "queuedMillis" : 0,
  "executeMillis" : 1012,
  "getResultMillis" : 0,
  "iterateMillis" : 923,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 2887030920,
  "query" : "with msa_jobs as (\n    select sj.jobid as agg_job_id\n    , jlmsa\n    , trim(jlmsa) as jlmsa_trimmed\n    , sum(case when jlmsa!='' then 1 else 0 end) AS urban_jobs\n    , sum(case when jlmsa='' then 1 else 0 end) AS rural_jobs\n    , max_by(jlmsa, hour) as latest_msa\n    from datalake.imhotep.searchablejobs sj\n    where hour between date_format(from_unixtime(to_unixtime(date('2024-10-01'))), '%Y-%m-%d %H:%i:%s') and date_format(from_unixtime(to_unixtime(date('2024-10-02'))), '%Y-%m-%d %H:%i:%s')\n    group by 1,2,3\n)\n\nselect * from msa_jobs\nlimit 100\n",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 188,
  "runStartToQueryComplete" : 1264
}, {
  "elapsedMillis" : 385,
  "totalScheduledMillis" : 90,
  "cpuMillis" : 77,
  "queuedMillis" : 0,
  "executeMillis" : 273,
  "getResultMillis" : 0,
  "iterateMillis" : 131,
  "rows" : 147,
  "error" : null,
  "scannedBytes" : 228759,
  "query" : "select * from datalake.tiller.\"oplin_events_usage$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_usage$partitions" ],
  "queryIndex" : 189,
  "runStartToQueryComplete" : 1269
}, {
  "elapsedMillis" : 1790,
  "totalScheduledMillis" : 302,
  "cpuMillis" : 10,
  "queuedMillis" : 0,
  "executeMillis" : 1211,
  "getResultMillis" : 0,
  "iterateMillis" : 590,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 1266067,
  "query" : "SELECT CAST(FROM_UNIXTIME(unixtime) AS timestamp)  AS most_recent_unixtime, dc, ldap,query_duration_millis\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 190,
  "runStartToQueryComplete" : 1298
}, {
  "elapsedMillis" : 1103,
  "totalScheduledMillis" : 195,
  "cpuMillis" : 11,
  "queuedMillis" : 1,
  "executeMillis" : 738,
  "getResultMillis" : 0,
  "iterateMillis" : 377,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 1266067,
  "query" : "SELECT FROM_UNIXTIME(unixtime) AS most_recent_unixtime, dc, ldap,query_duration_millis\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 191,
  "runStartToQueryComplete" : 1300
}, {
  "elapsedMillis" : 1347,
  "totalScheduledMillis" : 121,
  "cpuMillis" : 13,
  "queuedMillis" : 0,
  "executeMillis" : 896,
  "getResultMillis" : 0,
  "iterateMillis" : 484,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 1266067,
  "query" : "SELECT FROM_UNIXTIME(unixtime) AT TIME ZONE 'UTC' AS most_recent_unixtime, dc, ldap,query_duration_millis\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 192,
  "runStartToQueryComplete" : 1301
}, {
  "elapsedMillis" : 14862,
  "totalScheduledMillis" : 22678650,
  "cpuMillis" : 3314940,
  "queuedMillis" : 0,
  "executeMillis" : 2557,
  "getResultMillis" : 0,
  "iterateMillis" : 12333,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 363460969769,
  "query" : "WITH\n\tunverified_accounts AS (\n        SELECT \n            accountid\n            , verified\n        FROM imhotep.passdailysnapshot\n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND verified != 1\n    ), profiles AS (\n        SELECT \n            acctid AS accountid\n            , resumes\n            , fileinformation\n        FROM jssdi.profile_snapshot \n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND cardinality(\n            \tFILTER(\n                \tfileinformation\n                    , f -> f.type = 'RESUME'\n                )\n            ) > 1\n            -- none resume or single resume in PARSED or IR type\n            AND (\n            \tresumes is null\n                OR ( CARDINALITY(resumes) = 1 and resumes[1].resumetype in ('PARSED', 'INDEED_RESUME')\n                )\n\t)\n   )\n    \n    SELECT\n    p.accountid\n    , '<batch_name>_xxx'\n--    , a.verified\n--    , CARDINALITY(resumes) as rez_count\n--    , if (resumes = null, null, resumes[1].resumetype) as rez_type\n--    , cardinality(\n--            \tFILTER(\n--                \tfileinformation\n--                    , f -> f.type = 'RESUME'\n--                )\n--            ) as rez_file_count\n    from profiles p\n    join unverified_accounts a\n    on p.accountid = a.accountid\n --   limit 100",
  "queryTables" : [ "skipperhive.imhotep.passdailysnapshot", "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 193,
  "runStartToQueryComplete" : 1316
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 4917,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001435_00119_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "SELECT\n \t\tappName,\n        moderationApi, \n        moderationType, \n        count(*) as request_count,\n        COUNT_IF(flagged=1) as flagged_count,\n        COUNT_IF(flagged=1)  * 100.0 / count(*) as block_rate_pct,\n        min(FROM_UNIXTIME(unixtime)) as start_date,\n\t\tmax(FROM_UNIXTIME(unixtime)) as end_date,\n        now() as query_current_date,\n        DATE_DIFF('hour', max(FROM_UNIXTIME(unixtime)), now()) as data_lag_hours\nFROM\n       logrepo.log.llm_proxy lp\n    WHERE\n        lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and ('ContentGenerationService' = '' or appName='ContentGenerationService')\n        and ('openAIModerations' = '' or moderationApi='openAIModerations')\n        and ('' = '' or moderationType='')\ngroup by 1, 2, 3",
  "queryTables" : [ "logrepo.log.llm_proxy" ],
  "queryIndex" : 194,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 2168,
  "totalScheduledMillis" : 994551,
  "cpuMillis" : 267681,
  "queuedMillis" : 0,
  "executeMillis" : 867,
  "getResultMillis" : 0,
  "iterateMillis" : 1353,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 2887337187,
  "query" : "with msa_jobs as (\n    select sj.jobid as agg_job_id\n    , jlmsa\n    , trim(jlmsa) as jlmsa_trimmed\n    , sum(case when jlmsa!='' then 1 else 0 end) AS urban_jobs\n    , sum(case when jlmsa='' then 1 else 0 end) AS rural_jobs\n    , max_by(jlmsa, hour) as latest_msa\n    from datalake.imhotep.searchablejobs sj\n    where hour between date_format(from_unixtime(to_unixtime(date('2024-10-01'))), '%Y-%m-%d %H:%i:%s') and date_format(from_unixtime(to_unixtime(date('2024-10-02'))), '%Y-%m-%d %H:%i:%s')\n    group by 1,2,3\n)\n\nselect * from msa_jobs\nlimit 100\n",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 195,
  "runStartToQueryComplete" : 1321
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 7168,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001439_00121_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "with examples as (\nSELECT\n\t\tmoderationtext,\n        flagged,\n        flaggedcategories,\n        flaggedwords,\n \t\tappName,\n        moderationApi, \n        moderationType, \n        FROM_UNIXTIME(unixtime) as request_date,\n        requestid,\n        reqbody,\n        respbody\nFROM\n       logrepo.log.llm_proxy lp\n    WHERE\n    \tflagged = 1\n        and lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and ('ContentGenerationService' = '' or appName='ContentGenerationService')\n        and ('openAIModerations' = '' or moderationApi='openAIModerations')\n        and ('' = '' or moderationType='')\nlimit 500\n)\n\nSELECT\n        m.*,\n        r.model,\n        r.moderationHeader,\n        r.locale        \nFROM logrepo.log.llm_proxy r\n        join examples m on m.requestid = r.requestid -- switching to imhotep for speed even though there is a 1D data lag\n    WHERE\n        r.unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and r.metric = 'llm-proxy'",
  "queryTables" : [ "logrepo.log.llm_proxy", "logrepo.log.llm_proxy" ],
  "queryIndex" : 196,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 18086,
  "totalScheduledMillis" : 1477391,
  "cpuMillis" : 696159,
  "queuedMillis" : 0,
  "executeMillis" : 5693,
  "getResultMillis" : 0,
  "iterateMillis" : 12422,
  "rows" : 1999,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_5 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 197,
  "runStartToQueryComplete" : 1340
}, {
  "elapsedMillis" : 4948,
  "totalScheduledMillis" : 4309315,
  "cpuMillis" : 506654,
  "queuedMillis" : 0,
  "executeMillis" : 967,
  "getResultMillis" : 0,
  "iterateMillis" : 4010,
  "rows" : 500,
  "error" : null,
  "scannedBytes" : 28356921137,
  "query" : "with examples as (\nSELECT\n\t\tmoderationtext,\n        flagged,\n        flaggedcategories,\n        flaggedwords,\n \t\tappName,\n        moderationApi, \n        moderationType,\n        FROM_UNIXTIME(unixtime) as request_date,\n        requestid       \nFROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n    \tflagged = '1'\n        and lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and ('ContentGenerationService' = '' or appName='ContentGenerationService')\n        and ('openAIModerations' = '' or moderationApi='openAIModerations')\n        and ('input' = '' or moderationType='input')\nlimit 500\n)\n\nSELECT\n        m.*,\n        r.model,\n        r.moderationHeader,\n        r.locale        \nFROM datalake.imhotep.llmproxy r\n        join examples m on m.requestid = r.requestid -- switching to imhotep for speed even though there is a 1D data lag\n    WHERE\n        r.unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and r.metric = 'llm-proxy'",
  "queryTables" : [ "datalakehive.imhotep.llmproxy", "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 198,
  "runStartToQueryComplete" : 1327
}, {
  "elapsedMillis" : 14208,
  "totalScheduledMillis" : 1527473,
  "cpuMillis" : 669568,
  "queuedMillis" : 0,
  "executeMillis" : 5231,
  "getResultMillis" : 0,
  "iterateMillis" : 9014,
  "rows" : 1999,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_6 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5, t6 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 199,
  "runStartToQueryComplete" : 1337
}, {
  "elapsedMillis" : 15988,
  "totalScheduledMillis" : 3903821,
  "cpuMillis" : 234039,
  "queuedMillis" : 0,
  "executeMillis" : 9049,
  "getResultMillis" : 0,
  "iterateMillis" : 7020,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 6986849668,
  "query" : "SELECT \n\tCOUNT(*) as total_profile_views, \n   \tSUM(r.sent) as outreaches,\n    SUM(r.resp_pos) as resp_pos,\n    COUNT(js.account_id) as profile_with_linkedIn,\n    ROUND(CAST(SUM(r.sent) AS DOUBLE) * 100 / COUNT(*), 1) as outreach_rate\nFROM \n\tdatalake.imhotep.rsresumeview v\n\nLEFT JOIN\n\tdatalake.imhotep.rscontacts r\nON\n\tv.recruiterAccountId = r.senderAccountId\n    AND v.resumeAccountId = r.recipientAccountId\n    AND r.unixtime BETWEEN IMHOTEP_UNIXTIME('5d') AND IMHOTEP_UNIXTIME('4d')\n\nLEFT JOIN \n\tdatalake.imhotep.js_fact_store_snapshot_prod js\nON\n\tv.resumeAccountId = js.account_id\n\tAND js.resume_links_size > 0\n\tAND CARDINALITY(FILTER(js.resume_links_array, x -> x LIKE '%linkedin%')) > 0\n    AND js.unixtime BETWEEN IMHOTEP_UNIXTIME('5d') AND IMHOTEP_UNIXTIME('4d')\n\nWHERE \n\tv.unixtime BETWEEN IMHOTEP_UNIXTIME('5d') AND IMHOTEP_UNIXTIME('4d')\n--    AND js.account_id IS NOT NULL -- with LinkedIn link\n    AND js.account_id IS NULL -- exclude linkedIn users\n--    AND r.batchSize = 1",
  "queryTables" : [ "datalakehive.imhotep.js_fact_store_snapshot_prod", "datalakehive.imhotep.rscontacts", "datalakehive.imhotep.rsresumeview" ],
  "queryIndex" : 200,
  "runStartToQueryComplete" : 1340
}, {
  "elapsedMillis" : 6143,
  "totalScheduledMillis" : 2768142,
  "cpuMillis" : 315100,
  "queuedMillis" : 0,
  "executeMillis" : 1909,
  "getResultMillis" : 0,
  "iterateMillis" : 4260,
  "rows" : 37,
  "error" : null,
  "scannedBytes" : 16193723695,
  "query" : "SELECT\n \t\tappName,\n        moderationApi, \n        moderationType, \n        count(*) as request_count,\n        COUNT_IF(flagged='1') as flagged_count,\n        COUNT_IF(flagged='1')  * 100.0 / count(*) as block_rate_pct,\n        min(FROM_UNIXTIME(unixtime)) as start_date,\n\t\tmax(FROM_UNIXTIME(unixtime)) as end_date,\n        now() as query_current_date,\n        DATE_DIFF('hour', max(FROM_UNIXTIME(unixtime)), now()) as data_lag_hours\nFROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\ngroup by 1, 2, 3",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 201,
  "runStartToQueryComplete" : 1332
}, {
  "elapsedMillis" : 14540,
  "totalScheduledMillis" : 328460,
  "cpuMillis" : 68915,
  "queuedMillis" : 1,
  "executeMillis" : 4848,
  "getResultMillis" : 0,
  "iterateMillis" : 9782,
  "rows" : 30,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        Lower(tw.tier_1) NOT IN ('global revenue', 'marketing')\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n\t('Marketplace'='' OR t1 = 'Marketplace')\nORDER BY date, t0, t1 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 202,
  "runStartToQueryComplete" : 1342
}, {
  "elapsedMillis" : 15170,
  "totalScheduledMillis" : 285240,
  "cpuMillis" : 59566,
  "queuedMillis" : 0,
  "executeMillis" : 4973,
  "getResultMillis" : 0,
  "iterateMillis" : 10234,
  "rows" : 869,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_4 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '')\nORDER BY date, t0, t1, t2, t3, t4 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 203,
  "runStartToQueryComplete" : 1344
}, {
  "elapsedMillis" : 7498,
  "totalScheduledMillis" : 2476230,
  "cpuMillis" : 268520,
  "queuedMillis" : 0,
  "executeMillis" : 998,
  "getResultMillis" : 0,
  "iterateMillis" : 6552,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 15939507737,
  "query" : "SELECT\n \t\tappName,\n        moderationApi, \n        moderationType,\n        flaggedwords,\n        count(*) as request_count,\n        sum(count(*)) over (partition by appName, moderationApi, moderationType) as app_request_count,\n        count(*) * 100.0 * 100.0 / sum(count(*)) over (partition by appName, moderationApi, moderationType) as bps_of_app_requests\nFROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n        lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and ('ContentGenerationService' = '' or appName='ContentGenerationService')\n        and moderationApi = 'mcmurdoDenylist'\n        and ('input' = '' or moderationType='input')\ngroup by 1, 2, 3,4\norder by 5 DESC\nlimit 500",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 204,
  "runStartToQueryComplete" : 1337
}, {
  "elapsedMillis" : 12766,
  "totalScheduledMillis" : 288398,
  "cpuMillis" : 56797,
  "queuedMillis" : 0,
  "executeMillis" : 2453,
  "getResultMillis" : 0,
  "iterateMillis" : 10379,
  "rows" : 240,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_3 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '')\nORDER BY date, t0, t1, t2, t3 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 205,
  "runStartToQueryComplete" : 1344
}, {
  "elapsedMillis" : 9560,
  "totalScheduledMillis" : 112835,
  "cpuMillis" : 34471,
  "queuedMillis" : 0,
  "executeMillis" : 3682,
  "getResultMillis" : 0,
  "iterateMillis" : 5910,
  "rows" : 173,
  "error" : null,
  "scannedBytes" : 235424030,
  "query" : "WITH populated_day AS (\n    SELECT MIN(day) AS most_recent_day\n    FROM (\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"jiracloudissues$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"ownershipSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"teamwrksSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n    )\n)\n\n-- Count of a11y violations remediated within SLO in the last 30 days / (All a11y violations remediated within last 30 days + Open issues outside SLO)\nSELECT\n    teamId,\n\tproject,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Closed_Within_SLO,\n    ALL_REM_30 AS All_Remediated_Last_30,\n    UNREM AS Open_Outside_SLO,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7\nFROM (\n\tSELECT\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        \n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- last 30 days\n                  ((ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 1000) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  ((TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 1000) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- last 30 days\n                  (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                -- last 30 days\n                (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.day = (SELECT most_recent_day from populated_day) AND\n        ow.day = (SELECT most_recent_day from populated_day) AND\n        tw.day = (SELECT most_recent_day from populated_day) AND\n        ji.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        ow.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        tw.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        tw.tier_1 NOT IN ('Global Revenue', 'Marketing')\n    GROUP BY ow.accountablePartyId, ji.projectkey, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n        HAVING COUNT(\n        CASE WHEN\n            ji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            -- last 30 days\n            (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n            THEN 1 END\n    ) IS NOT NULL AND (\n        COUNT(\n            CASE WHEN\n                ji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                -- last 30 days\n                (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n                THEN 1 END\n        ) + COUNT(\n            CASE WHEN\n                (\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n                ) AND (\n                    (\n                        ji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                        (ji.unixtime - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400 > 30\n                    ) OR (\n                        -- SLO days to complete\n                        ji.priority NOT IN ('Blocker') AND\n                        (ji.unixtime - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) > 0\n    )\n)\nWHERE\n\t('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY teamId, project, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.jiracloudissues$partitions", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.ownershipsnapshot$partitions", "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot$partitions" ],
  "queryIndex" : 206,
  "runStartToQueryComplete" : 1342
}, {
  "elapsedMillis" : 9529,
  "totalScheduledMillis" : 207972,
  "cpuMillis" : 56022,
  "queuedMillis" : 0,
  "executeMillis" : 2986,
  "getResultMillis" : 0,
  "iterateMillis" : 6560,
  "rows" : 2555,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_7 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '') \nORDER BY date, t0, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 207,
  "runStartToQueryComplete" : 1346
}, {
  "elapsedMillis" : 14085,
  "totalScheduledMillis" : 1725838,
  "cpuMillis" : 741582,
  "queuedMillis" : 0,
  "executeMillis" : 3373,
  "getResultMillis" : 0,
  "iterateMillis" : 10750,
  "rows" : 1999,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_7 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 208,
  "runStartToQueryComplete" : 1350
}, {
  "elapsedMillis" : 6609,
  "totalScheduledMillis" : 116183,
  "cpuMillis" : 24686,
  "queuedMillis" : 0,
  "executeMillis" : 1125,
  "getResultMillis" : 0,
  "iterateMillis" : 5553,
  "rows" : 6,
  "error" : null,
  "scannedBytes" : 37237234,
  "query" : "select day,\njob_visibility_level,\ncount_if(is_live=1) as count_is_live\nfrom datalake.imhotep.live_jobs\nwhere day BETWEEN '2024-10-01' AND '2024-10-02'\ngroup by 1,2\norder by 1,2",
  "queryTables" : [ "datalakehive.imhotep.live_jobs" ],
  "queryIndex" : 209,
  "runStartToQueryComplete" : 1344
}, {
  "elapsedMillis" : 923,
  "totalScheduledMillis" : 109,
  "cpuMillis" : 14,
  "queuedMillis" : 0,
  "executeMillis" : 759,
  "getResultMillis" : 0,
  "iterateMillis" : 282,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 1266067,
  "query" : "SELECT CAST(FROM_UNIXTIME(unixtime) AS timestamp)  AS most_recent_unixtime, dc, ldap,query_duration_millis\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 210,
  "runStartToQueryComplete" : 1338
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 570,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001456_00136_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "SELECT min(day) FROM jssdi.profile_snapshot WHERE day >= '2020-01-01'",
  "queryTables" : [ "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 211,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 9358,
  "totalScheduledMillis" : 227856,
  "cpuMillis" : 54030,
  "queuedMillis" : 0,
  "executeMillis" : 2500,
  "getResultMillis" : 0,
  "iterateMillis" : 6882,
  "rows" : 2303,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_6 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '')\nORDER BY date, t0, t1, t2, t3, t4, t5, t6 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 212,
  "runStartToQueryComplete" : 1349
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 2452,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001458_00138_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "SELECT \n\tELEMENT_AT(data, 'rum_context_element') as element,\n    ELEMENT_AT(data, 'rum_context_module') as module\nFROM \n\tlogrepo.raw_log.one_host_rum_action\nWHERE \n\tunixtime > IMHOTEP_UNIXTIME('yesterday') AND\n    ELEMENT_AT(data, 'rum_view_name') = '/jobs' AND\n    ELEMENT_AT(data, 'rum_context_scope') = 'jobman-feature-modules' AND\n    ELEMENT_AT(data, 'rum_context_customActionType') = 'click' AND\n    ELEMENT_AT(data, 'rum_context_additional_searchTkUuid') IS NOT NULL AND\n    ELEMENT_AT(data, 'rum_context_additional_clickDepthIndex') IS NOT NULL AND\n    ELEMENT_AT(data, 'rum_context_additional_clickDepthPage') IS NOT NULL\n--    CARDINALITY(SPLIT(ELEMENT_AT(data, 'rum_context_additional_clickDepthIndex'), ',')) > 1 AND\n--    CARDINALITY(SPLIT(ELEMENT_AT(data, 'rum_context_additional_clickDepthPage'), ',')) > 1\nGROUP BY\n\tELEMENT_AT(data, 'rum_context_element'),\n    ELEMENT_AT(data, 'rum_context_module')",
  "queryTables" : [ "logrepo.raw_log.one_host_rum_action" ],
  "queryIndex" : 213,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 13724,
  "totalScheduledMillis" : 2981374,
  "cpuMillis" : 1011434,
  "queuedMillis" : 1,
  "executeMillis" : 2764,
  "getResultMillis" : 0,
  "iterateMillis" : 10994,
  "rows" : 1999,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_4 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0  \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 214,
  "runStartToQueryComplete" : 1355
}, {
  "elapsedMillis" : 8521,
  "totalScheduledMillis" : 89491,
  "cpuMillis" : 16554,
  "queuedMillis" : 0,
  "executeMillis" : 3132,
  "getResultMillis" : 0,
  "iterateMillis" : 5424,
  "rows" : 5,
  "error" : null,
  "scannedBytes" : 244816122,
  "query" : "WITH populated_day AS (\n    SELECT MIN(day) AS most_recent_day\n    FROM (\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"jiracloudissues$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"ownershipSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"teamwrksSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n    )\n),\n\nslo_data AS (\n    SELECT\n        teamId,\n        project,\n        CASE\n            WHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n            ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n        END AS SLO_Score,\n        REM_30 AS Closed_Within_SLO,\n        ALL_REM_30 AS All_Remediated_Last_30,\n        UNREM AS Open_Outside_SLO,\n        t1,\n        t2,\n        t3,\n        t4,\n        t5,\n        t6,\n        t7\n    FROM (\n        SELECT\n            ji.projectkey AS project,\n            ow.accountablePartyId AS teamId,\n            tw.tier_1 AS t1,\n            tw.tier_2 AS t2,\n            tw.tier_3 AS t3,\n            tw.tier_4 AS t4,\n            tw.tier_5 AS t5,\n            tw.tier_6 AS t6,\n            tw.tier_7 AS t7,\n\n            COUNT(CASE WHEN\n                ji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n                (\n                    (ji.priority = 'Blocker' AND\n                    ((ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 1000) / 86400 <= 30 AND\n                    ((TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 1000) / 86400000 <= 30) OR \n                    (ji.priority NOT IN ('Blocker') AND\n                    (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000) / 86400 <= 30 AND\n                    (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400000 <= 90)\n                )\n            THEN 1 END) AS REM_30,\n\n            COUNT(CASE WHEN\n                ji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n            THEN 1 END) AS ALL_REM_30,\n\n            COUNT(CASE WHEN\n                ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                (\n                    (ji.priority = 'Blocker' AND (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 > 30) OR\n                    (ji.priority NOT IN ('Blocker') AND (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 > 90)\n                )\n            THEN 1 END) AS UNREM\n        FROM datalake.imhotep.jiracloudissues AS ji\n        JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n        JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n        WHERE\n            ji.day = (SELECT most_recent_day FROM populated_day) AND\n            ow.day = (SELECT most_recent_day FROM populated_day) AND\n            tw.day = (SELECT most_recent_day FROM populated_day) AND\n            ji.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n            ow.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n            tw.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n            CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n            NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n            ji.projectkey NOT IN ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n            ji.issuetype = 'Bug' AND\n            tw.tier_1 NOT IN ('Global Revenue', 'Marketing')\n        GROUP BY ow.accountablePartyId, ji.projectkey, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n    )\n)\n\nSELECT *\nFROM slo_data\nWHERE \n    SLO_Score > 0 AND SLO_Score != 100 AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY teamId, project, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.jiracloudissues$partitions", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.ownershipsnapshot$partitions", "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot$partitions" ],
  "queryIndex" : 215,
  "runStartToQueryComplete" : 1350
}, {
  "elapsedMillis" : 7682,
  "totalScheduledMillis" : 338952,
  "cpuMillis" : 63497,
  "queuedMillis" : 0,
  "executeMillis" : 2714,
  "getResultMillis" : 0,
  "iterateMillis" : 4994,
  "rows" : 120,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing')\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '')\nORDER BY date, t0, t1, t2 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 216,
  "runStartToQueryComplete" : 1351
}, {
  "elapsedMillis" : 1770,
  "totalScheduledMillis" : 150,
  "cpuMillis" : 16,
  "queuedMillis" : 0,
  "executeMillis" : 704,
  "getResultMillis" : 0,
  "iterateMillis" : 1086,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 1266067,
  "query" : "SELECT FROM_UNIXTIME(unixtime) AT TIME ZONE 'UTC' AS most_recent_unixtime, dc, ldap,query_duration_millis\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 217,
  "runStartToQueryComplete" : 1346
}, {
  "elapsedMillis" : 7572,
  "totalScheduledMillis" : 129101,
  "cpuMillis" : 22154,
  "queuedMillis" : 0,
  "executeMillis" : 3433,
  "getResultMillis" : 0,
  "iterateMillis" : 4151,
  "rows" : 714,
  "error" : null,
  "scannedBytes" : 609492687,
  "query" : "WITH\n\npopulated_day AS (\n    SELECT MIN(day) AS most_recent_day\n    FROM (\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"jiracloudissues$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"ownershipSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"teamwrksSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n    )\n)\n\n-- Count of a11y violations remediated within SLO in the last 30 days / (All a11y violations remediated within last 30 days + Open issues)\nSELECT\n    inslo,\n    duedate,\n    teamId,\n\t-- project,\n    issuekey,\n    -- issuetype,\n    priority,\n    status,\n    summary,\n    lastupdated,\n    createdate,\n    labels,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7\nFROM (\n\tSELECT\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        ji.issuekey AS issuekey,\n        ji.issuetype AS issuetype,\n        ji.priority AS priority,\n        ji.summary AS summary,\n        ji.status AS status,\n        DATE_FORMAT(FROM_UNIXTIME(CAST(ji.lastupdated AS BIGINT) / 1000), '%Y-%m-%d') AS lastupdated,\n        DATE_FORMAT(DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d'), '%Y-%m-%d') AS createdate,\n        DATE_FORMAT(DATE_ADD('day',\n        \tCASE \n            \tWHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')), '%Y-%m-%d') AS duedate,\n        CAST(duedate >= DATE_FORMAT(CURRENT_DATE, '%Y-%m-%d') AS BOOLEAN) AS inslo,\n        ji.labels AS labels,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.day = (SELECT most_recent_day from populated_day) AND\n        ow.day = (SELECT most_recent_day from populated_day) AND\n        tw.day = (SELECT most_recent_day from populated_day) AND\n        ji.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        ow.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        tw.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            -- The line below will include \"Pending Closure\" tickets in the resulting table\n            -- OR ji.resolutiontimestamp = 0\n        )\n    GROUP BY\n    \tow.accountablePartyId,\n    \tji.projectkey,\n        ji.issuekey,\n\t\tji.issuetype,\n        ji.priority,\n        ji.summary,\n        ji.status,\n        DATE_FORMAT(FROM_UNIXTIME(CAST(ji.lastupdated AS BIGINT) / 1000), '%Y-%m-%d'),\n        DATE_FORMAT(DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d'), '%Y-%m-%d'),\n        DATE_FORMAT(DATE_ADD('day',\n        \tCASE \n            \tWHEN ji.priority = 'Blocker' THEN 30\n                ELSE 90 \n           \tEND, DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')), '%Y-%m-%d'),\n        CAST(duedate >= DATE_FORMAT(CURRENT_DATE, '%Y-%m-%d') AS BOOLEAN),\n        ji.labels,\n        tw.tier_0,\n        tw.tier_1,\n        tw.tier_2,\n        tw.tier_3,\n        tw.tier_4,\n        tw.tier_5,\n        tw.tier_6,\n        tw.tier_7\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    (''='' OR t7 = '')\nORDER BY \n\tinslo ASC,\n\tduedate ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.jiracloudissues$partitions", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.ownershipsnapshot$partitions", "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot$partitions" ],
  "queryIndex" : 218,
  "runStartToQueryComplete" : 1353
}, {
  "elapsedMillis" : 7225,
  "totalScheduledMillis" : 355164,
  "cpuMillis" : 69021,
  "queuedMillis" : 1,
  "executeMillis" : 2521,
  "getResultMillis" : 0,
  "iterateMillis" : 4728,
  "rows" : 1642,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_5 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '')\nORDER BY date, t0, t1, t2, t3, t4, t5 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 219,
  "runStartToQueryComplete" : 1355
}, {
  "elapsedMillis" : 12054,
  "totalScheduledMillis" : 2937508,
  "cpuMillis" : 857271,
  "queuedMillis" : 0,
  "executeMillis" : 2403,
  "getResultMillis" : 0,
  "iterateMillis" : 9683,
  "rows" : 1999,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0\n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 220,
  "runStartToQueryComplete" : 1364
}, {
  "elapsedMillis" : 11868,
  "totalScheduledMillis" : 2749186,
  "cpuMillis" : 841090,
  "queuedMillis" : 0,
  "executeMillis" : 2681,
  "getResultMillis" : 0,
  "iterateMillis" : 9220,
  "rows" : 1999,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace')\nORDER BY date, teamId, project, t0, t1 ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 221,
  "runStartToQueryComplete" : 1365
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 78367,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001516_00147_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "WITH experiment_data AS (\n\tSELECT \n    \tCASE WHEN ARRAYS_OVERLAP(ndxgrp, \n            ARRAY['#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275']) \n            THEN 'test' ELSE 'control' END AS _group, \n        q, jobid\n    FROM mobileorganic -- TABLESAMPLE BERNOULLI (10) -- sampling rate\n    WHERE \n    \tunixtime >= imhotep_unixtime('2024-10-15') AND unixtime < imhotep_unixtime('2024-11-12') -- test start date & end date\n        AND country = 'ca'\n        AND ARRAYS_OVERLAP(ndxgrp, \n        \tARRAY['#b15:idxmatchingcontrolplane_e274','#B15:idxmatchingcontrolplane_e274',\n            \t  '#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275'])\n)\n\nSELECT \n\t_group, \n    COUNT_IF(answer='No') as bad_matches, \n    COUNT() AS all_labels, \n    COUNT_IF(answer='No') / CAST(COUNT() AS DOUBLE) AS BMR\nFROM jobtoqueryrelevance\nINNER JOIN experiment_data ON\n\tjobtoqueryrelevance.query = experiment_data.q\n    AND jobtoqueryrelevance.jobid = experiment_data.jobid\nWHERE \n\tjobtoqueryrelevance.unixtime >= imhotep_unixtime('2024-01-01') AND jobtoqueryrelevance.unixtime < imhotep_unixtime('2024-11-12')\n\tAND jobtoqueryrelevance.country='CA'\nGROUP BY _group",
  "queryTables" : [ "skipperhive.imhotep.jobtoqueryrelevance", "skipperhive.imhotep.mobileorganic" ],
  "queryIndex" : 222,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 14899,
  "totalScheduledMillis" : 10985156,
  "cpuMillis" : 1411238,
  "queuedMillis" : 0,
  "executeMillis" : 9147,
  "getResultMillis" : 0,
  "iterateMillis" : 11152,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 31127390221,
  "query" : "WITH scalablesegmenthomepageimpression_ AS (\n  SELECT accountid\n         ,agg_job_id \n         ,qual_attributes_job_js_overlap_positive_count\n         ,qual_job_attributes_count\n         ,'scalablesegmenthomepageimpression' AS source\n  FROM datalake.imhotep.scalablesegmenthomepageimpression\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-02') AND IMHOTEP_UNIXTIME('2024-10-03')\n  AND qual_attributes_job_js_overlap_positive_count > 0\n  AND  qual_job_attributes_count > 0 \n  GROUP BY 1, 2, 3, 4\n)\n\nSELECT a.account_id\n       ,a.job_id\n       ,a.overlappingjobandjsqualifications AS cnt_overlapping_quals_mce\n       ,b.qual_attributes_job_js_overlap_positive_count AS cnt_overlapping_quals_scalableseg\n       ,a.numjobqualifications AS cnt_job_quals_mce\n       ,b.qual_job_attributes_count AS cnt_job_quals_scalableseg\nFROM datalake.imhotep.mce_evaluation AS a \nINNER JOIN scalablesegmenthomepageimpression_ AS b\nON a.account_id = b.accountid \nAND a.job_id = b.agg_job_id\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-02') AND IMHOTEP_UNIXTIME('2024-10-03')\nAND a.overlappingjobandjsqualifications != b.qual_attributes_job_js_overlap_positive_count \nAND a.numjobqualifications != b.qual_job_attributes_count\nLIMIT 10 ",
  "queryTables" : [ "datalakehive.imhotep.mce_evaluation", "datalakehive.imhotep.scalablesegmenthomepageimpression" ],
  "queryIndex" : 223,
  "runStartToQueryComplete" : 1380
}, {
  "elapsedMillis" : 14176,
  "totalScheduledMillis" : 2782468,
  "cpuMillis" : 861107,
  "queuedMillis" : 0,
  "executeMillis" : 2779,
  "getResultMillis" : 0,
  "iterateMillis" : 11432,
  "rows" : 1999,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace') AND\n    (''='' OR t2 = '')\nORDER BY date, teamId, project, t0, t1, t2 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 224,
  "runStartToQueryComplete" : 1374
}, {
  "elapsedMillis" : 352911,
  "totalScheduledMillis" : 185977675,
  "cpuMillis" : 27082337,
  "queuedMillis" : 0,
  "executeMillis" : 84041,
  "getResultMillis" : 0,
  "iterateMillis" : 268905,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 450746253766,
  "query" : "WITH experiment_data AS (\n\tSELECT \n    \tCASE WHEN ARRAYS_OVERLAP(ndxgrp, \n            ARRAY['#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275']) \n            THEN 'test' ELSE 'control' END AS _group, \n        q, jobid\n    FROM sponsored -- TABLESAMPLE BERNOULLI (10) -- sampling rate\n    WHERE \n    \tunixtime >= imhotep_unixtime('2024-10-15') AND unixtime < imhotep_unixtime('2024-11-12') -- test start date & end date\n        AND country = 'ca'\n        AND ARRAYS_OVERLAP(ndxgrp, \n        \tARRAY['#b15:idxmatchingcontrolplane_e274','#B15:idxmatchingcontrolplane_e274',\n            \t  '#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275'])\n        AND EXISTS(SELECT * FROM synset WHERE CONTAINS(synset.cp_configs, 'SQMATCHING-6593-destination') AND synset.status='IN_PRODUCTION' AND synset.unixtime >= imhotep_unixtime('3d') AND synset.unixtime < imhotep_unixtime('2d') AND CONTAINS(synset.query_variants, sponsored.q))\n)\n\nSELECT \n\t_group, \n    COUNT_IF(answer='No') as bad_matches, \n    COUNT() AS all_labels, \n    COUNT_IF(answer='No') / CAST(COUNT() AS DOUBLE) AS BMR\nFROM jobtoqueryrelevance\nINNER JOIN experiment_data ON\n\tjobtoqueryrelevance.query = experiment_data.q\n    AND jobtoqueryrelevance.jobid = experiment_data.jobid\nWHERE \n\tjobtoqueryrelevance.unixtime >= imhotep_unixtime('2024-01-01') AND jobtoqueryrelevance.unixtime < imhotep_unixtime('2024-11-12')\n\tAND jobtoqueryrelevance.country='CA'\nGROUP BY _group",
  "queryTables" : [ "skipperhive.imhotep.jobtoqueryrelevance", "skipperhive.imhotep.sponsored", "skipperhive.imhotep.synset" ],
  "queryIndex" : 225,
  "runStartToQueryComplete" : 1722
}, {
  "elapsedMillis" : 396430,
  "totalScheduledMillis" : 2268050,
  "cpuMillis" : 746074,
  "queuedMillis" : 0,
  "executeMillis" : 4849,
  "getResultMillis" : 0,
  "iterateMillis" : 391597,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 181074159,
  "query" : "select count(1) from logrepo.raw_log.orgClk where unixtime >= imhotep_unixtime('2019-11-02 00:00:00') and unixtime < imhotep_unixtime('2019-11-02 01:00:00')",
  "queryTables" : [ "logrepo.raw_log.orgclk" ],
  "queryIndex" : 226,
  "runStartToQueryComplete" : 1769
}, {
  "elapsedMillis" : 370,
  "totalScheduledMillis" : 58,
  "cpuMillis" : 47,
  "queuedMillis" : 0,
  "executeMillis" : 313,
  "getResultMillis" : 0,
  "iterateMillis" : 67,
  "rows" : 21,
  "error" : null,
  "scannedBytes" : 13806,
  "query" : "select * from datalake.tiller.\"oplin_events_start$snapshots\"",
  "queryTables" : [ "datalake.tiller.oplin_events_start$snapshots" ],
  "queryIndex" : 227,
  "runStartToQueryComplete" : 1384
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 36675,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001544_00153_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "SELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n--, (case when feedid = 50461 then 'Hosted' else 'Indexed' end) as jobProduct\n--, count(DISTINCT(jobId)) as totalJobs\n, count(DISTINCT(case when has_employer_base_pay = 1 then jobId else null end)) as totalExplicitPayJobs\n, count(DISTINCT(case when jlpostal != '' OR jlenhanced = 1 then jobId else null end)) as totalBetterLocationJobs\n, count(DISTINCT(case when contains(applyvisibility, 'mobile') then jobId else null end)) as totalIndeedApplyableJobs\n, count(DISTINCT(case when has_employer_base_pay = 1 AND contains(applyvisibility, 'mobile') AND (jlpostal != '' OR jlenhanced = 1) then jobId else null end)) as totalBetterJobs\n-- logic for the above fields comes from https://link.indeed.tech/YD476T \nFROM datalake.imhotep.searchablejobs\nWHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \nAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\nAND jobcountry != 'JP'\nGROUP BY 1 --,2",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 228,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 558,
  "totalScheduledMillis" : 261,
  "cpuMillis" : 113,
  "queuedMillis" : 0,
  "executeMillis" : 293,
  "getResultMillis" : 0,
  "iterateMillis" : 294,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 445175,
  "query" : "select * from datalake.tiller.\"oplin_events_start$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_start$partitions" ],
  "queryIndex" : 229,
  "runStartToQueryComplete" : 1396
}, {
  "elapsedMillis" : 361,
  "totalScheduledMillis" : 54,
  "cpuMillis" : 44,
  "queuedMillis" : 0,
  "executeMillis" : 288,
  "getResultMillis" : 0,
  "iterateMillis" : 83,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 445175,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_start$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_start$partitions" ],
  "queryIndex" : 230,
  "runStartToQueryComplete" : 1413
}, {
  "elapsedMillis" : 3851,
  "totalScheduledMillis" : 3857110,
  "cpuMillis" : 387437,
  "queuedMillis" : 0,
  "executeMillis" : 1470,
  "getResultMillis" : 0,
  "iterateMillis" : 2703,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 2926315293,
  "query" : "with msa_jobs as (\n    select sj.jobid as agg_job_id\n    , jlmsa\n    , trim(jlmsa) as jlmsa_trimmed\n    , sum(case when jlmsa!='' then 1 else 0 end) AS urban_jobs\n    , sum(case when jlmsa='' then 1 else 0 end) AS rural_jobs\n    , max_by(jlmsa, hour) as latest_msa\n    from datalake.imhotep.searchablejobs sj\n    where hour between date_format(from_unixtime(to_unixtime(date('2024-10-01'))), '%Y-%m-%d %H:%i:%s') and date_format(from_unixtime(to_unixtime(date('2024-10-02'))), '%Y-%m-%d %H:%i:%s')\n    group by 1,2,3\n)\n\nselect * from msa_jobs\nlimit 100\n",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 231,
  "runStartToQueryComplete" : 1438
}, {
  "elapsedMillis" : 305248,
  "totalScheduledMillis" : 124527410,
  "cpuMillis" : 6325434,
  "queuedMillis" : 0,
  "executeMillis" : 13523,
  "getResultMillis" : 0,
  "iterateMillis" : 291783,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 190947249598,
  "query" : "SELECT \n\tCOUNT(*) as total_profile_views, \n   \tSUM(r.sent) as outreaches,\n    SUM(r.resp_pos) as resp_pos,\n    COUNT(js.account_id) as profile_with_linkedIn,\n    ROUND(CAST(SUM(r.sent) AS DOUBLE) * 100 / COUNT(*), 1) as outreach_rate\nFROM \n\tdatalake.imhotep.rsresumeview v\n\nLEFT JOIN\n\tdatalake.imhotep.rscontacts r\nON\n\tv.recruiterAccountId = r.senderAccountId\n    AND v.resumeAccountId = r.recipientAccountId\n    AND r.unixtime BETWEEN IMHOTEP_UNIXTIME('29d') AND IMHOTEP_UNIXTIME('1d')\n\nLEFT JOIN \n\tdatalake.imhotep.js_fact_store_snapshot_prod js\nON\n\tv.resumeAccountId = js.account_id\n\tAND js.resume_links_size > 0\n\tAND CARDINALITY(FILTER(js.resume_links_array, x -> x LIKE '%linkedin%')) > 0\n    AND js.unixtime BETWEEN IMHOTEP_UNIXTIME('29d') AND IMHOTEP_UNIXTIME('1d')\n\nWHERE \n\tv.unixtime BETWEEN IMHOTEP_UNIXTIME('29d') AND IMHOTEP_UNIXTIME('1d')\n--    AND js.account_id IS NOT NULL -- with LinkedIn link\n    AND js.account_id IS NULL -- exclude linkedIn users\n--    AND r.batchSize = 1",
  "queryTables" : [ "datalakehive.imhotep.js_fact_store_snapshot_prod", "datalakehive.imhotep.rscontacts", "datalakehive.imhotep.rsresumeview" ],
  "queryIndex" : 232,
  "runStartToQueryComplete" : 1743
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 1775,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001638_00158_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "SELECT min(day) FROM jssdi.profile_snapshot WHERE day >= '2020-01-01'",
  "queryTables" : [ "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 233,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 5987,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001639_00159_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "WITH date_range AS (\n    SELECT\n        IMHOTEP_UNIXTIME('3month') AS start_date,\n        IMHOTEP_UNIXTIME('today') AS end_date\n)\nSELECT\n    COUNT(DISTINCT signin.accountid)\nFROM\n    datalake.imhotep.passdailysnapshot snapshot\nJOIN\n    datalake.imhotep.passsigninattempt signin ON snapshot.accountid = signin.accountid\nJOIN\n    date_range dr ON snapshot.unixtime BETWEEN dr.start_date AND dr.end_date\n    AND signin.unixtime BETWEEN dr.start_date AND dr.end_date\nWHERE\n    snapshot.deleted = 0\n    AND signin.successful = 1",
  "queryTables" : [ "datalakehive.imhotep.passdailysnapshot", "datalakehive.imhotep.passsigninattempt" ],
  "queryIndex" : 234,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 148063,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001641_00160_mzvde): Cannot cast '6377170458' to INT",
  "scannedBytes" : 0,
  "query" : "--ca_en with QB exclusion seasonaljobsearch\nwith base as( \nselect accountid,upper(country) as country,lang as language \nfrom imhotep.jobsearch \nwhere unixtime between imhotep_unixtime('7d') and imhotep_unixtime('0d') \nand contains(grp, 'privileged') = false \nand contains(grp, 'spider') = false\nand contains(rcv, 'jsv') = true \nand contains(rcv, 'interaction') = true\nand contains(useragent, 'catchpoint') = false\nand accountid > 0 \nand country='ca'\nand contains(slcity,'quebec')=false\nand ipcountry='ca'\nand lang='fr'\nand regexp_like(qnorm, '.*(christmas|part|time|parttime|seasonalweekend|evening|hiring|immediately|flexible|night|shift|temporary|temps).*')\nunion\nselect accountid,upper(country) as country,lang as language \nfrom imhotep.mobsearch \nwhere unixtime between imhotep_unixtime('7d') and imhotep_unixtime('0d') \nand contains(grp, 'privileged') = false \nand contains(grp, 'spider') = false\nand contains(rcv, 'jsv') = true \nand contains(rcv, 'interaction')=true\nand contains(useragent, 'catchpoint')=false \nand accountid > 0 \nand country='ca'\nand contains(slcity,'quebec')=false\nand ipcountry='ca'\nand lang='fr'\nand regexp_like(qnorm, '.*(christmas|part|time|parttime|seasonalweekend|evening|hiring|immediately|flexible|night|shift|temporary|temps).*')), \nexclusions as (\nselect cast(accountId AS INT) as account_id\nfrom imhotep.advertiserUsers\nwhere unixtime between imhotep_unixtime('7d') and imhotep_unixtime('0d')\nand cast(accountId AS INT) > 0\nunion\nSELECT reportedAccountId AS account_id\nFROM imhotep.fraudulentResumeStatusActivity\nWHERE isfraud='1'\nAND unixtime between imhotep_unixtime('2019-12-09') and imhotep_unixtime('0d'))\nselect base.accountId,base.country,base.language\nfrom base\nleft join exclusions on base.accountid=exclusions.account_id\nwhere exclusions.account_id is null",
  "queryTables" : [ "skipperhive.imhotep.advertiserusers", "skipperhive.imhotep.fraudulentresumestatusactivity", "skipperhive.imhotep.jobsearch", "skipperhive.imhotep.mobsearch" ],
  "queryIndex" : 235,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 313,
  "totalScheduledMillis" : 52,
  "cpuMillis" : 41,
  "queuedMillis" : 0,
  "executeMillis" : 300,
  "getResultMillis" : 0,
  "iterateMillis" : 61,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 445175,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_start$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_start$partitions" ],
  "queryIndex" : 236,
  "runStartToQueryComplete" : 1444
}, {
  "elapsedMillis" : 598096,
  "totalScheduledMillis" : 237968621,
  "cpuMillis" : 87288344,
  "queuedMillis" : 0,
  "executeMillis" : 4828,
  "getResultMillis" : 0,
  "iterateMillis" : 593371,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 6229383923884,
  "query" : "SELECT\n \t\tappName,\n        moderationApi, \n        moderationType, \n        count(*) as request_count,\n        COUNT_IF(flagged=1) as flagged_count,\n        COUNT_IF(flagged=1)  * 100.0 / count(*) as block_rate_pct,\n        min(FROM_UNIXTIME(unixtime)) as start_date,\n\t\tmax(FROM_UNIXTIME(unixtime)) as end_date,\n        now() as query_current_date,\n        DATE_DIFF('hour', max(FROM_UNIXTIME(unixtime)), now()) as data_lag_hours\nFROM\n       logrepo.log.llm_proxy lp\n    WHERE\n        lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and ('ContentGenerationService' = '' or appName='ContentGenerationService')\n        and ('openAIModerations' = '' or moderationApi='openAIModerations')\n        and ('input' = '' or moderationType='input')\ngroup by 1, 2, 3",
  "queryTables" : [ "logrepo.log.llm_proxy" ],
  "queryIndex" : 237,
  "runStartToQueryComplete" : 2049
}, {
  "elapsedMillis" : 16905,
  "totalScheduledMillis" : 1192363,
  "cpuMillis" : 230793,
  "queuedMillis" : 0,
  "executeMillis" : 5595,
  "getResultMillis" : 0,
  "iterateMillis" : 11340,
  "rows" : 355,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Business Technology & Trust'='' OR t1 = 'Business Technology & Trust')\nORDER BY date, teamId, project, t0, t1 ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 238,
  "runStartToQueryComplete" : 1469
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 10847,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001652_00164_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "with cjs as (\nSELECT \n    dcr.job_hash,\n    sum(cast(dcr.numhiresMade as int)) as numhiresMade,\n    sum(cast(dcr.numHiresMadeIndeed as int)) as numHiresMadeIndeed,\n    sum(cast(dcr.numHiresMadeExternal as int)) as numHiresMadeExternal\nFROM datalake.imhotep.dradisClosedJobReason as dcr\nWHERE dcr.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-04-01') AND IMHOTEP_UNIXTIME('2024-10-24')\n\tand dcr.numhiresMade> '0'\n    and closedJobReason = 'notIndeed'\ngroup by 1\n)\n,job_id_job_hash_mapping as (\nSELECT \n\tjobhash_underscore, max(agg_job_id) as agg_job_id\nFROM datalake.imhotep.job_id_job_hash_mapping\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-23') AND IMHOTEP_UNIXTIME('2024-10-24')\n\tand jobhash_underscore in (select distinct job_hash from cjs)\ngroup by 1 \n)\n,jam as (\nselect \n\tagg_job_id,\n    max(job_product) as job_product,\n    max(job_country_code) as job_country_code,\n    sum(clicks) as clicks,\n    sum(apply_starts) as apply_starts,\n    sum(applies) as applies\nfrom datalake.imhotep.jobactivitymetrics\nwhere day between '2022-04-01' and 'today'\t\n\tand agg_job_id in (select distinct agg_job_id from job_id_job_hash_mapping)\ngroup by 1 \n)\n\n,view as (\nselect \n\tjob_id_job_hash_mapping.agg_job_id,\n    coalesce(applies,0) as applies,\n    coalesce(apply_starts,0) as apply_starts,\n    coalesce(clicks,0) as clicks,\n    numhiresMade,\n    numHiresMadeIndeed,\n    numHiresMadeExternal,\n    job_product,\n    job_country_code\nfrom cjs \njoin job_id_job_hash_mapping on job_id_job_hash_mapping.jobhash_underscore = cjs.job_hash\nleft join jam\n\ton job_id_job_hash_mapping.agg_job_id = jam.agg_job_id\n)\n\nselect \n   case when hs.jobid is not null then 1 else 0 end as reported_internally, \n   sum(view.numhiresMade) as hires,\n   1.00*count(distinct case when view.applies>0 then view.agg_job_id end)/count(distinct view.agg_job_id) as pct_job_w_apply,\n   1.00*count(distinct case when view.applies<10 then view.agg_job_id end)/count(distinct view.agg_job_id) as pct_job_w_lessthan10applies,\n   avg(view.applies) as avg_applies,\n   avg(view.apply_starts) as avg_applystarts,\n   avg(view.clicks) as avg_clicks\nfrom view \nleft join datalake.imhotep.hiredsignal hs \n\ton hs.unixtime BETWEEN IMHOTEP_UNIXTIME('2022-04-01') AND IMHOTEP_UNIXTIME('2024-10-24') \n    and view.agg_job_id = hs.jobid \ngroup by 1",
  "queryTables" : [ "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.hiredsignal", "datalakehive.imhotep.job_id_job_hash_mapping", "datalakehive.imhotep.job_id_job_hash_mapping", "datalakehive.imhotep.jobactivitymetrics" ],
  "queryIndex" : 239,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 219351,
  "totalScheduledMillis" : 99171904,
  "cpuMillis" : 8181207,
  "queuedMillis" : 0,
  "executeMillis" : 78751,
  "getResultMillis" : 0,
  "iterateMillis" : 140650,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 241432614672,
  "query" : "WITH experiment_data AS (\n\tSELECT \n    \tCASE WHEN ARRAYS_OVERLAP(ndxgrp, \n            ARRAY['#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275']) \n            THEN 'test' ELSE 'control' END AS _group, \n        q, jobid\n    FROM organic -- TABLESAMPLE BERNOULLI (10) -- sampling rate\n    WHERE \n    \tunixtime >= imhotep_unixtime('2024-10-15') AND unixtime < imhotep_unixtime('2024-11-12') -- test start date & end date\n        AND country = 'ca'\n        AND ARRAYS_OVERLAP(ndxgrp, \n        \tARRAY['#b15:idxmatchingcontrolplane_e274','#B15:idxmatchingcontrolplane_e274',\n            \t  '#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275'])\n)\n\nSELECT \n\t_group, \n    COUNT_IF(answer='No') as bad_matches, \n    COUNT() AS all_labels, \n    COUNT_IF(answer='No') / CAST(COUNT() AS DOUBLE) AS BMR\nFROM jobtoqueryrelevance\nINNER JOIN experiment_data ON\n\tjobtoqueryrelevance.query = experiment_data.q\n    AND jobtoqueryrelevance.jobid = experiment_data.jobid\nWHERE \n\tjobtoqueryrelevance.unixtime >= imhotep_unixtime('2024-01-01') AND jobtoqueryrelevance.unixtime < imhotep_unixtime('2024-11-12')\n\tAND jobtoqueryrelevance.country='CA'\nGROUP BY _group",
  "queryTables" : [ "skipperhive.imhotep.jobtoqueryrelevance", "skipperhive.imhotep.organic" ],
  "queryIndex" : 240,
  "runStartToQueryComplete" : 1675
}, {
  "elapsedMillis" : 3785,
  "totalScheduledMillis" : 243680,
  "cpuMillis" : 24066,
  "queuedMillis" : 0,
  "executeMillis" : 1695,
  "getResultMillis" : 0,
  "iterateMillis" : 2110,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 2427995627,
  "query" : "        SELECT \n            *\n        FROM imhotep.passdailysnapshot\n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND verified != 1\n            and accountid = 1228375881",
  "queryTables" : [ "skipperhive.imhotep.passdailysnapshot" ],
  "queryIndex" : 241,
  "runStartToQueryComplete" : 1460
}, {
  "elapsedMillis" : 477385,
  "totalScheduledMillis" : 427078982,
  "cpuMillis" : 145383275,
  "queuedMillis" : 0,
  "executeMillis" : 9085,
  "getResultMillis" : 0,
  "iterateMillis" : 468325,
  "rows" : 500,
  "error" : null,
  "scannedBytes" : 9684559004954,
  "query" : "with examples as (\nSELECT\n\t\tmoderationtext,\n        flagged,\n        flaggedcategories,\n        flaggedwords,\n \t\tappName,\n        moderationApi, \n        moderationType, \n        FROM_UNIXTIME(unixtime) as request_date,\n        requestid,\n        reqbody,\n        respbody\nFROM\n       logrepo.log.llm_proxy lp\n    WHERE\n    \tflagged = 1\n        and lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and ('ContentGenerationService' = '' or appName='ContentGenerationService')\n        and ('openAIModerations' = '' or moderationApi='openAIModerations')\n        and ('input' = '' or moderationType='input')\nlimit 500\n)\n\nSELECT\n        m.*,\n        r.model,\n        r.moderationHeader,\n        r.locale        \nFROM logrepo.log.llm_proxy r\n        join examples m on m.requestid = r.requestid -- switching to imhotep for speed even though there is a 1D data lag\n    WHERE\n        r.unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and r.metric = 'llm-proxy'",
  "queryTables" : [ "logrepo.log.llm_proxy", "logrepo.log.llm_proxy" ],
  "queryIndex" : 242,
  "runStartToQueryComplete" : 1938
}, {
  "elapsedMillis" : 10893,
  "totalScheduledMillis" : 654546,
  "cpuMillis" : 71689,
  "queuedMillis" : 0,
  "executeMillis" : 3196,
  "getResultMillis" : 0,
  "iterateMillis" : 7736,
  "rows" : 7,
  "error" : null,
  "scannedBytes" : 3821617499,
  "query" : "SELECT\n    DATE_TRUNC('month', activity_time) AS activity_month,\n    is_ai_outreach,\n    SUM(contact_sent) AS contact_count,\n    SUM(positive_response) AS positive_responses_count,\n    (SUM(positive_response) * 1.00) / SUM(contact_sent) AS pos_resp_rate\nFROM\n    datalake.employer_analytics_platform.xpa_fct_sourcing_engagement\nWHERE \n\tactivity_time >= DATE('2024-08-01')\n    --AND subscription_tier IN ('Standard')\n    AND contact_sent > 0\n    AND advertiser_id = 16775855\nGROUP BY 1, 2\nORDER BY 1, 2",
  "queryTables" : [ "datalakehive.employer_analytics_platform.xpa_fct_sourcing_engagement" ],
  "queryIndex" : 243,
  "runStartToQueryComplete" : 1474
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 579,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001702_00170_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "SELECT min(day) FROM jssdi.profile_snapshot WHERE day >= '2020-01-01'",
  "queryTables" : [ "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 244,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 3191,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001702_00169_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "SELECT \n\tELEMENT_AT(data, 'rum_context_element') as element,\n    ELEMENT_AT(data, 'rum_context_module') as module\nFROM \n\tlogrepo.raw_log.one_host_rum_action\nWHERE \n\tunixtime > IMHOTEP_UNIXTIME('yesterday') AND\n    ELEMENT_AT(data, 'rum_view_url') LIKE '%?%' AND\n    ELEMENT_AT(data, 'rum_view_name') = '/jobs' AND\n    ELEMENT_AT(data, 'rum_context_scope') = 'jobman-feature-modules' AND\n    ELEMENT_AT(data, 'rum_context_customActionType') = 'click' AND\n    ELEMENT_AT(data, 'rum_context_additional_searchTkUuid') IS NOT NULL AND\n    ELEMENT_AT(data, 'rum_context_additional_clickDepthIndex') IS NOT NULL AND\n    ELEMENT_AT(data, 'rum_context_additional_clickDepthPage') IS NOT NULL\n--    CARDINALITY(SPLIT(ELEMENT_AT(data, 'rum_context_additional_clickDepthIndex'), ',')) > 1 AND\n--    CARDINALITY(SPLIT(ELEMENT_AT(data, 'rum_context_additional_clickDepthPage'), ',')) > 1\nGROUP BY\n\tELEMENT_AT(data, 'rum_context_element'),\n    ELEMENT_AT(data, 'rum_context_module')",
  "queryTables" : [ "logrepo.raw_log.one_host_rum_action" ],
  "queryIndex" : 245,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 17455,
  "totalScheduledMillis" : 2352599,
  "cpuMillis" : 666800,
  "queuedMillis" : 0,
  "executeMillis" : 2555,
  "getResultMillis" : 0,
  "iterateMillis" : 14931,
  "rows" : 1999,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    ('Marketplace'='' OR t1 = 'Marketplace')\nORDER BY date, teamId, project, t0, t1 ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 246,
  "runStartToQueryComplete" : 1501
}, {
  "elapsedMillis" : 163112,
  "totalScheduledMillis" : 54770883,
  "cpuMillis" : 9026529,
  "queuedMillis" : 0,
  "executeMillis" : 1402,
  "getResultMillis" : 0,
  "iterateMillis" : 161760,
  "rows" : 246,
  "error" : null,
  "scannedBytes" : 22946178822,
  "query" : "-- Secondary metric:  number of unique jobs identified as present in a repost cluster (including both copy and original in the cluster)\n\nwith\ntruth_copies as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           date_trunc('day', from_unixtime(jobCopyCreateDate/1000)) as day,\n           jobCopyJobId,\n           count(*) as c\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   modelId='Legacy RepostDetector'\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n    group by 1,2,3 ),\ntruth_originals as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           date_trunc('day', from_unixtime(jobCopyCreateDate/1000)) as day,\n           originalJobId,\n           count(*) as c\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   modelId='Legacy RepostDetector'\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n    group by 1,2,3 ),\ntruth_jobs as\n(   select model, day, jobCopyJobId as jobId from truth_copies\n    union\n    select model, day, originalJobId as jobId from truth_originals ),\nmodel_copies as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           date_trunc('day', from_unixtime(jobCopyCreateDate/1000)) as day,\n           jobCopyJobId as jobId,\n           count(*) as c\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   modelId='legacyRepostModel-2.2'\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n    group by 1,2,3 ),\nmodel_originals as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           date_trunc('day', from_unixtime(jobCopyCreateDate/1000)) as day,\n           originalJobId as jobId,\n           count(*) as c\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   modelId='legacyRepostModel-2.2'\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n    group by 1,2,3 ),\nmodel_jobs as\n(   select model, day, jobId from model_copies\n    union\n    select model, day, jobId from model_originals ),\nmatches as\n(   select coalesce(mj.day, tj.day) as day,\n           tj.jobId is not null as truth_match,\n           mj.jobId is not null as model_match\n\tfrom truth_jobs tj\n    full outer join model_jobs mj on (    mj.model != tj.model\n                                      and mj.day = tj.day\n                                      and mj.jobId = tj.jobId)\n),\nvenn as\n(   select day,\n           truth_match,\n           model_match,\n           count(*) as count\n    from matches\n    group by 1,2,3 ),\nintersection_count as\n(   select day,\n           count\n    from venn\n    where truth_match\n    and   model_match\n),\nunion_count as\n(   select day,\n           sum(count) as count\n    from venn -- ignoring truth_match and model_match fields\n    group by 1 )\nselect ic.day,\n       ic.count as intersection_count,\n       uc.count as union_count,\n       cast(ic.count as double) / cast(uc.count as double) as jaccard\nfrom intersection_count ic,\n     union_count uc\nwhere ic.day = uc.day",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 247,
  "runStartToQueryComplete" : 1648
}, {
  "elapsedMillis" : 54462,
  "totalScheduledMillis" : 13837890,
  "cpuMillis" : 3165400,
  "queuedMillis" : 0,
  "executeMillis" : 2372,
  "getResultMillis" : 0,
  "iterateMillis" : 52129,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 15295099999,
  "query" : "-- once a copy is assigned to an original, it is never reassigned to a different original\n\nwith\ntoday as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           concat(radArtifactName, '-', modelId) as artifactModel,\n           jobCopyJobId,\n           originalJobId\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n),\nyesterday as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           concat(radArtifactName, '-', modelId) as artifactModel,\n           jobCopyJobId,\n           originalJobId\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') - 24*60*60 <= unixtime and unixtime < imhotep_unixtime('1d')\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n)\nselect y.model,\n       y.jobCopyJobId,\n       y.originalJobId as yesterdayOriginal,\n       t.originalJobId as todayOriginal\nfrom yesterday y\njoin today t on (    t.artifactModel = y.artifactModel\n                 and t.jobCopyJobId = y.jobCopyJobId\n                 and t.originalJobId != y.originalJobId)",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 248,
  "runStartToQueryComplete" : 1539
}, {
  "elapsedMillis" : 401206,
  "totalScheduledMillis" : 10359609,
  "cpuMillis" : 3653408,
  "queuedMillis" : 0,
  "executeMillis" : 5692,
  "getResultMillis" : 0,
  "iterateMillis" : 395526,
  "rows" : 8,
  "error" : null,
  "scannedBytes" : 4426796,
  "query" : "SELECT password\nFROM logrepo.log.rozNavTiming\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1000d') AND IMHOTEP_UNIXTIME('1d') \nGROUP BY password\nLIMIT 10",
  "queryTables" : [ "logrepo.log.roznavtiming" ],
  "queryIndex" : 249,
  "runStartToQueryComplete" : 1890
}, {
  "elapsedMillis" : 33288,
  "totalScheduledMillis" : 3451090,
  "cpuMillis" : 848760,
  "queuedMillis" : 0,
  "executeMillis" : 1886,
  "getResultMillis" : 0,
  "iterateMillis" : 31438,
  "rows" : 9253,
  "error" : null,
  "scannedBytes" : 5680842401,
  "query" : "-- do create dates match between truth and compare?\n\nwith\ntruth as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           jobCopyJobId,\n           jobCopyCreateDate,\n           originalJobId,\n           originalJobCreateDate\n    from datalake.imhotep.duplicationDetectionRadContents legacy\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   modelId = 'Legacy RepostDetector'\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n),\ncompare as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           jobCopyJobId,\n           jobCopyCreateDate,\n           originalJobId,\n           originalJobCreateDate\n    from datalake.imhotep.duplicationDetectionRadContents legacy\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   modelId = 'legacyRepostModel-2.2'\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n)\nselect t.jobCopyJobId jobId,\n       t.model as truthModel,\n       t.jobCopyCreateDate as truthJobCreate,\n       c.model as compareModel,\n       c.jobCopyCreateDate as compareJobCreate,\n       date_diff('hour', from_unixtime(t.jobCopyCreateDate/1000), from_unixtime(c.jobCopyCreateDate/1000))/24.0 as diffDays\nfrom truth t,\n     compare c\nwhere t.jobCopyJobId = c.jobCopyJobId\nand   t.jobCopyCreateDate != c.jobCopyCreateDate",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 250,
  "runStartToQueryComplete" : 1526
}, {
  "elapsedMillis" : 18807,
  "totalScheduledMillis" : 1806631,
  "cpuMillis" : 440650,
  "queuedMillis" : 0,
  "executeMillis" : 899,
  "getResultMillis" : 0,
  "iterateMillis" : 17927,
  "rows" : 4,
  "error" : null,
  "scannedBytes" : 1752045432,
  "query" : "-- the artifact contains only copies which were. created <= 245 days from artifact creation\n\n-- WARNING: this will only produce correct results if clusters are defined only by root and leaves (no deeper structures)\n\nwith\ncopy_ages as\n(   select distinct concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n                    date_diff('day', from_unixtime(jobCopyCreateDate / 1000), from_unixtime(imhotep_unixtime('1d'))) as copyAge--,\n--                    date_diff('day', from_unixtime(originalJobCreateDate / 1000), from_unixtime(imhotep_unixtime('1d'))) as originalAge\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n)\nselect model,\n       approx_percentile(copyAge, array [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99, 0.999, 1]) clusterDaySpans\nfrom copy_ages\ngroup by 1",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 251,
  "runStartToQueryComplete" : 1516
}, {
  "elapsedMillis" : 154924,
  "totalScheduledMillis" : 43630495,
  "cpuMillis" : 7781642,
  "queuedMillis" : 0,
  "executeMillis" : 1008,
  "getResultMillis" : 0,
  "iterateMillis" : 153960,
  "rows" : 4,
  "error" : null,
  "scannedBytes" : 21461320076,
  "query" : "-- if we know the structure of the clusters within the data are simple, only leaves and root (no intermediate nodes)\n-- then we can analyze clusters in SQL without having to traverse graphs (in code)\n\n-- lets exclude cycles (of length 2) here, they are listed in a separate invariant\n\nwith\nmappings as\n(   select distinct\n           concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           jobCopyJobId,\n           jobCopyCreateDate,\n           originalJobId,\n           originalJobCreateDate\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n),\nincident_edges as\n(   select distinct\n           e1.model,\n           e1.jobCopyJobId as jobId,\n           e1.jobCopyCreateDate as createDate,\n           (e1.jobCopyCreateDate = e2.originalJobCreateDate) as datesMatch\n    from mappings e1,\n         mappings e2\n    where e1.model = e2.model\n    and   e1.jobCopyJobId = e2.originalJobId\n    and   e1.originalJobId != e2.jobCopyJobId -- exclude length 2 cycles\n)\nselect model, \n       count(*)\nfrom incident_edges\ngroup by 1",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 252,
  "runStartToQueryComplete" : 1654
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 10728,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001737_00178_mzvde): Division by zero",
  "scannedBytes" : 0,
  "query" : "with \n-- Pull all conversations initiated by employers \ndremrToConvs as (\nselect  unixtime, advertiserId,  eventId, conversationId\n  \t\t, case when starts_with(subEventType, 'INBOUND_EMAIL') then 1 else 0 end as is_email\n        , case when starts_with(source, 'Dremr')  then 1 else 0 end as is_Dremr  \n  from conversationEvents \nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n  and context = 'APPLICATION'\n  and ownerRole != 'JOBSEEKER'\n  and cast(isInitialMessage as int)=1 -- Look at initial messages only, as a proxy for when a conversation is created\ngroup by 1,2,3,4,5,6\n\n              )\n              \n-- Employers with any ATS Integration\n,atsEmployers as (\n\nselect advertiserId  \n  from partnerEmployerMap\n where unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n   and confidenceScore >0\ngroup by  1 \n\n                )\n\n-- advertisers that have had at least 1 candidate delivered to the ATS with a DREMR email\n, atsSync as (\n\nselect advertiser_id, connection_type as ats \n      , agg_job_id\n      ,concat('DRADIS/', cast(advertiser_id as varchar), '-', cast(dradis_candidate_id as varchar)) as conversationId\n\n  from acdcTransferLifecycle\n where unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n   and pipeline_internal_type='INTERNAL_LOCATION_TYPE_EMPLOYER_JOB'\n   and connection_type!= 'h2ia'\n   and driver_type not in ('pipeline-csul', 'pipeline-csul-historical')\ngroup by 1,2,3,4\n            \n            )\n -- Dradis bulk export           \n,dradisBulkExport as (    \t\n\n        SELECT advertiserId\n\t\t\t  ,concat('DRADIS/', cast(advertiserId as varchar), '-', cast(candidateId as varchar)) as conversationId\n        FROM datalake.imhotep.dradisCandidateExportRequest\n        CROSS JOIN UNNEST(split(candidateIds, ',')) as t (candidateId)\n        WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('56d') and IMHOTEP_UNIXTIME('28d')\n       group by 1,2\n  )     \n\n\n-- advertiser segment details\n,advertiserDetails as ( \n\nselect advertiser_id\n   , max_by(parent_company_size_segment, unixtime) as parent_company_size_segment\n   , max_by(type, unixtime) as type\n   , max_by(billing_country, unixtime) as billing_country\n  from  daily_employer2 \n  where unixtime BETWEEN IMHOTEP_UNIXTIME('29d') and IMHOTEP_UNIXTIME('28d')\ngroup by 1\n\n   )\n\n\n -- Aggregate convo counts per query\n, empDataAgg as (\n\n\n select \n\tdremrToConvs.advertiserId\n   ,case when atsEmployers.advertiserId is not null then 1 else 0 end as isAtsIntegrated   \n   ,count(*) as allConvos\n   ,count_if(is_Dremr=1) as dremrConvos\n   --,count_if(is_Dremr=1 and is_email=1) as dremrEmailConvos\n   ,count_if(is_Dremr=1 and atsSync.conversationId is not null) as dremrAtsSyncAll\n   ,count_if(is_Dremr=1 and dradisBulkExport.conversationId is not null) as dremrDradisBulkExport\n   ,count_if(is_Dremr=1 and (atsSync.conversationId is not null or dradisBulkExport.conversationId is not null)) as dremrAtsSyncOrBulkExport\n   \n   \n  from dremrToConvs \n  left join atsEmployers on dremrToConvs.advertiserId=atsEmployers.advertiserId\n  left join atsSync on dremrToConvs.advertiserId=atsSync.advertiser_id\n        and dremrToConvs.conversationId=atsSync.conversationId\n  left join dradisBulkExport on dremrToConvs.advertiserId=dradisBulkExport.advertiserId\n        and dremrToConvs.conversationId=dradisBulkExport.conversationId\n    \n  group by 1,2\n                )\n                \n \n,segConvos as (\nselect\n    ce.advertiserId\n   ,eventId\n   ,ce.encryptedConversationId\n   ,(isImported = '1') as isEmail\nFROM conversationEvents ce\njoin convsConversationCreation cc on  ce.encryptedConversationId = cc.encryptedConversationId \n\nWHERE ce.unixtime BETWEEN IMHOTEP_UNIXTIME('209d') AND IMHOTEP_UNIXTIME('28d')\nand   cc.unixtime BETWEEN IMHOTEP_UNIXTIME('223d') AND IMHOTEP_UNIXTIME('28d')\nand ((ce.unixtime - cc.unixtime) < (60*60*24*14))\nand ce.context='APPLICATION'\nand ce.ownerRole != 'JOBSEEKER'\nGROUP BY 1,2,3,4\n)\n\n\n, segConvosAgg as (\n\nselect advertiserId\n      ,count(*) as totalMessages\n      ,count_if(isEmail) as emailMessages\n      ,count_if(NOT isEmail) as onPlatformMessages \n  from segConvos\ngroup by 1 \n\n\t\t)\n\n,segConvosAgg2 as (\n\n select advertiserId, case when emailMessages> 0 and onPlatformMessages=0 then 'Email Only'\n\t\t\t when  emailMessages=0 and onPlatformMessages>0 then 'Chat Only'\n\t\t\t when  emailMessages>0 and onPlatformMessages>0 then 'Email & Chat'\n             end as usageType\n\n  from segConvosAgg\n                 )\n\n\nselect \n        segConvosAgg2.usageType\n       ,count() as empCount\n       , 100.0*count()/sum(count()) over () pctEmpCountOfAll\n    --  ,count_if(isAtsIntegrated=1) as empCountAtsIntegrated\n      ,100.00* count_if(isAtsIntegrated=1)/count(*) as pctEmpAtsIntegrated\n      ,sum(allConvos) as ttlConvosInitiated\n\t  ,sum(DremrConvos) as DremrConvosInitiated\n  --    ,sum(dremrConvos) as ttlDremrConvosInitiated\n      ,100.00* sum(dremrConvos)/sum(allConvos) as dremrConvosPctOfAllInitiated\n      \n      ,100.00* sum(dremrAtsSyncAll)/sum(dremrConvos) as pctOfDremrConvosAtsSyncAll\n      ,100.00* sum(dremrDradisBulkExport)/sum(dremrConvos) as pctOfDremrConvosDradisBulkExport\n      ,100.00* sum(dremrAtsSyncOrBulkExport)/sum(dremrConvos) as pctOfDremrConvosAtsSyncOrBulkExport\n     \n from empDataAgg\nleft join advertiserDetails on empDataAgg.advertiserId=advertiserDetails.advertiser_id\nleft join segConvosAgg2     on empDataAgg.advertiserId=segConvosAgg2.advertiserId\n    where  parent_company_size_segment in ('L', 'XL') and usageType is not null\ngroup by 1\n                \n\n                \n\n              \n              \n              ",
  "queryTables" : [ "datalakehive.imhotep.dradiscandidateexportrequest", "skipperhive.imhotep.acdctransferlifecycle", "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.conversationevents", "skipperhive.imhotep.convsconversationcreation", "skipperhive.imhotep.daily_employer2", "skipperhive.imhotep.partneremployermap" ],
  "queryIndex" : 253,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 3420,
  "totalScheduledMillis" : 182660,
  "cpuMillis" : 12601,
  "queuedMillis" : 0,
  "executeMillis" : 1129,
  "getResultMillis" : 0,
  "iterateMillis" : 2306,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 252110395,
  "query" : "select count(1) from clickanalytics where unixtime between imhotep_unixtime('2019-03-01') and imhotep_unixtime('2019-03-02')",
  "queryTables" : [ "skipperhive.imhotep.clickanalytics" ],
  "queryIndex" : 254,
  "runStartToQueryComplete" : 1506
}, {
  "elapsedMillis" : 59249,
  "totalScheduledMillis" : 9745218,
  "cpuMillis" : 2031390,
  "queuedMillis" : 0,
  "executeMillis" : 810,
  "getResultMillis" : 0,
  "iterateMillis" : 58477,
  "rows" : 4,
  "error" : null,
  "scannedBytes" : 7712780411,
  "query" : "-- copies are assigned originals which were created between 0 and 90 days prior\n\n-- WARNING: this will only produce correct results if clusters are defined only by root and leaves (no deeper structures)\n\nwith\ncluster_day_spans as\n(   select distinct concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n                    originalJobId as clusterId,\n                    date_diff('day', from_unixtime(originalJobCreateDate / 1000), from_unixtime(jobCopyCreateDate / 1000)) as daySpan\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n)\nselect model,\n       approx_percentile(daySpan, array [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99, 0.999, 1]) clusterDaySpans\nfrom cluster_day_spans\ngroup by 1",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 255,
  "runStartToQueryComplete" : 1564
}, {
  "elapsedMillis" : 113610,
  "totalScheduledMillis" : 30878570,
  "cpuMillis" : 3603580,
  "queuedMillis" : 0,
  "executeMillis" : 2105,
  "getResultMillis" : 0,
  "iterateMillis" : 111531,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 363427038615,
  "query" : "WITH\n\tunverified_accounts AS (\n        SELECT \n            accountid\n            , verified\n        FROM imhotep.passdailysnapshot\n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND verified != 1\n    ), profiles AS (\n        SELECT \n            acctid AS accountid\n            , resumes\n            , fileinformation\n        FROM jssdi.profile_snapshot \n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND cardinality(\n            \tFILTER(\n                \tfileinformation\n                    , f -> f.type = 'RESUME'\n                )\n            ) > 1\n            -- none resume or single resume in PARSED or IR type\n            AND (\n            \tresumes is null\n                OR ( CARDINALITY(resumes) = 1 and resumes[1].resumetype in ('PARSED', 'INDEED_RESUME')\n                )\n\t)\n   )\n    \n    SELECT\n    p.accountid\n    , '<batch_name>_xxx'\n--    , a.verified\n--    , CARDINALITY(resumes) as rez_count\n--    , if (resumes = null, null, resumes[1].resumetype) as rez_type\n--    , cardinality(\n--            \tFILTER(\n--                \tfileinformation\n--                    , f -> f.type = 'RESUME'\n--                )\n--            ) as rez_file_count\n    from profiles p\n    join unverified_accounts a\n    on p.accountid = a.accountid\n --   limit 100",
  "queryTables" : [ "skipperhive.imhotep.passdailysnapshot", "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 256,
  "runStartToQueryComplete" : 1619
}, {
  "elapsedMillis" : 304063,
  "totalScheduledMillis" : 179720,
  "cpuMillis" : 28419,
  "queuedMillis" : 0,
  "executeMillis" : 675,
  "getResultMillis" : 0,
  "iterateMillis" : 303462,
  "rows" : 11374193,
  "error" : null,
  "scannedBytes" : 1079745280,
  "query" : "select *\nfrom datalake.tiller.oplin_events_start\nwhere ingest_time > date '2024-10-29'",
  "queryTables" : [ "datalake.tiller.oplin_events_start" ],
  "queryIndex" : 257,
  "runStartToQueryComplete" : 1810
}, {
  "elapsedMillis" : 92161,
  "totalScheduledMillis" : 22361350,
  "cpuMillis" : 3536567,
  "queuedMillis" : 0,
  "executeMillis" : 1084,
  "getResultMillis" : 0,
  "iterateMillis" : 91110,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 363422586652,
  "query" : "WITH\n\tunverified_accounts AS (\n        SELECT \n            accountid\n            , verified\n        FROM imhotep.passdailysnapshot\n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND verified != 1\n    ), profiles AS (\n        SELECT \n            acctid AS accountid\n            , resumes\n            , fileinformation\n        FROM jssdi.profile_snapshot \n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND cardinality(\n            \tFILTER(\n                \tfileinformation\n                    , f -> f.type = 'RESUME'\n                )\n            ) > 1\n            -- none resume or single resume in PARSED or IR type\n            AND (\n            \tresumes is null\n                OR ( CARDINALITY(resumes) = 1 and resumes[1].resumetype in ('PARSED', 'INDEED_RESUME')\n                )\n\t)\n   )\n    \n    SELECT\n    count(distinct p.accountid)\n    from profiles p\n    join unverified_accounts a\n    on p.accountid = a.accountid\n    limit 100",
  "queryTables" : [ "skipperhive.imhotep.passdailysnapshot", "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 258,
  "runStartToQueryComplete" : 1606
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 981,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001752_00184_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "-- Secondary metric:  jaccard similarity of clusters (when clusters can \"match\" even when repostIds are different -- a relaxation of the primary metric)\n\n-- WARNING: this will only produce correct results if clusters are defined only by root and leaves (no deeper structures)\n\nwith\n--lets try to collect the clusters into sets, a special case for adding the original into the set also\ncluster_job_array_prep as\n(   select modelId,\n           concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           originalJobId as originalRepostId,\n           array_distinct(array_agg(jobCopyJobId)) as copyJobIds\n    from datalake.imhotep.duplicationDetectionRadContents -- datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and modelId in ('Legacy RepostDetector', 'legacyRepostModel-2.2')\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n    group by 1,2,3\n)\n--select modelId, count(*) from cluster_job_array_prep group by 1 -- 39,329,833\n--select count(*) from cluster_job_array_prep where array_position(copyJobIds, originalRepostId) = 0 -- 39,329,833\n--select count(distinct originalRepostId) from cluster_job_array_prep -- 39,329,833\n--select * from cluster_job_array_prep limit 10\n,\n--/*\ntruth_cluster_job_arrays as\n(   select model,\n           originalRepostId,\n           array_min(array_union(copyJobIds, array [originalRepostId])) as newRepostId,\n           array_sort(array_distinct(array_union(copyJobIds, array [originalRepostId]))) as jobIds\n    from cluster_job_array_prep\n    where modelId='Legacy RepostDetector'\n)\n--select model, count(*) from truth_cluster_job_arrays group by 1 -- 39,329,833\n--select count(distinct originalRepostId) from truth_cluster_job_arrays -- 39,329,833\n--select count(distinct newRepostId) from truth_cluster_job_arrays -- 39,329,832 -- ??? why one less?\n--select originalRepostId = newRepostId, count(*) from truth_cluster_job_arrays group by 1 -- 39,328,980 + 853 = 39,329,833\n--select * from truth_cluster_job_arrays limit 10\n,\n--/*\ncompare_cluster_job_arrays as\n(   select model,\n           originalRepostId,\n           array_min(array_union(copyJobIds, array [originalRepostId])) as newRepostId,\n           array_sort(array_distinct(array_union(copyJobIds, array [originalRepostId]))) as jobIds\n    from cluster_job_array_prep\n    where modelId='legacyRepostModel-2.2'\n)\n--select model, count(*) from compare_cluster_job_arrays group by 1 -- 39,329,833\n--select count(distinct originalRepostId) from compare_cluster_job_arrays -- 39,329,833\n--select count(distinct newRepostId) from compare_cluster_job_arrays -- 39,329,832 -- ??? why one less?\n--select originalRepostId = newRepostId, count(*) from compare_cluster_job_arrays group by 1 -- 39,328,980 + 853 = 39,329,833\n--select * from compare_cluster_job_arrays limit 10\n,\n--/*\nmatches as\n(   select tcja.newRepostId is not null as truth_match,\n           ccja.newRepostId is not null as compare_match\n    from            truth_cluster_job_arrays tcja\n    full outer join compare_cluster_job_arrays ccja on (    tcja.newRepostId = ccja.newRepostId\n                                                        and tcja.jobIds = ccja.jobIds)\n)\n,\n--select count(*) from matches -- 49,442,531\n--select count(distinct newRepostId) from matches -- 39,329,832\n--select truth_match, compare_match, count(*) from matches group by 1,2\n--select * from matches limit 10\n--/*\nintersection_count as\n(   select count(*) as count\n    from matches\n    where truth_match\n    and   compare_match\n)\n--select * from intersection_count limit 10\n,\n--/*\nunion_count as\n(   select count(*) as count\n    from matches\n)\n--select * from union_count limit 10\n--/*\nselect ic.count as intersection_count,\n       uc.count as union_count,\n       cast(ic.count as double) / cast(uc.count as double) as jaccard\nfrom intersection_count ic,\n     union_count uc\n/*\n*/\n",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 259,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 29092,
  "totalScheduledMillis" : 13568910,
  "cpuMillis" : 2495630,
  "queuedMillis" : 0,
  "executeMillis" : 1761,
  "getResultMillis" : 0,
  "iterateMillis" : 27353,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 4773639941,
  "query" : "-- a copy can be assigned at most one original\n\nselect concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n       jobCopyJobId,\n       count(*)\nfrom datalake.imhotep.duplicationDetectionRadContents\nwhere imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\nand   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\ngroup by 1,2\nhaving count(*) > 1",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 260,
  "runStartToQueryComplete" : 1545
}, {
  "elapsedMillis" : 36298,
  "totalScheduledMillis" : 6781275,
  "cpuMillis" : 1279663,
  "queuedMillis" : 1,
  "executeMillis" : 833,
  "getResultMillis" : 0,
  "iterateMillis" : 35497,
  "rows" : 1000,
  "error" : null,
  "scannedBytes" : 9752119231,
  "query" : "-- do create dates match between truth and compare?\n\nwith\ntruth as\n(   select distinct\n           concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           originalJobId,\n           originalJobCreateDate\n    from datalake.imhotep.duplicationDetectionRadContents legacy\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   modelId = 'Legacy RepostDetector'\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n),\ncompare as\n(   select distinct\n           concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           originalJobId,\n           originalJobCreateDate\n    from datalake.imhotep.duplicationDetectionRadContents legacy\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   modelId = 'legacyRepostModel-2.2'\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n)\nselect t.originalJobId jobId,\n       t.model as truthModel,\n       t.originalJobCreateDate as truthJobCreate,\n       c.model as compareModel,\n       c.originalJobCreateDate as compareJobCreate,\n       date_diff('hour', from_unixtime(t.originalJobCreateDate/1000), from_unixtime(c.originalJobCreateDate/1000))/24.0 as diffDays\nfrom truth t,\n     compare c\nwhere t.originalJobId = c.originalJobId\nand   t.originalJobCreateDate != c.originalJobCreateDate\nand   date_diff('hour', from_unixtime(t.originalJobCreateDate/1000), from_unixtime(c.originalJobCreateDate/1000))/24.0 > 0\norder by diffDays asc\nlimit 1000",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 261,
  "runStartToQueryComplete" : 1558
}, {
  "elapsedMillis" : 19799,
  "totalScheduledMillis" : 1698347,
  "cpuMillis" : 434822,
  "queuedMillis" : 0,
  "executeMillis" : 825,
  "getResultMillis" : 0,
  "iterateMillis" : 18996,
  "rows" : 4,
  "error" : null,
  "scannedBytes" : 4843781918,
  "query" : "-- the artifact contains only copies which were. created <= 245 days from artifact creation\n\n-- WARNING: this will only produce correct results if clusters are defined only by root and leaves (no deeper structures)\n\nwith\ncopy_ages as\n(   select distinct concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n                    date_diff('day', from_unixtime(originalJobCreateDate / 1000), from_unixtime(imhotep_unixtime('1d'))) as originalAge\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n)\nselect model,\n       approx_percentile(originalAge, array [0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99, 0.999, 1]) clusterDaySpans\nfrom copy_ages\ngroup by 1",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 262,
  "runStartToQueryComplete" : 1545
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 859,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001804_00188_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "-- Secondary metric:  jaccard similarity of clusters (when clusters can \"match\" even when repostIds are different -- a relaxation of the primary metric)\n\n-- WARNING: this will only produce correct results if clusters are defined only by root and leaves (no deeper structures)\n\nwith\n--lets try to collect the clusters into sets, a special case for adding the original into the set also\ncluster_job_array_prep as\n(   select modelId,\n           concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           originalJobId as originalRepostId,\n           array_distinct(array_agg(jobCopyJobId)) as copyJobIds\n    from datalake.imhotep.duplicationDetectionRadContents -- datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and modelId in ('Legacy RepostDetector', 'legacyRepostModel-2.2')\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n    group by 1,2,3\n),\n--/*\ntruth_cluster_job_arrays as\n(   select model,\n           originalRepostId,\n           array_min(array_union(copyJobIds, array [originalRepostId])) as newRepostId,\n           array_sort(array_distinct(array_union(copyJobIds, array [originalRepostId]))) as jobIds\n    from cluster_job_array_prep\n    where modelId='Legacy RepostDetector'\n),\n--/*\ncompare_cluster_job_arrays as\n(   select model,\n           originalRepostId,\n           array_min(array_union(copyJobIds, array [originalRepostId])) as newRepostId,\n           array_sort(array_distinct(array_union(copyJobIds, array [originalRepostId]))) as jobIds\n    from cluster_job_array_prep\n    where modelId='legacyRepostModel-2.2'\n),\n--/*\njaccards as\n(   select 1.000 * cardinality(array_intersect(coalesce(tcja.jobIds, array []), coalesce(ccja.jobIds, array []))) /\n                 cardinality(array_union(coalesce(tcja.jobIds, array []), coalesce(ccja.jobIds, array []))) as jaccard,\n           cardinality(array_intersect(coalesce(tcja.jobIds, array []), coalesce(ccja.jobIds, array []))) as intersectSize,\n           cardinality(array_union(coalesce(tcja.jobIds, array []), coalesce(ccja.jobIds, array []))) as unionSize,\n           tcja.originalRepostId as truthRepostId,\n           ccja.originalRepostId as compareRepostId,\n           array_union(array_except(coalesce(tcja.jobIds, array []), coalesce(ccja.jobIds, array [])),\n                       array_except(coalesce(ccja.jobIds, array []), coalesce(tcja.jobIds, array []))) as symDiff,\n           tcja.jobIds as truthJobIds,\n           ccja.jobIds as compareJobIds\n    from            truth_cluster_job_arrays tcja\n    full outer join compare_cluster_job_arrays ccja on (    tcja.newRepostId = ccja.newRepostId)\n)\nselect *\nfrom jaccards\nwhere jaccard < 1.0 -- eliminate exact matches\nand   truthRepostId is not null\nand   compareRepostId is not null\norder by unionSize asc,\n         cardinality(symDiff) asc,\n         jaccard desc\nlimit 1000",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 263,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 80050,
  "totalScheduledMillis" : 20420337,
  "cpuMillis" : 3402093,
  "queuedMillis" : 0,
  "executeMillis" : 2151,
  "getResultMillis" : 0,
  "iterateMillis" : 77929,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 363128377031,
  "query" : "WITH\n\tdeleted_accounts AS (\n        SELECT \n            accountid\n            , deleted\n        FROM imhotep.passdailysnapshot\n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            -- deleted accounts\n            AND deleted = 1\n    ), profiles AS (\n        SELECT \n            acctid AS accountid\n            , resumes\n            , fileinformation\n        FROM jssdi.profile_snapshot \n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND cardinality(\n            \tFILTER(\n                \tfileinformation\n                    , f -> f.type = 'RESUME'\n                )\n            ) > 1\n            -- none resume or single resume in PARSED or IR type\n            AND (\n            \tresumes is null\n                OR ( CARDINALITY(resumes) = 1 and resumes[1].resumetype in ('PARSED', 'INDEED_RESUME')\n                )\n\t)\n   )\n    \n    SELECT\n    p.accountid\n    , '<batch_name>_xxx'\n    --, a.deleted\n    --, CARDINALITY(resumes) as rez_count\n    --, if (resumes = null, null, resumes[1].resumetype) as rez_type\n--    , cardinality(\n--            \tFILTER(\n--                \tfileinformation\n--                    , f -> f.type = 'RESUME'\n--                )\n--            ) as rez_file_count\n    from profiles p\n    join deleted_accounts a\n    on p.accountid = a.accountid\n    --limit 100",
  "queryTables" : [ "skipperhive.imhotep.passdailysnapshot", "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 264,
  "runStartToQueryComplete" : 1613
}, {
  "elapsedMillis" : 42586,
  "totalScheduledMillis" : 7087039,
  "cpuMillis" : 1685186,
  "queuedMillis" : 0,
  "executeMillis" : 777,
  "getResultMillis" : 0,
  "iterateMillis" : 41840,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 13573395576,
  "query" : "-- lets find all pairs of jobs where A is marked a repost of B, and ALSO B is marked a repost of A\n-- these are length 2 cycles (as opposed to deeper trees)\n\nwith\nmappings as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           jobCopyJobId,\n           jobCopyCreateDate,\n           originalJobId,\n           originalJobCreateDate\n    from datalake.imhotep.duplicationDetectionRadContents -- datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and modelId in ('Legacy RepostDetector', 'legacyRepostModel-2.2')\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n),\ncycles_l2 as\n(   select fwd.model,\n           fwd.originalJobId as jobIdA,\n           fwd.jobCopyJobId as jobIdB,\n           abs(fwd.jobCopyCreateDate - fwd.originalJobCreateDate) as createSpan,\n           abs(fwd.jobCopyJobId - fwd.originalJobId) jobIdSpan\n    from mappings fwd,\n         mappings rev\n    where fwd.model = rev.model\n    and   fwd.originalJobId = rev.jobCopyJobId\n    and   fwd.jobCopyJobId = rev.originalJobId\n)\nselect model,\n       count(*) l2_cycles\nfrom cycles_l2 -- 92,796\ngroup by 1",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 265,
  "runStartToQueryComplete" : 1579
}, {
  "elapsedMillis" : 85653,
  "totalScheduledMillis" : 19995338,
  "cpuMillis" : 3400645,
  "queuedMillis" : 0,
  "executeMillis" : 1297,
  "getResultMillis" : 0,
  "iterateMillis" : 84395,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 363155390577,
  "query" : "WITH\n\tdeleted_accounts AS (\n        SELECT \n            accountid\n            , deleted\n        FROM imhotep.passdailysnapshot\n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            -- deleted accounts\n            AND deleted = 1\n    ), profiles AS (\n        SELECT \n            acctid AS accountid\n            , resumes\n            , fileinformation\n        FROM jssdi.profile_snapshot \n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND cardinality(\n            \tFILTER(\n                \tfileinformation\n                    , f -> f.type = 'RESUME'\n                )\n            ) > 1\n            -- none resume or single resume in PARSED or IR type\n            AND (\n            \tresumes is null\n                OR ( CARDINALITY(resumes) = 1 and resumes[1].resumetype in ('PARSED', 'INDEED_RESUME')\n                )\n\t)\n   )\n    \n    SELECT\n    count(distinct p.accountid)\n    from profiles p\n    join deleted_accounts a\n    on p.accountid = a.accountid\n    --limit 100",
  "queryTables" : [ "skipperhive.imhotep.passdailysnapshot", "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 266,
  "runStartToQueryComplete" : 1629
}, {
  "elapsedMillis" : 71250,
  "totalScheduledMillis" : 16558925,
  "cpuMillis" : 1995121,
  "queuedMillis" : 0,
  "executeMillis" : 4902,
  "getResultMillis" : 0,
  "iterateMillis" : 66395,
  "rows" : 500,
  "error" : null,
  "scannedBytes" : 12852144576,
  "query" : "with app_status as(SELECT ats,apply_id, disposition_status, max(unixtime) as unixtime\nFROM signal_back_disposition a\nWHERE \n  -- remove apply_id that existed 1y nefore april\n  NOT EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition b\n      WHERE unixtime between imhotep_unixtime('2023-04-01') and imhotep_unixtime('2024-07-01')\n        AND a.apply_id=b.apply_id\n  )\n  -- apply_id first appears in april\n  AND EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition c\n      WHERE unixtime between imhotep_unixtime('2024-07-01') and imhotep_unixtime('2024-08-01')\n        AND a.apply_id=c.apply_id\n  )\n  AND unixtime between imhotep_unixtime('2024-04-01') and imhotep_unixtime('today')\n  \n  GROUP BY 1,2,3\n)\n,max_times as (\nSELECT ats\n, apply_id\n, disposition_status\n, unixtime\n, ROW_NUMBER() OVER(PARTITION BY apply_id ORDER BY unixtime ASC) as status_number\n, COUNT(*) OVER(PARTITION BY apply_id) as total_signals_for_apply\nFROM app_status)\n\nSELECT apply_id, total_signals_for_apply, status_number, FROM_UNIXTIME(unixtime) as disposition_date, disposition_status\nFROM max_times\nWHERE ats='ukg-prodd'\nORDER BY 1,3\n\nlimit 500",
  "queryTables" : [ "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition" ],
  "queryIndex" : 267,
  "runStartToQueryComplete" : 1621
}, {
  "elapsedMillis" : 79956,
  "totalScheduledMillis" : 19038010,
  "cpuMillis" : 4603920,
  "queuedMillis" : 0,
  "executeMillis" : 1984,
  "getResultMillis" : 0,
  "iterateMillis" : 77999,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 21458949342,
  "query" : "-- create dates assigned in the artifact never change\n\nwith\ntoday as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           concat(radArtifactName, '-', modelId) as artifactModel,\n           jobCopyJobId,\n           jobCopyCreateDate,\n           originalJobId,\n           originalJobCreateDate\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n),\nyesterday as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           concat(radArtifactName, '-', modelId) as artifactModel,\n           jobCopyJobId,\n           jobCopyCreateDate,\n           originalJobId,\n           originalJobCreateDate\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') - 24*60*60 <= unixtime and unixtime < imhotep_unixtime('1d')\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n)\nselect y.model as yesterday,\n       t.model as today,\n       y.jobCopyJobId,\n       y.jobCopyCreateDate as yesterdayCopyCreate,\n       t.jobCopyCreateDate as todayCopyCreate,\n       y.originalJobId,\n       y.originalJobCreateDate as yesterdayOriginalCreate,\n       t.originalJobCreateDate as todayOriginalCreate\nfrom yesterday y\njoin today t on (    t.artifactModel = y.artifactModel\n                 and t.jobCopyJobId = y.jobCopyJobId\n                 and t.originalJobId = y.originalJobId\n                 and (   t.jobCopyCreateDate != y.jobCopyCreateDate\n                      or t.originalJobCreateDate != y.originalJobCreateDate\n                     )\n                )",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 268,
  "runStartToQueryComplete" : 1631
}, {
  "elapsedMillis" : 114713,
  "totalScheduledMillis" : 14837886,
  "cpuMillis" : 3251343,
  "queuedMillis" : 0,
  "executeMillis" : 1020,
  "getResultMillis" : 0,
  "iterateMillis" : 113722,
  "rows" : 246,
  "error" : null,
  "scannedBytes" : 20882005208,
  "query" : "-- Primary metric:  jaccard similarity of MVP and legacy mappings (mappings of copy --> original)\n\nwith\ndistinct_non_truth_models as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as compare_model, -- distinct models we want to compare against truth\n           count(*)\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   modelId='legacyRepostModel-2.2'\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n    group by 1 ),\ntruth_mappings as\n(   select concat(ddrc.radArtifactName, '-', ddrc.modelId, '-', runStartTime) as truth_model,\n           dnlm.compare_model,\n           date_trunc('day', from_unixtime(ddrc.jobCopyCreateDate/1000)) as day,\n           ddrc.jobCopyJobId,\n           ddrc.originalJobId\n    from datalake.imhotep.duplicationDetectionRadContents ddrc,\n         distinct_non_truth_models dnlm -- cross join to make a separate copy of each legacy record dedicated to comparing with a model\n    where imhotep_unixtime('1d') <= ddrc.unixtime and ddrc.unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   ddrc.modelId='Legacy RepostDetector'\n    and   imhotep_unixtime('1000d') <= ddrc.jobCopyCreateDate/1000 and ddrc.jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n),\nmodel_mappings as\n(   select concat(radArtifactName, '-', modelId, '-', runStartTime) as model,\n           date_trunc('day', from_unixtime(jobCopyCreateDate/1000)) as day,\n           jobCopyJobId,\n           originalJobId\n    from datalake.imhotep.duplicationDetectionRadContents\n    where imhotep_unixtime('1d') <= unixtime and unixtime < imhotep_unixtime('1d') + 24*60*60\n    and   modelId='legacyRepostModel-2.2'\n    and   imhotep_unixtime('1000d') <= jobCopyCreateDate/1000 and jobCopyCreateDate/1000 < imhotep_unixtime('0d')\n),\nmatches as\n(   select coalesce(lm.day, mm.day) as day,\n           -- could use any field, just detecting null from outer join, using jobCopyJobId\n           lm.jobCopyJobId is not null as truth_match,\n           mm.jobCopyJobId is not null as model_match\n    from truth_mappings lm\n    full outer join model_mappings mm\n        on (    lm.compare_model = mm.model\n            and lm.day = mm.day\n            and lm.jobCopyJobId = mm.jobCopyJobId\n            and lm.originalJobId = mm.originalJobId ) ),\nvenn as\n(   select day,\n           truth_match,\n           model_match,\n           count(*) as count\n    from matches\n    group by 1,2,3 ),\nintersection_count as\n(   select day,\n           count\n    from venn\n    where truth_match\n    and   model_match\n),\nunion_count as\n(   select day,\n           sum(count) as count\n    from venn -- ignoring truth_match and model_match fields\n    group by 1 )\nselect ic.day,\n       ic.count as intersection_count,\n       uc.count as union_count,\n       cast(ic.count as double) / cast(uc.count as double) as jaccard\nfrom intersection_count ic,\n     union_count uc\nwhere ic.day = uc.day\n",
  "queryTables" : [ "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents", "datalakehive.imhotep.duplicationdetectionradcontents" ],
  "queryIndex" : 269,
  "runStartToQueryComplete" : 1669
}, {
  "elapsedMillis" : 90716,
  "totalScheduledMillis" : 19284643,
  "cpuMillis" : 3358845,
  "queuedMillis" : 0,
  "executeMillis" : 630,
  "getResultMillis" : 0,
  "iterateMillis" : 90169,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 359480402891,
  "query" : "WITH profiles AS (\n        SELECT \n            acctid AS accountid\n            , resumes\n            , fileinformation\n        FROM jssdi.profile_snapshot \n        WHERE \n            day = date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND cardinality(\n            \tFILTER(\n                \tfileinformation\n                    , f -> f.type = 'RESUME'\n                )\n            ) > 1\n            -- none resume or single resume in PARSED or IR type\n            AND (\n            \tresumes is null\n                OR ( CARDINALITY(resumes) = 1 and resumes[1].resumetype in ('PARSED', 'INDEED_RESUME')\n                )\n\t)\n   )\n    \n    SELECT\n    count(distinct p.accountid)\n    from profiles p\n    --limit 100",
  "queryTables" : [ "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 270,
  "runStartToQueryComplete" : 1645
}, {
  "elapsedMillis" : 353,
  "totalScheduledMillis" : 53,
  "cpuMillis" : 42,
  "queuedMillis" : 1,
  "executeMillis" : 299,
  "getResultMillis" : 0,
  "iterateMillis" : 61,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 445175,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_start$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_start$partitions" ],
  "queryIndex" : 271,
  "runStartToQueryComplete" : 1556
}, {
  "elapsedMillis" : 11628,
  "totalScheduledMillis" : 554517,
  "cpuMillis" : 65692,
  "queuedMillis" : 0,
  "executeMillis" : 2924,
  "getResultMillis" : 0,
  "iterateMillis" : 8735,
  "rows" : 475,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_3 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '')\nORDER BY date, t0, t1, t2, t3 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 272,
  "runStartToQueryComplete" : 1576
}, {
  "elapsedMillis" : 15639,
  "totalScheduledMillis" : 4071321,
  "cpuMillis" : 1028963,
  "queuedMillis" : 0,
  "executeMillis" : 2483,
  "getResultMillis" : 0,
  "iterateMillis" : 13184,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '')\nORDER BY date, teamId, project, t0, t1 ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 273,
  "runStartToQueryComplete" : 1581
}, {
  "elapsedMillis" : 9749,
  "totalScheduledMillis" : 545312,
  "cpuMillis" : 61085,
  "queuedMillis" : 0,
  "executeMillis" : 2713,
  "getResultMillis" : 0,
  "iterateMillis" : 7070,
  "rows" : 240,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing')\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '')\nORDER BY date, t0, t1, t2 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 274,
  "runStartToQueryComplete" : 1578
}, {
  "elapsedMillis" : 46516,
  "totalScheduledMillis" : 13964747,
  "cpuMillis" : 1521359,
  "queuedMillis" : 0,
  "executeMillis" : 1737,
  "getResultMillis" : 0,
  "iterateMillis" : 44833,
  "rows" : 500,
  "error" : null,
  "scannedBytes" : 10948001418,
  "query" : "with app_status as(SELECT ats,apply_id, disposition_status, max(unixtime) as unixtime\nFROM signal_back_disposition a\nWHERE \n  -- remove apply_id that existed 1y nefore april\n  NOT EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition b\n      WHERE unixtime between imhotep_unixtime('2023-04-01') and imhotep_unixtime('2024-07-01')\n        AND a.apply_id=b.apply_id\n  )\n  -- apply_id first appears in july\n  AND EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition c\n      WHERE unixtime between imhotep_unixtime('2024-07-01') and imhotep_unixtime('2024-08-01')\n        AND a.apply_id=c.apply_id\n  )\n  AND unixtime between imhotep_unixtime('2024-07-01') and imhotep_unixtime('today')\n  \n  GROUP BY 1,2,3\n)\n,max_times as (\nSELECT ats\n, apply_id\n, disposition_status\n, unixtime\n, ROW_NUMBER() OVER(PARTITION BY apply_id ORDER BY unixtime ASC) as status_number\n, COUNT(*) OVER(PARTITION BY apply_id) as total_signals_for_apply\nFROM app_status)\n\nSELECT apply_id, total_signals_for_apply, status_number, FROM_UNIXTIME(unixtime) as disposition_date, disposition_status\nFROM max_times\nWHERE ats='ukg-prodd'\nORDER BY 1,3\n\nlimit 500",
  "queryTables" : [ "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition" ],
  "queryIndex" : 275,
  "runStartToQueryComplete" : 1615
}, {
  "elapsedMillis" : 15334,
  "totalScheduledMillis" : 3834762,
  "cpuMillis" : 970321,
  "queuedMillis" : 0,
  "executeMillis" : 2655,
  "getResultMillis" : 0,
  "iterateMillis" : 12723,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_4 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0  \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 276,
  "runStartToQueryComplete" : 1586
}, {
  "elapsedMillis" : 8135,
  "totalScheduledMillis" : 530744,
  "cpuMillis" : 65015,
  "queuedMillis" : 0,
  "executeMillis" : 2629,
  "getResultMillis" : 0,
  "iterateMillis" : 5539,
  "rows" : 2116,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_5 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '')\nORDER BY date, t0, t1, t2, t3, t4, t5 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 277,
  "runStartToQueryComplete" : 1582
}, {
  "elapsedMillis" : 6994,
  "totalScheduledMillis" : 484333,
  "cpuMillis" : 59987,
  "queuedMillis" : 0,
  "executeMillis" : 2520,
  "getResultMillis" : 0,
  "iterateMillis" : 4502,
  "rows" : 90,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        Lower(tw.tier_1) NOT IN ('global revenue', 'marketing')\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n\t(''='' OR t1 = '')\nORDER BY date, t0, t1 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 278,
  "runStartToQueryComplete" : 1584
}, {
  "elapsedMillis" : 14704,
  "totalScheduledMillis" : 4091148,
  "cpuMillis" : 1088768,
  "queuedMillis" : 0,
  "executeMillis" : 2815,
  "getResultMillis" : 0,
  "iterateMillis" : 11928,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_6 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5, t6 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 279,
  "runStartToQueryComplete" : 1598
}, {
  "elapsedMillis" : 626,
  "totalScheduledMillis" : 277,
  "cpuMillis" : 59,
  "queuedMillis" : 4,
  "executeMillis" : 339,
  "getResultMillis" : 0,
  "iterateMillis" : 303,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 367076,
  "query" : "select * from datalake.tiller.\"oplin_events_error$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_error$partitions" ],
  "queryIndex" : 280,
  "runStartToQueryComplete" : 1586
}, {
  "elapsedMillis" : 14795,
  "totalScheduledMillis" : 3938050,
  "cpuMillis" : 1084682,
  "queuedMillis" : 0,
  "executeMillis" : 2525,
  "getResultMillis" : 0,
  "iterateMillis" : 12314,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '')\nORDER BY date, teamId, project, t0, t1, t2 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 281,
  "runStartToQueryComplete" : 1603
}, {
  "elapsedMillis" : 731,
  "totalScheduledMillis" : 80,
  "cpuMillis" : 2,
  "queuedMillis" : 0,
  "executeMillis" : 412,
  "getResultMillis" : 0,
  "iterateMillis" : 334,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 0,
  "query" : "SELECT *\nFROM datalake.sales_data_strategy_dsa.pipeline_skytree where adc_sales_rep_id = 19244 and quarter_dk = 20241001",
  "queryTables" : [ "datalakehive.sales_data_strategy_dsa.pipeline_skytree" ],
  "queryIndex" : 282,
  "runStartToQueryComplete" : 1591
}, {
  "elapsedMillis" : 279,
  "totalScheduledMillis" : 47,
  "cpuMillis" : 36,
  "queuedMillis" : 0,
  "executeMillis" : 236,
  "getResultMillis" : 0,
  "iterateMillis" : 55,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 367076,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_error$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_error$partitions" ],
  "queryIndex" : 283,
  "runStartToQueryComplete" : 1592
}, {
  "elapsedMillis" : 13970,
  "totalScheduledMillis" : 3654567,
  "cpuMillis" : 1062805,
  "queuedMillis" : 0,
  "executeMillis" : 2390,
  "getResultMillis" : 0,
  "iterateMillis" : 11611,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0\n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 284,
  "runStartToQueryComplete" : 1606
}, {
  "elapsedMillis" : 7602,
  "totalScheduledMillis" : 468281,
  "cpuMillis" : 66083,
  "queuedMillis" : 0,
  "executeMillis" : 2501,
  "getResultMillis" : 0,
  "iterateMillis" : 5133,
  "rows" : 1224,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_4 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '')\nORDER BY date, t0, t1, t2, t3, t4 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 285,
  "runStartToQueryComplete" : 1602
}, {
  "elapsedMillis" : 493,
  "totalScheduledMillis" : 33,
  "cpuMillis" : 3,
  "queuedMillis" : 0,
  "executeMillis" : 396,
  "getResultMillis" : 0,
  "iterateMillis" : 104,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 0,
  "query" : "SELECT *\nFROM datalake.sales_data_strategy_dsa.pipeline_skytree where adc_sales_rep_id = 18922 and quarter_dk = 20241001",
  "queryTables" : [ "datalakehive.sales_data_strategy_dsa.pipeline_skytree" ],
  "queryIndex" : 286,
  "runStartToQueryComplete" : 1597
}, {
  "elapsedMillis" : 6811,
  "totalScheduledMillis" : 361648,
  "cpuMillis" : 56000,
  "queuedMillis" : 0,
  "executeMillis" : 2642,
  "getResultMillis" : 0,
  "iterateMillis" : 4201,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_7 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    ('Permissions'='' OR t7 = 'Permissions') \nORDER BY date, t0, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 287,
  "runStartToQueryComplete" : 1608
}, {
  "elapsedMillis" : 12576,
  "totalScheduledMillis" : 3544008,
  "cpuMillis" : 1096413,
  "queuedMillis" : 0,
  "executeMillis" : 2570,
  "getResultMillis" : 0,
  "iterateMillis" : 10043,
  "rows" : 2437,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_5 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 288,
  "runStartToQueryComplete" : 1614
}, {
  "elapsedMillis" : 7051,
  "totalScheduledMillis" : 125595,
  "cpuMillis" : 17214,
  "queuedMillis" : 0,
  "executeMillis" : 3069,
  "getResultMillis" : 0,
  "iterateMillis" : 4020,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 270934201,
  "query" : "WITH populated_day AS (\n    SELECT MIN(day) AS most_recent_day\n    FROM (\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"jiracloudissues$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"ownershipSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"teamwrksSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n    )\n)\n\n-- Count of a11y violations remediated within SLO in the last 30 days / (All a11y violations remediated within last 30 days + Open issues outside SLO)\nSELECT\n    teamId,\n\tproject,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Closed_Within_SLO,\n    ALL_REM_30 AS All_Remediated_Last_30,\n    UNREM AS Open_Outside_SLO,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7\nFROM (\n\tSELECT\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        \n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- last 30 days\n                  ((ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 1000) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  ((TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 1000) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- last 30 days\n                  (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                -- last 30 days\n                (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.day = (SELECT most_recent_day from populated_day) AND\n        ow.day = (SELECT most_recent_day from populated_day) AND\n        tw.day = (SELECT most_recent_day from populated_day) AND\n        ji.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        ow.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        tw.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        tw.tier_1 NOT IN ('Global Revenue', 'Marketing')\n    GROUP BY ow.accountablePartyId, ji.projectkey, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n        HAVING COUNT(\n        CASE WHEN\n            ji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            -- last 30 days\n            (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n            THEN 1 END\n    ) IS NOT NULL AND (\n        COUNT(\n            CASE WHEN\n                ji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n                -- last 30 days\n                (ji.unixtime - TRY_CAST(ji.resolutiontimestamp AS BIGINT)) / 86400 <= 30\n                THEN 1 END\n        ) + COUNT(\n            CASE WHEN\n                (\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n                ) AND (\n                    (\n                        ji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                        (ji.unixtime - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400 > 30\n                    ) OR (\n                        -- SLO days to complete\n                        ji.priority NOT IN ('Blocker') AND\n                        (ji.unixtime - TRY_CAST(ji.createtimestamp AS BIGINT) / 1000) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) > 0\n    )\n)\nWHERE\n\t(''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    ('Permissions'='' OR t7 = 'Permissions')\nORDER BY teamId, project, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.jiracloudissues$partitions", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.ownershipsnapshot$partitions", "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot$partitions" ],
  "queryIndex" : 289,
  "runStartToQueryComplete" : 1611
}, {
  "elapsedMillis" : 7205,
  "totalScheduledMillis" : 420296,
  "cpuMillis" : 66336,
  "queuedMillis" : 0,
  "executeMillis" : 2517,
  "getResultMillis" : 0,
  "iterateMillis" : 4708,
  "rows" : 2777,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 as t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_6 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '')\nORDER BY date, t0, t1, t2, t3, t4, t5, t6 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 290,
  "runStartToQueryComplete" : 1612
}, {
  "elapsedMillis" : 7234,
  "totalScheduledMillis" : 125790,
  "cpuMillis" : 18653,
  "queuedMillis" : 0,
  "executeMillis" : 3045,
  "getResultMillis" : 0,
  "iterateMillis" : 4219,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 654600418,
  "query" : "WITH\n\npopulated_day AS (\n    SELECT MIN(day) AS most_recent_day\n    FROM (\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"jiracloudissues$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"ownershipSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n\n        UNION ALL\n\n        SELECT MAX(day) AS day\n        FROM datalake.imhotep.\"teamwrksSnapshot$partitions\"\n        WHERE day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR)\n    )\n)\n\n-- Count of a11y violations remediated within SLO in the last 30 days / (All a11y violations remediated within last 30 days + Open issues)\nSELECT\n    inslo,\n    duedate,\n    teamId,\n\t-- project,\n    issuekey,\n    -- issuetype,\n    priority,\n    status,\n    summary,\n    lastupdated,\n    createdate,\n    labels,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7\nFROM (\n\tSELECT\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        ji.issuekey AS issuekey,\n        ji.issuetype AS issuetype,\n        ji.priority AS priority,\n        ji.summary AS summary,\n        ji.status AS status,\n        DATE_FORMAT(FROM_UNIXTIME(CAST(ji.lastupdated AS BIGINT) / 1000), '%Y-%m-%d') AS lastupdated,\n        DATE_FORMAT(DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d'), '%Y-%m-%d') AS createdate,\n        DATE_FORMAT(DATE_ADD('day',\n        \tCASE \n            \tWHEN ji.priority = 'Blocker' THEN 30 \n                ELSE 90 \n            END, DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')), '%Y-%m-%d') AS duedate,\n        CAST(duedate >= DATE_FORMAT(CURRENT_DATE, '%Y-%m-%d') AS BOOLEAN) AS inslo,\n        ji.labels AS labels,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.day = (SELECT most_recent_day from populated_day) AND\n        ow.day = (SELECT most_recent_day from populated_day) AND\n        tw.day = (SELECT most_recent_day from populated_day) AND\n        ji.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        ow.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        tw.day >= CAST(DATE_FORMAT(CURRENT_DATE - INTERVAL '7' DAY, '%Y-%m-%d') AS VARCHAR) AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            -- The line below will include \"Pending Closure\" tickets in the resulting table\n            -- OR ji.resolutiontimestamp = 0\n        )\n    GROUP BY\n    \tow.accountablePartyId,\n    \tji.projectkey,\n        ji.issuekey,\n\t\tji.issuetype,\n        ji.priority,\n        ji.summary,\n        ji.status,\n        DATE_FORMAT(FROM_UNIXTIME(CAST(ji.lastupdated AS BIGINT) / 1000), '%Y-%m-%d'),\n        DATE_FORMAT(DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d'), '%Y-%m-%d'),\n        DATE_FORMAT(DATE_ADD('day',\n        \tCASE \n            \tWHEN ji.priority = 'Blocker' THEN 30\n                ELSE 90 \n           \tEND, DATE_PARSE(CAST(ji.createdate AS VARCHAR), '%Y%m%d')), '%Y-%m-%d'),\n        CAST(duedate >= DATE_FORMAT(CURRENT_DATE, '%Y-%m-%d') AS BOOLEAN),\n        ji.labels,\n        tw.tier_0,\n        tw.tier_1,\n        tw.tier_2,\n        tw.tier_3,\n        tw.tier_4,\n        tw.tier_5,\n        tw.tier_6,\n        tw.tier_7\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    ('Permissions'='' OR t7 = 'Permissions')\nORDER BY \n\tinslo ASC,\n\tduedate ASC ",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.jiracloudissues$partitions", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.ownershipsnapshot$partitions", "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot$partitions" ],
  "queryIndex" : 291,
  "runStartToQueryComplete" : 1615
}, {
  "elapsedMillis" : 7173,
  "totalScheduledMillis" : 434889,
  "cpuMillis" : 79480,
  "queuedMillis" : 0,
  "executeMillis" : 2404,
  "getResultMillis" : 0,
  "iterateMillis" : 4806,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 1298001542,
  "query" : "SELECT\n\tdate,\n    teamId,\n\tproject,\n    t0,\n    t1,\n    t2,\n    t3,\n    t4,\n    t5,\n    t6,\n    t7,\n    open_outside_slo\nFROM (\n\tSELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n    \tji.projectkey AS project,\n    \tow.accountablePartyId AS teamId,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        tw.tier_4 AS t4,\n        tw.tier_5 AS t5,\n        tw.tier_6 AS t6,\n        tw.tier_7 AS t7,\n        COUNT(DISTINCT(ji.issuekey)) AS open_outside_slo\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n        tw.tier_7 != 'Unspecified' AND\n        (\n        \tji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') \n            OR TRY_CAST(ji.resolutiontimestamp AS BIGINT) = 0 \n            AND (\n                (\n                    ji.priority = 'Blocker' AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 30\n                ) OR (\n                    ji.priority NOT IN ('Blocker') AND\n                \t(TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 > 90\n                )\n            )\n        )\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), ow.accountablePartyId, ji.projectkey, tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3, tw.tier_4, tw.tier_5, tw.tier_6, tw.tier_7\n)\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    (''='' OR t3 = '') AND\n    (''='' OR t4 = '') AND\n    (''='' OR t5 = '') AND\n    (''='' OR t6 = '') AND\n    ('Permissions'='' OR t7 = 'Permissions')\nORDER BY date, teamId, project, t0, t1, t2, t3, t4, t5, t6, t7 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 292,
  "runStartToQueryComplete" : 1616
}, {
  "elapsedMillis" : 527,
  "totalScheduledMillis" : 209,
  "cpuMillis" : 54,
  "queuedMillis" : 0,
  "executeMillis" : 314,
  "getResultMillis" : 0,
  "iterateMillis" : 220,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 508015,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_end$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_end$partitions" ],
  "queryIndex" : 293,
  "runStartToQueryComplete" : 1611
}, {
  "elapsedMillis" : 501851,
  "totalScheduledMillis" : 483488116,
  "cpuMillis" : 88828258,
  "queuedMillis" : 0,
  "executeMillis" : 77450,
  "getResultMillis" : 0,
  "iterateMillis" : 424431,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 1010493981258,
  "query" : "WITH experiment_data AS (\n\tSELECT \n    \tCASE WHEN ARRAYS_OVERLAP(ndxgrp, \n            ARRAY['#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275']) \n            THEN 'test' ELSE 'control' END AS _group, \n        q, jobid\n    FROM mobileorganic -- TABLESAMPLE BERNOULLI (10) -- sampling rate\n    WHERE \n    \tunixtime >= imhotep_unixtime('2024-10-15') AND unixtime < imhotep_unixtime('2024-11-12') -- test start date & end date\n        AND country = 'ca'\n        AND ARRAYS_OVERLAP(ndxgrp, \n        \tARRAY['#b15:idxmatchingcontrolplane_e274','#B15:idxmatchingcontrolplane_e274',\n            \t  '#b15:idxmatchingcontrolplane_e275','#B15:idxmatchingcontrolplane_e275'])\n        AND EXISTS(SELECT * FROM synset WHERE CONTAINS(synset.cp_configs, 'SQMATCHING-6593-destination') AND synset.status='IN_PRODUCTION' AND synset.unixtime >= imhotep_unixtime('3d') AND synset.unixtime < imhotep_unixtime('2d') AND CONTAINS(synset.query_variants, mobileorganic.q))\n)\n\nSELECT \n\t_group, \n    COUNT_IF(answer='No') as bad_matches, \n    COUNT() AS all_labels, \n    COUNT_IF(answer='No') / CAST(COUNT() AS DOUBLE) AS BMR\nFROM jobtoqueryrelevance\nINNER JOIN experiment_data ON\n\tjobtoqueryrelevance.query = experiment_data.q\n    AND jobtoqueryrelevance.jobid = experiment_data.jobid\nWHERE \n\tjobtoqueryrelevance.unixtime >= imhotep_unixtime('2024-01-01') AND jobtoqueryrelevance.unixtime < imhotep_unixtime('2024-11-12')\n\tAND jobtoqueryrelevance.country='CA'\nGROUP BY _group",
  "queryTables" : [ "skipperhive.imhotep.jobtoqueryrelevance", "skipperhive.imhotep.mobileorganic", "skipperhive.imhotep.synset" ],
  "queryIndex" : 294,
  "runStartToQueryComplete" : 2118
}, {
  "elapsedMillis" : 314,
  "totalScheduledMillis" : 64,
  "cpuMillis" : 54,
  "queuedMillis" : 0,
  "executeMillis" : 249,
  "getResultMillis" : 0,
  "iterateMillis" : 73,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 445175,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_start$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_start$partitions" ],
  "queryIndex" : 295,
  "runStartToQueryComplete" : 1621
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 136771,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_001939_00222_mzvde): Cannot cast '6377170458' to INT",
  "scannedBytes" : 0,
  "query" : "--ca_en with QB exclusion seasonaljobsearch\nwith base as( \nselect accountid,upper(country) as country,lang as language \nfrom imhotep.jobsearch \nwhere unixtime between imhotep_unixtime('7d') and imhotep_unixtime('0d') \nand contains(grp, 'privileged') = false \nand contains(grp, 'spider') = false\nand contains(rcv, 'jsv') = true \nand contains(rcv, 'interaction') = true\nand contains(useragent, 'catchpoint') = false\nand accountid > 0 \nand country='ca'\nand contains(slcity,'quebec')=false\nand ipcountry='ca'\nand lang='fr'\nand regexp_like(qnorm, '.*(christmas|part|time|parttime|seasonalweekend|evening|hiring|immediately|flexible|night|shift|temporary|temps).*')\nunion\nselect accountid,upper(country) as country,lang as language \nfrom imhotep.mobsearch \nwhere unixtime between imhotep_unixtime('7d') and imhotep_unixtime('0d') \nand contains(grp, 'privileged') = false \nand contains(grp, 'spider') = false\nand contains(rcv, 'jsv') = true \nand contains(rcv, 'interaction')=true\nand contains(useragent, 'catchpoint')=false \nand accountid > 0 \nand country='ca'\nand contains(slcity,'quebec')=false\nand ipcountry='ca'\nand lang='fr'\nand regexp_like(qnorm, '.*(christmas|part|time|parttime|seasonalweekend|evening|hiring|immediately|flexible|night|shift|temporary|temps).*')), \nexclusions as (\nselect cast(accountId AS BIGINT) as account_id\nfrom imhotep.advertiserUsers\nwhere unixtime between imhotep_unixtime('7d') and imhotep_unixtime('0d')\nand cast(accountId AS INT) > 0\nunion\nSELECT cast(reportedAccountId AS BIGINT) AS account_id\nFROM imhotep.fraudulentResumeStatusActivity\nWHERE isfraud='1'\nAND unixtime between imhotep_unixtime('2019-12-09') and imhotep_unixtime('0d'))\nselect base.accountId,base.country,base.language\nfrom base\nleft join exclusions on base.accountid=exclusions.account_id\nwhere exclusions.account_id is null",
  "queryTables" : [ "skipperhive.imhotep.advertiserusers", "skipperhive.imhotep.fraudulentresumestatusactivity", "skipperhive.imhotep.jobsearch", "skipperhive.imhotep.mobsearch" ],
  "queryIndex" : 296,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 20735,
  "totalScheduledMillis" : 12486911,
  "cpuMillis" : 1508752,
  "queuedMillis" : 0,
  "executeMillis" : 1340,
  "getResultMillis" : 0,
  "iterateMillis" : 19462,
  "rows" : 500,
  "error" : null,
  "scannedBytes" : 10948001418,
  "query" : "with app_status as(SELECT ats,apply_id, disposition_status, max(unixtime) as unixtime\nFROM signal_back_disposition a\nWHERE \n  -- remove apply_id that existed 1y nefore april\n  NOT EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition b\n      WHERE unixtime between imhotep_unixtime('2023-04-01') and imhotep_unixtime('2024-07-01')\n        AND a.apply_id=b.apply_id\n  )\n  -- apply_id first appears in july\n  AND EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition c\n      WHERE unixtime between imhotep_unixtime('2024-07-01') and imhotep_unixtime('2024-08-01')\n        AND a.apply_id=c.apply_id\n  )\n  AND unixtime between imhotep_unixtime('2024-07-01') and imhotep_unixtime('today')\n  \n  GROUP BY 1,2,3\n)\n,max_times as (\nSELECT ats\n, apply_id\n, disposition_status\n, unixtime\n, ROW_NUMBER() OVER(PARTITION BY apply_id ORDER BY unixtime ASC) as status_number\n, COUNT(*) OVER(PARTITION BY apply_id) as total_signals_for_apply\nFROM app_status)\n\nSELECT apply_id, total_signals_for_apply, status_number, FROM_UNIXTIME(unixtime) as disposition_date, disposition_status\nFROM max_times\nWHERE ats='ukg-prodd'\nand total_signals_for_apply>1\nORDER BY 1,3\n\nlimit 500",
  "queryTables" : [ "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition" ],
  "queryIndex" : 297,
  "runStartToQueryComplete" : 1643
}, {
  "elapsedMillis" : 462681,
  "totalScheduledMillis" : 510570116,
  "cpuMillis" : 56216645,
  "queuedMillis" : 1,
  "executeMillis" : 39825,
  "getResultMillis" : 0,
  "iterateMillis" : 422892,
  "rows" : 54,
  "error" : null,
  "scannedBytes" : 3537782604315,
  "query" : "SELECT \n--date_format(weekStart, '%Y-%m-%d %H:%i:%s') as weekStart\ndate_format(weekStart, '%Y-%m-%d') as weekStart\n, jobProduct\n, sum(totalJobs) as totalJobs\n, sum(totalDistinctEstimateJobs) as totalDistinctJobsEstimate\n, sum(jsDiscoverableJobs) as jsDiscoverableJobs\nFROM\n\n\t(\n    SELECT weekStart\n    , jobProduct\n    , sum(jobs) as totalJobs\n    , sum(case when feedRank = 1 then jobs else 0 end) as totalDistinctEstimateJobs\n    , 0 as jsDiscoverableJobs\n    FROM\n\n        (\n        SELECT weekStart \n        , jobProduct\n        , sourceid\n        , feedid\n        , jobs\n        , ROW_NUMBER() over (PARTITION BY weekStart, jobProduct, sourceid ORDER BY jobs DESC) as feedRank\n        FROM \n\n            (\n            SELECT DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)) as weekStart\n            , (case when feedid = 50461 then 'Hosted' else 'Indexed' end) as jobProduct\n            , sourceId\n            , feedId\n            , count(distinct(jobid)) as jobs\n            FROM datalake.imhotep.searchablejobs\n            WHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \n                AND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n            AND jobcountry != 'JP'\n            --AND sourceid = 1191667\n            GROUP BY 1,2,3,4\n            ) s1\n        ) s2\n    GROUP BY 1,2\n\n    UNION ALL \n\n    SELECT DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)) as weekStart\n    , (case when feedid = 50461 then 'Hosted' else 'Indexed' end) as jobProduct\n    , 0 as totalJobs\n    , 0 as totalDistinctEstimateJobs\n    , count(distinct(jobid)) as jsDiscoverableJobs\n    FROM datalake.imhotep.searchablejobs\n    WHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \n        AND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n    AND jobcountry != 'JP'\n    AND (waldovisibilitylevel in ('organic','jobalert')\n        OR (waldovisibilitylevel = 'sponsored' AND sponvisibility = 'spon_active'))\n    GROUP BY 1,2\n    ) s3\nGROUP BY 1,2\nORDER BY 1 ASC",
  "queryTables" : [ "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 298,
  "runStartToQueryComplete" : 2085
}, {
  "elapsedMillis" : 3978,
  "totalScheduledMillis" : 275303,
  "cpuMillis" : 18502,
  "queuedMillis" : 0,
  "executeMillis" : 753,
  "getResultMillis" : 0,
  "iterateMillis" : 3237,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 466617892,
  "query" : "SELECT\t\n\tSUM(contact_count) AS contacts_sent\nFROM \n\tdatalake.employer_analytics_platform.xpa_sourcing_engagement_daily\nWHERE \n\trecruitment_advertiser_id = 8981804\n\tAND activity_date >= DATE('2023-10-01')\n    AND activity_date <= DATE('2024-10-01')",
  "queryTables" : [ "datalakehive.employer_analytics_platform.xpa_sourcing_engagement_daily" ],
  "queryIndex" : 299,
  "runStartToQueryComplete" : 1629
}, {
  "elapsedMillis" : 410601,
  "totalScheduledMillis" : 266016365,
  "cpuMillis" : 34995689,
  "queuedMillis" : 0,
  "executeMillis" : 35927,
  "getResultMillis" : 0,
  "iterateMillis" : 374738,
  "rows" : 54,
  "error" : null,
  "scannedBytes" : 2130517734194,
  "query" : "SELECT weekStart\n, isUtilized as isDiscoverableByJS\n, indexedJobsWithAdvId\n, indexedJobs\n, format('%.2f%%', (cast(indexedJobsWithAdvId as double)/cast(indexedJobs as double)*100)) as linkedAdvertiserRate\nFROM\n\n      (SELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n      , (case when (waldovisibilitylevel in ('organic','jobalert')\n          OR (waldovisibilitylevel = 'sponsored' AND sponvisibility = 'spon_active')) \n          then 1 else 0 end) as isUtilized\n      , count(DISTINCT((case when asl.source_id is not null then jobId else null end))) as indexedJobsWithAdvId\n      , count(DISTINCT(jobId)) as indexedJobs\n      FROM datalake.imhotep.searchablejobs sj\n          LEFT JOIN (SELECT source_id\n                      FROM datalake.tiller.adsystemdb_tbladvertiserjobsource\n                      WHERE status = 'VERIFIED'\n                      GROUP BY 1) asl\n          ON sj.sourceid = asl.source_id\n      WHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \n      AND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n      AND feedid != 50461\n      AND jobcountry != 'JP'\n      GROUP BY 1,2)",
  "queryTables" : [ "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 300,
  "runStartToQueryComplete" : 2039
}, {
  "elapsedMillis" : 6082,
  "totalScheduledMillis" : 526204,
  "cpuMillis" : 60351,
  "queuedMillis" : 0,
  "executeMillis" : 532,
  "getResultMillis" : 0,
  "iterateMillis" : 5581,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 1994554172,
  "query" : "SELECT \n\tDATE_TRUNC('month', all_grc.activity_date) as yr_mo\n    , all_grc.advertiser_id\n\t, all_grc.salesrep_id as historically_accurate_salesrep_id\n    , current_reps.salesrep_id as current_salesrep_id\n    , SUM(all_grc.revenue_local_value) as rev\nFROM datalake.bi.vw_rev_global_revenue_and_credit as all_grc\nLEFT JOIN datalake.bi.vw_rev_global_revenue_and_credit as current_reps on all_grc.advertiser_id = current_reps.advertiser_id\nWHERE all_grc.activity_date >= DATE('2024-06-01') \nand current_reps.activity_date >= DATE_ADD('day', -1, CURRENT_DATE)\nGROUP BY \n\tDATE_TRUNC('month', all_grc.activity_date)\n    , all_grc.advertiser_id\n    , all_grc.salesrep_id\n    , current_reps.salesrep_id\nORDER BY \t\n\tDATE_TRUNC('month', all_grc.activity_date)\n    , all_grc.advertiser_id\n    , all_grc.salesrep_id\n    , current_reps.salesrep_id",
  "queryTables" : [ "datalakehive.bi.vw_rev_global_revenue_and_credit", "datalakehive.bi.vw_rev_global_revenue_and_credit" ],
  "queryIndex" : 301,
  "runStartToQueryComplete" : 1634
}, {
  "elapsedMillis" : 5546,
  "totalScheduledMillis" : 549228,
  "cpuMillis" : 57314,
  "queuedMillis" : 0,
  "executeMillis" : 548,
  "getResultMillis" : 0,
  "iterateMillis" : 5032,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 2189086523,
  "query" : "SELECT \n\tDATE_TRUNC('month', all_grc.activity_date) as yr_mo\n    , all_grc.advertiser_id\n\t, all_grc.salesrep_id as historically_accurate_salesrep_id\n    , current_reps.salesrep_id as current_salesrep_id\n    , SUM(all_grc.revenue_local_value) as rev\nFROM datalake.bi.vw_rev_global_revenue_and_credit as all_grc\nLEFT JOIN datalake.bi.vw_rev_global_revenue_and_credit as current_reps on all_grc.advertiser_id = current_reps.advertiser_id\nWHERE all_grc.activity_date >= DATE('2024-10-01') \nand current_reps.activity_date >= DATE_ADD('day', -1, CURRENT_DATE)\nGROUP BY \n\tDATE_TRUNC('month', all_grc.activity_date)\n    , all_grc.advertiser_id\n    , all_grc.salesrep_id\n    , current_reps.salesrep_id\nORDER BY \t\n\tDATE_TRUNC('month', all_grc.activity_date)\n    , all_grc.advertiser_id\n    , all_grc.salesrep_id\n    , current_reps.salesrep_id",
  "queryTables" : [ "datalakehive.bi.vw_rev_global_revenue_and_credit", "datalakehive.bi.vw_rev_global_revenue_and_credit" ],
  "queryIndex" : 302,
  "runStartToQueryComplete" : 1634
}, {
  "elapsedMillis" : 249,
  "totalScheduledMillis" : 1,
  "cpuMillis" : 1,
  "queuedMillis" : 0,
  "executeMillis" : 255,
  "getResultMillis" : 0,
  "iterateMillis" : 2,
  "rows" : 7,
  "error" : null,
  "scannedBytes" : 4548,
  "query" : "select * from datalake.tiller.\"oplin_events_error$snapshots\"",
  "queryTables" : [ "datalake.tiller.oplin_events_error$snapshots" ],
  "queryIndex" : 303,
  "runStartToQueryComplete" : 1632
}, {
  "elapsedMillis" : 71123,
  "totalScheduledMillis" : 14137128,
  "cpuMillis" : 1177388,
  "queuedMillis" : 2,
  "executeMillis" : 11313,
  "getResultMillis" : 0,
  "iterateMillis" : 59852,
  "rows" : 41,
  "error" : null,
  "scannedBytes" : 34124173112,
  "query" : "WITH \n\nw_cl as (\n\tSELECT job_hash_underscore as jobhash\n\tFROM datalake.imhotep.dradis_job2\n\tWHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '31' DAY) \n\t\tAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n\tAND claim_version = '2'\n\tAND status = 'ACTIVE'\n    AND country != 'JP'\n    GROUP BY 1)    \n\nSELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n, (CASE WHEN w_cl.jobhash IS NOT NULL THEN 'CLAIMED' ELSE 'CREATED' END) as action \n, count(DISTINCT(advid)) as advIdsWithJobActions\n, count(DISTINCT(employerjobid)) as jobsActioned\nFROM datalake.imhotep.dradisjobcreateorupdate dcu\nLEFT JOIN w_cl ON dcu.jobhash = w_cl.jobhash\nWHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '31' DAY) \n\tAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\nAND action = 'CREATE'\nAND country != 'JP'\nGROUP BY 1,2\n\nUNION ALL\nSELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n, action \n, count(DISTINCT(advid)) as advIdsWithJobActions\n, count(DISTINCT(employerjobid)) as jobsActioned\nFROM datalake.imhotep.dradisjobcreateorupdate dcu\nWHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '31' DAY) \n\tAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\nAND action = 'UPDATE'\nAND country != 'JP'\nGROUP BY 1,2\n\nUNION ALL\nSELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n, enrollment as action\n, count(DISTINCT(advertiserid)) as advIdsWithJobActions\n, count(DISTINCT(employerjobid)) as jobsActioned\nFROM datalake.imhotep.employerjobenrollments\nWHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '31' DAY) \n\tAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\nGROUP BY 1,2\n\nUNION ALL\nSELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n, product as action\n, count(DISTINCT(auditadvertiserid)) as advIdsWithJobActions\n, count(DISTINCT(employerjobid)) as jobsActioned\nFROM datalake.imhotep.ejgapigroupmembershipchange\nWHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '31' DAY) \n\tAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\nAND product != 'TEST_SINGLETON'\nGROUP BY 1,2",
  "queryTables" : [ "datalakehive.imhotep.dradis_job2", "datalakehive.imhotep.dradisjobcreateorupdate", "datalakehive.imhotep.dradisjobcreateorupdate", "datalakehive.imhotep.ejgapigroupmembershipchange", "datalakehive.imhotep.employerjobenrollments" ],
  "queryIndex" : 304,
  "runStartToQueryComplete" : 1705
}, {
  "elapsedMillis" : 141446,
  "totalScheduledMillis" : 108842100,
  "cpuMillis" : 21625888,
  "queuedMillis" : 1,
  "executeMillis" : 575,
  "getResultMillis" : 0,
  "iterateMillis" : 140905,
  "rows" : 20,
  "error" : null,
  "scannedBytes" : 964657818877,
  "query" : "SELECT desired_job_title_raw_unnest \n       ,COUNT(DISTINCT(acctid)) AS cnt_users \n       ,100 * (CAST(COUNT(DISTINCT(acctid)) AS DOUBLE) / CAST(SUM(COUNT(DISTINCT(acctid))) OVER () AS DOUBLE)) AS pct_users\nFROM (\nSELECT  acctId\n        ,COALESCE(\n            TRY(\n                FLATTEN(FILTER(TRANSFORM(MAP_VALUES(normalizeddata['User'].preferences) , p -> COALESCE(TRY(FILTER(TRANSFORM(FILTER(MAP_VALUES(p.titles) \n                , x -> x.sentiment = 'POSITIVE') \n                , x -> COALESCE(REGEXP_REPLACE(LOWER(TRIM(x.title)), '\\s+', ' '), '')) , x -> x != '')) , ARRAY[])) , x -> x IS NOT NULL))) \n            , ARRAY[] ) AS desired_job_title_raw\n         ,COALESCE(\n            TRY(\n                FLATTEN(FILTER(TRANSFORM(MAP_VALUES(normalizeddata['User'].preferences) , p -> COALESCE(TRY(FILTER(TRANSFORM(FILTER(MAP_VALUES(p.titles) \n                , x -> x.sentiment = 'POSITIVE') \n                , x -> COALESCE(REGEXP_REPLACE(LOWER(TRIM(x.normalizedvalue)), '\\s+', ' '), '')) , x -> x != '')) , ARRAY[])) , x -> x IS NOT NULL))) \n            , ARRAY[] ) AS desired_job_title_normalized\n         ,CASE WHEN CARDINALITY(resumes) > 0 THEN resumes[1].location.country END AS country \n\nFROM jssdi.profile_snapshot\nWHERE day = '2024-10-14' \n) \nCROSS JOIN UNNEST(desired_job_title_raw) AS t(desired_job_title_raw_unnest)\nWHERE country = 'US'\nAND CARDINALITY(desired_job_title_raw) > 0 \nGROUP BY 1 \nORDER BY 2 DESC \nLIMIT 20",
  "queryTables" : [ "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 305,
  "runStartToQueryComplete" : 1782
}, {
  "elapsedMillis" : 135831,
  "totalScheduledMillis" : 103739060,
  "cpuMillis" : 21646589,
  "queuedMillis" : 0,
  "executeMillis" : 655,
  "getResultMillis" : 0,
  "iterateMillis" : 135197,
  "rows" : 20,
  "error" : null,
  "scannedBytes" : 964660539878,
  "query" : "SELECT desired_job_title_raw_unnest \n       ,COUNT(DISTINCT(acctid)) AS cnt_users \n       ,100 * (CAST(COUNT(DISTINCT(acctid)) AS DOUBLE) / CAST(SUM(COUNT(DISTINCT(acctid))) OVER () AS DOUBLE)) AS pct_users\nFROM (\nSELECT  acctId\n        ,COALESCE(\n            TRY(\n                FLATTEN(FILTER(TRANSFORM(MAP_VALUES(normalizeddata['User'].preferences) , p -> COALESCE(TRY(FILTER(TRANSFORM(FILTER(MAP_VALUES(p.titles) \n                , x -> x.sentiment = 'POSITIVE') \n                , x -> COALESCE(REGEXP_REPLACE(LOWER(TRIM(x.title)), '\\s+', ' '), '')) , x -> x != '')) , ARRAY[])) , x -> x IS NOT NULL))) \n            , ARRAY[] ) AS desired_job_title_raw\n         ,COALESCE(\n            TRY(\n                FLATTEN(FILTER(TRANSFORM(MAP_VALUES(normalizeddata['User'].preferences) , p -> COALESCE(TRY(FILTER(TRANSFORM(FILTER(MAP_VALUES(p.titles) \n                , x -> x.sentiment = 'POSITIVE') \n                , x -> COALESCE(REGEXP_REPLACE(LOWER(TRIM(x.normalizedvalue)), '\\s+', ' '), '')) , x -> x != '')) , ARRAY[])) , x -> x IS NOT NULL))) \n            , ARRAY[] ) AS desired_job_title_normalized\n         ,CASE WHEN CARDINALITY(resumes) > 0 THEN resumes[1].location.country END AS country \n\nFROM jssdi.profile_snapshot\nWHERE day = '2024-10-14' \n) \nCROSS JOIN UNNEST(desired_job_title_raw) AS t(desired_job_title_raw_unnest)\nWHERE country = 'US'\nAND CARDINALITY(desired_job_title_raw) > 0 \nGROUP BY 1 \nORDER BY 2 DESC \nLIMIT 20",
  "queryTables" : [ "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 306,
  "runStartToQueryComplete" : 1782
}, {
  "elapsedMillis" : 460277,
  "totalScheduledMillis" : 432024960,
  "cpuMillis" : 19371423,
  "queuedMillis" : 0,
  "executeMillis" : 81633,
  "getResultMillis" : 0,
  "iterateMillis" : 378697,
  "rows" : 2818,
  "error" : null,
  "scannedBytes" : 1186373765243,
  "query" : "with jobs as (SELECT coalesce(parent_company_id, job_source_id+1000000) as parent_company_id \n, coalesce(max(parent_company_name), max(job_source_name)) as parent_company_name   \n, min(job_source_id) as job_source_id\n, count(distinct agg_job_id) as jobs_l30days \n, sum(organic_apply_starts) as orgAS_l30days \n\nFROM datalake.imhotep.jobactivitymetrics j\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nand job_country_code = 'US' and is_job_searchable > 0 \n--and parent_company_id is not null \nand advertiser_type not in ('Test', 'Indeed')\ngroup by 1)\n\n, rev as (select parent_company_id \n, sum(net_revenue_cents)/100 as spend_lyear \nFROM datalake.imhotep.grdm j \nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1y') AND IMHOTEP_UNIXTIME('today') \ngroup by 1) \n\n, together as (select jobs.* \n, coalesce(spend_lyear, 0) as spend_lyear \n, coalesce(last_any_products, cast('2000-01-01' as date)) as last_revenue_date  -- if no last revenue date, then say they last spend in 2000 \n, date_diff('day', coalesce(last_any_products, cast('2000-01-01' as date)), CURRENT_DATE) as days_since_last_spend \nfrom jobs \nleft join rev on rev.parent_company_id = jobs.parent_company_id \nleft join datalake.core.product_analytics_fct_first_last_activity_dates_parent f \n\ton f.parent_company_id = jobs.parent_company_id\n--where jobs_l30days > 100    \n)\n, together_ordered as (select *, \nntile(10) over (order by jobs_l30days desc) as jobs_ntile -- 1 is most jobs \n, ntile(10) over (order by spend_lyear asc) as spend_ntile -- 1 is lowest spend \n, ntile(10) over (order by days_since_last_spend desc) as days_since_last_spend_ntile -- 1 is never spender\nfrom together )\n\n, together_ordered_fixed as (select parent_company_id  \n, job_source_id\n, parent_company_name \n, orgAS_l30days \n, jobs_l30days\n, days_since_last_spend\n, spend_lyear\n, jobs_ntile \n, case when spend_lyear <= 0 then 1 else spend_ntile end as spend_ntile \n, case when days_since_last_spend > 9000 then 1 else days_since_last_spend_ntile end as days_since_last_spend_ntile\n\nfrom together_ordered )\n\n, rfm as (select * \n, jobs_ntile + spend_ntile + days_since_last_spend_ntile as rfm_unweighted \n, 1*jobs_ntile + 100*spend_ntile + 10*days_since_last_spend_ntile as rfm_weighted -- the higher this number the bigger the penalty\nfrom together_ordered_fixed\norder by 1*jobs_ntile + 100*spend_ntile + 10*days_since_last_spend_ntile desc\n)\n\n, jobs_first_hour as (SELECT sourceid\n, jobid  \n, max(normtitle) as normtitle \n, coalesce(max(jlmsa), 'rural') as msa \n, sum(coalesce(applystarts,0)) as applystarts\n\nFROM datalake.imhotep.mobileorganic j\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today')\nand country in ('us', 'US') and jobageminutes <= 60\ngroup by 1, 2)\n\n, jobs_first_hour_ordered as (SELECT * \n, ntile(10) over (order by applystarts asc) as job_ntile\nFROM jobs_first_hour)\n\n\n, subset as (select case when rfm_weighted > 1000 then 'A_1000+ Biggest Penalty'\nwhen rfm_weighted > 900 then 'B_900'\nwhen rfm_weighted > 800 then 'C_800'\nwhen rfm_weighted > 700 then 'D_700'\nwhen rfm_weighted > 600 then 'E_600'\nwhen rfm_weighted > 500 then 'F_500'\nwhen rfm_weighted > 400 then 'G_400'\nwhen rfm_weighted > 300 then 'H_300'\nwhen rfm_weighted > 200 then 'I_200' else 'J_100 Lowest Penalty' end as rfm_segment \n, rfm.* \n, j.* \n, row_number() over (partition by job_source_id order by applystarts desc) as rn \n\nfrom rfm  \nleft join jobs_first_hour_ordered j on j.sourceid = rfm.job_source_id \nwhere rfm_weighted > 1000 or rfm_weighted < 200\nand j.job_ntile = 10 )\n\nselect * from subset \nwhere rn <= 5 and orgAS_l30days > 10000\n\n\n\n",
  "queryTables" : [ "datalakehive.core.product_analytics_fct_first_last_activity_dates_parent", "datalakehive.imhotep.grdm", "datalakehive.imhotep.jobactivitymetrics", "datalakehive.imhotep.mobileorganic" ],
  "queryIndex" : 307,
  "runStartToQueryComplete" : 2112
}, {
  "elapsedMillis" : 318408,
  "totalScheduledMillis" : 7991766,
  "cpuMillis" : 3627208,
  "queuedMillis" : 0,
  "executeMillis" : 3743,
  "getResultMillis" : 0,
  "iterateMillis" : 314680,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 3463254347,
  "query" : "SELECT yyyymmdd, COUNT()\nFROM logrepo.log.jsapirelevantjobsapplystart\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-10') AND IMHOTEP_UNIXTIME('2024-10-21')\ngroup by yyyymmdd\nLIMIT 10",
  "queryTables" : [ "logrepo.log.jsapirelevantjobsapplystart" ],
  "queryIndex" : 308,
  "runStartToQueryComplete" : 1973
}, {
  "elapsedMillis" : 586,
  "totalScheduledMillis" : 236,
  "cpuMillis" : 44,
  "queuedMillis" : 0,
  "executeMillis" : 351,
  "getResultMillis" : 0,
  "iterateMillis" : 244,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 446359,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_notification$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_notification$partitions" ],
  "queryIndex" : 309,
  "runStartToQueryComplete" : 1671
}, {
  "elapsedMillis" : 278,
  "totalScheduledMillis" : 34,
  "cpuMillis" : 24,
  "queuedMillis" : 0,
  "executeMillis" : 251,
  "getResultMillis" : 0,
  "iterateMillis" : 40,
  "rows" : 147,
  "error" : null,
  "scannedBytes" : 228759,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_usage$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_usage$partitions" ],
  "queryIndex" : 310,
  "runStartToQueryComplete" : 1698
}, {
  "elapsedMillis" : 860,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 725,
  "getResultMillis" : 0,
  "iterateMillis" : 155,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 0,
  "query" : "with dat as (\nSELECT \n  jobid,\n  accountid,\n  old_backfiller_mom_qualification_overlap_score_v2,\n  mom_qualification_overlap_score_v2,\n  taxo_attribute_suids as old_backfiller_attributes,\n  job_feature_jsbe_corejobdata_features_attributesuids as new_backfiller_attributes,\n  job_reqs_required, \n  job_reqs_preferred,\n  taxo_attribute_suids_req_level_required,\n  taxo_attribute_suids_req_level_optional,\n  occupations_most_likely,\n  \n  jobseeker_feature_jsdp_user_submitted_qualification_certification,\n  jobseeker_feature_jsdp_user_submitted_qualification_education,\n  jobseeker_feature_jsdp_user_submitted_qualification_language,\n  jobseeker_feature_jsdp_user_submitted_qualification_license,\n  jobseeker_feature_jsdp_user_submitted_qualification_preemploymentcondition,\n  jobseeker_feature_jsdp_user_submitted_qualification_securityclearance,\n  jobseeker_feature_jsdp_user_submitted_qualification_skill,\n  jobseeker_feature_jsdp_resume_extraction_accountid_attributes,\n  jobseeker_feature_jsdp_resume_extraction_accountid_occupations\nFROM \n\tdatalake.imhotep_qa.rjpimpressionsv1_1\nWHERE \n\tunixtime BETWEEN IMHOTEP_UNIXTIME('2024-08-15 09:00:00') AND IMHOTEP_UNIXTIME('2024-08-15 10:00:00') AND\n    old_backfiller_mom_qualification_overlap_score_v2 != mom_qualification_overlap_score_v2 \n\nLIMIT 100)\n\nSELECT * from dat\n\n\n  ",
  "queryTables" : [ "datalakehive.imhotep_qa.rjpimpressionsv1_1" ],
  "queryIndex" : 311,
  "runStartToQueryComplete" : 1703
}, {
  "elapsedMillis" : 402,
  "totalScheduledMillis" : 88,
  "cpuMillis" : 50,
  "queuedMillis" : 0,
  "executeMillis" : 324,
  "getResultMillis" : 0,
  "iterateMillis" : 96,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 508015,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_end$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_end$partitions" ],
  "queryIndex" : 312,
  "runStartToQueryComplete" : 1711
}, {
  "elapsedMillis" : 386,
  "totalScheduledMillis" : 98,
  "cpuMillis" : 42,
  "queuedMillis" : 0,
  "executeMillis" : 390,
  "getResultMillis" : 0,
  "iterateMillis" : 4,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 445175,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_start$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_start$partitions" ],
  "queryIndex" : 313,
  "runStartToQueryComplete" : 1718
}, {
  "elapsedMillis" : 407,
  "totalScheduledMillis" : 85,
  "cpuMillis" : 44,
  "queuedMillis" : 0,
  "executeMillis" : 321,
  "getResultMillis" : 0,
  "iterateMillis" : 96,
  "rows" : 299,
  "error" : null,
  "scannedBytes" : 446359,
  "query" : "select partition, record_count, file_count, ceil(cast(total_size as double) / 1024 / 1024 / 500) as file_estimate, cast(cast(total_size as double) / 1024 / 1024 as decimal(10,3)) as total_size_MB\nfrom datalake.tiller.\"oplin_events_notification$partitions\"",
  "queryTables" : [ "datalake.tiller.oplin_events_notification$partitions" ],
  "queryIndex" : 314,
  "runStartToQueryComplete" : 1725
}, {
  "elapsedMillis" : 700,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 1,
  "executeMillis" : 689,
  "getResultMillis" : 0,
  "iterateMillis" : 25,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 0,
  "query" : "SELECT *\nFROM datalake.imhotep.marketplace_supply_demand_job_seeker\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nAND CARDINALITY(filter(locations, (x) -> x.latitude < -90 OR x.latitude > 90 OR x.longitude < -180 OR x.longitude > 180)) > 0\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.marketplace_supply_demand_job_seeker" ],
  "queryIndex" : 315,
  "runStartToQueryComplete" : 1758
}, {
  "elapsedMillis" : 2610,
  "totalScheduledMillis" : 72298,
  "cpuMillis" : 21524,
  "queuedMillis" : 0,
  "executeMillis" : 1344,
  "getResultMillis" : 0,
  "iterateMillis" : 1282,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 1079811498,
  "query" : "explain analyze select *\nfrom datalake.tiller.oplin_events_start\nwhere ingest_time > date '2024-10-29'",
  "queryTables" : [ "datalake.tiller.oplin_events_start" ],
  "queryIndex" : 316,
  "runStartToQueryComplete" : 1764
}, {
  "elapsedMillis" : 4077,
  "totalScheduledMillis" : 3362,
  "cpuMillis" : 425,
  "queuedMillis" : 0,
  "executeMillis" : 693,
  "getResultMillis" : 0,
  "iterateMillis" : 3407,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 71456067,
  "query" : "SELECT *\nFROM datalake.skytree_iceberg.salesforce_account_rep_assignment\nlimit 10",
  "queryTables" : [ "datalake.skytree_iceberg.salesforce_account_rep_assignment" ],
  "queryIndex" : 317,
  "runStartToQueryComplete" : 1810
}, {
  "elapsedMillis" : 16917,
  "totalScheduledMillis" : 7805544,
  "cpuMillis" : 1389855,
  "queuedMillis" : 0,
  "executeMillis" : 2357,
  "getResultMillis" : 0,
  "iterateMillis" : 14608,
  "rows" : 500,
  "error" : null,
  "scannedBytes" : 10948001418,
  "query" : "with app_status as(SELECT ats,apply_id, disposition_status, max(unixtime) as unixtime\nFROM signal_back_disposition a\nWHERE \n  -- remove apply_id that existed 1y before july\n  NOT EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition b\n      WHERE unixtime between imhotep_unixtime('2023-04-01') and imhotep_unixtime('2024-07-01')\n        AND a.apply_id=b.apply_id\n  )\n  -- apply_id first appears in july\n  AND EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition c\n      WHERE unixtime between imhotep_unixtime('2024-07-01') and imhotep_unixtime('2024-08-01')\n        AND a.apply_id=c.apply_id\n  )\n  AND unixtime between imhotep_unixtime('2024-07-01') and imhotep_unixtime('today')\n  \n  GROUP BY 1,2,3\n)\n,max_times as (\nSELECT ats\n, apply_id\n, disposition_status\n, unixtime\n, ROW_NUMBER() OVER(PARTITION BY apply_id ORDER BY unixtime ASC) as status_number\n, COUNT(*) OVER(PARTITION BY apply_id) as total_signals_for_apply\nFROM app_status)\n\nSELECT apply_id, total_signals_for_apply, status_number, FROM_UNIXTIME(unixtime) as disposition_date, disposition_status\nFROM max_times\nWHERE ats='ukg-prodd'\nAND total_signals_for_apply=1 AND disposition_status='NEW'\nORDER BY 1,3\n\nlimit 500",
  "queryTables" : [ "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition" ],
  "queryIndex" : 318,
  "runStartToQueryComplete" : 1854
}, {
  "elapsedMillis" : 382905,
  "totalScheduledMillis" : 1071865802,
  "cpuMillis" : 205741498,
  "queuedMillis" : 0,
  "executeMillis" : 48565,
  "getResultMillis" : 0,
  "iterateMillis" : 334386,
  "rows" : 27,
  "error" : null,
  "scannedBytes" : 14082175936454,
  "query" : "WITH\n\nw_va as (\n\tSELECT source_id, advertiser_id\n    FROM datalake.tiller.adsystemdb_tbladvertiserjobsource\n    WHERE status = 'VERIFIED'\n    GROUP BY 1,2),\n\nw_sj as (\n\tSELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(sj.unixtime)),'%Y-%m-%d') as weekStart\n\t, w_va.advertiser_id\n    FROM datalake.imhotep.searchablejobs sj\n\tINNER JOIN w_va ON sj.sourceid = w_va.source_id\n    WHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \n\tAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n\tAND jobcountry != 'JP'\n\tGROUP BY 1,2),\n\nw_oi as (\n\tSELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n    , advertiser_id \n    from datalake.imhotep.employer_user_actions_spark\n    WHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \n\t\tAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n    AND country != 'JP'\n    AND is_privileged != 1\n    GROUP BY 1,2),\n    \nw_s as (\n\tSELECT date_format(DATE_TRUNC('WEEK', FROM_UNIXTIME(unixtime)),'%Y-%m-%d') as weekStart\n\t, CAST(advertiserid as BIGINT) as advertiser_id\n\tFROM datalake.imhotep.employerjobsearch\n    WHERE unixtime between imhotep_unixtime(CURRENT_DATE - INTERVAL '183' DAY) \n\t\tAND imhotep_unixtime(CURRENT_DATE - INTERVAL '1' DAY)\n\tAND lower(gqlOperationName) not like '%facet%' \n    AND lower(gqlOperationName) not like '%count%' \n\tAND country != 'JP'\n    AND coalesce(is_privileged,0) != 1\n    GROUP BY 1,2)\n       \nSELECT w_sj.weekStart\n, count(DISTINCT(w_sj.advertiser_id)) as advIdsWithJob\n, count(DISTINCT(w_oi.advertiser_id)) as advIdsOnIndeed\n, count(DISTINCT(w_s.advertiser_id)) as advIdsWithSearch\nFROM w_sj\nLEFT JOIN w_oi \n\tON w_sj.advertiser_id = w_oi.advertiser_id\n\tAND w_sj.weekstart = w_oi.weekstart\nLEFT JOIN w_s\n\tON w_sj.advertiser_id = w_s.advertiser_id\n\tAND w_sj.weekstart = w_s.weekstart\nGROUP BY 1",
  "queryTables" : [ "datalake.tiller.adsystemdb_tbladvertiserjobsource", "datalakehive.imhotep.employer_user_actions_spark", "datalakehive.imhotep.employerjobsearch", "datalakehive.imhotep.searchablejobs" ],
  "queryIndex" : 319,
  "runStartToQueryComplete" : 2237
}, {
  "elapsedMillis" : 1640,
  "totalScheduledMillis" : 696,
  "cpuMillis" : 117,
  "queuedMillis" : 0,
  "executeMillis" : 1644,
  "getResultMillis" : 0,
  "iterateMillis" : 6,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 320,
  "runStartToQueryComplete" : 1858
}, {
  "elapsedMillis" : 1222,
  "totalScheduledMillis" : 299,
  "cpuMillis" : 121,
  "queuedMillis" : 0,
  "executeMillis" : 856,
  "getResultMillis" : 0,
  "iterateMillis" : 443,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 321,
  "runStartToQueryComplete" : 1889
}, {
  "elapsedMillis" : 1137,
  "totalScheduledMillis" : 277,
  "cpuMillis" : 109,
  "queuedMillis" : 0,
  "executeMillis" : 759,
  "getResultMillis" : 0,
  "iterateMillis" : 430,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 322,
  "runStartToQueryComplete" : 1895
}, {
  "elapsedMillis" : 1387,
  "totalScheduledMillis" : 288,
  "cpuMillis" : 128,
  "queuedMillis" : 0,
  "executeMillis" : 819,
  "getResultMillis" : 0,
  "iterateMillis" : 579,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 323,
  "runStartToQueryComplete" : 1913
}, {
  "elapsedMillis" : 0,
  "totalScheduledMillis" : 0,
  "cpuMillis" : 0,
  "queuedMillis" : 0,
  "executeMillis" : 603,
  "getResultMillis" : 0,
  "iterateMillis" : 0,
  "rows" : 0,
  "error" : "Query failed (#20250110_002437_00250_mzvde): Query exceeded the maximum execution time limit of 10.00m",
  "scannedBytes" : 0,
  "query" : "SELECT min(day) FROM jssdi.profile_snapshot WHERE day >= '2020-01-01'",
  "queryTables" : [ "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 324,
  "runStartToQueryComplete" : 0
}, {
  "elapsedMillis" : 1164,
  "totalScheduledMillis" : 265,
  "cpuMillis" : 107,
  "queuedMillis" : 0,
  "executeMillis" : 751,
  "getResultMillis" : 0,
  "iterateMillis" : 422,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 325,
  "runStartToQueryComplete" : 1934
}, {
  "elapsedMillis" : 1292,
  "totalScheduledMillis" : 326,
  "cpuMillis" : 136,
  "queuedMillis" : 0,
  "executeMillis" : 766,
  "getResultMillis" : 0,
  "iterateMillis" : 537,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 326,
  "runStartToQueryComplete" : 1981
}, {
  "elapsedMillis" : 1279,
  "totalScheduledMillis" : 275,
  "cpuMillis" : 122,
  "queuedMillis" : 0,
  "executeMillis" : 880,
  "getResultMillis" : 0,
  "iterateMillis" : 413,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 327,
  "runStartToQueryComplete" : 1981
}, {
  "elapsedMillis" : 1417,
  "totalScheduledMillis" : 410,
  "cpuMillis" : 137,
  "queuedMillis" : 0,
  "executeMillis" : 1419,
  "getResultMillis" : 0,
  "iterateMillis" : 10,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 328,
  "runStartToQueryComplete" : 1993
}, {
  "elapsedMillis" : 261147,
  "totalScheduledMillis" : 126426004,
  "cpuMillis" : 89785203,
  "queuedMillis" : 0,
  "executeMillis" : 3954,
  "getResultMillis" : 0,
  "iterateMillis" : 257205,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 6233557111187,
  "query" : "SELECT\n\t\t\n\t\tmoderationtext,\n        flagged,\n        flaggedcategories,\n        flaggedwords,\n \t\tappName,\n        moderationApi, \n        moderationType, \n   \t\tmodel,\n        moderationHeader,\n        locale,               \n        metric,\n        FROM_UNIXTIME(unixtime) as request_date,\n        requestid,\n        reqbody,\n        respbody\nFROM\n       logrepo.log.llm_proxy lp\n    WHERE\n    \trequestid = '0f7ae361-eeda-45db-90bb-26540b984598'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\nlimit 50\n",
  "queryTables" : [ "logrepo.log.llm_proxy" ],
  "queryIndex" : 329,
  "runStartToQueryComplete" : 2279
}, {
  "elapsedMillis" : 1680,
  "totalScheduledMillis" : 579,
  "cpuMillis" : 129,
  "queuedMillis" : 0,
  "executeMillis" : 927,
  "getResultMillis" : 0,
  "iterateMillis" : 771,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 330,
  "runStartToQueryComplete" : 2071
}, {
  "elapsedMillis" : 1166,
  "totalScheduledMillis" : 309,
  "cpuMillis" : 139,
  "queuedMillis" : 1,
  "executeMillis" : 708,
  "getResultMillis" : 0,
  "iterateMillis" : 472,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 331,
  "runStartToQueryComplete" : 2088
}, {
  "elapsedMillis" : 1261,
  "totalScheduledMillis" : 337,
  "cpuMillis" : 91,
  "queuedMillis" : 0,
  "executeMillis" : 786,
  "getResultMillis" : 0,
  "iterateMillis" : 488,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 332,
  "runStartToQueryComplete" : 2149
}, {
  "elapsedMillis" : 169588,
  "totalScheduledMillis" : 186440544,
  "cpuMillis" : 55038027,
  "queuedMillis" : 0,
  "executeMillis" : 11191,
  "getResultMillis" : 0,
  "iterateMillis" : 158451,
  "rows" : 73,
  "error" : null,
  "scannedBytes" : 1395587327857,
  "query" : "with status_change as (\nselect \n\tcast(adv_id as int) as advertiser_id, cast(cand_id as int) as candidate_id, cand_new_status, unixtime\nFROM datalake.imhotep.candStatusChange2\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2017-01-01') AND IMHOTEP_UNIXTIME('today')\nunion distinct \nselect \n\tadvid as advertiser_id, candid as candidate_id, candnewstatus as cand_new_status, unixtime\nfrom datalake.imhotep.dradiscandstatuschange\nwhere unixtime BETWEEN IMHOTEP_UNIXTIME('2017-01-01') AND IMHOTEP_UNIXTIME('today')\n)\n,hires as (\nselect from_unixtime(unixtime) as hire_timestamp, advertiser_id, candidate_id, cand_new_status \nfrom status_change \nwhere cand_new_status = 'Hired'\n\tand unixtime BETWEEN IMHOTEP_UNIXTIME('2019-01-01') AND IMHOTEP_UNIXTIME('today')\n)\n\n,job_id_mapping AS (\nSELECT a.advertiser_id\n          ,a.candidate_id\n          ,MAX(a.candidate_account_id) AS accountid\n          ,MAX(a.agg_job_id) AS agg_job_id\n          ,MAX(a.ats_job_id) AS ats_job_id\n          ,MAX(a.indeed_apply_ctk) AS indeed_apply_ctk\n          ,MAX(a.apply_id) AS apply_id\n          ,MAX(a.date_created) AS indeed_apply_timestamp\n    FROM datalake.imhotep.candidate_apply AS a\n    INNER JOIN hires AS b\n    ON a.advertiser_id = b.advertiser_id\n    AND a.candidate_id = b.candidate_id\n    where unixtime BETWEEN IMHOTEP_UNIXTIME('2017-01-01') AND IMHOTEP_UNIXTIME('today')\n    GROUP BY 1, 2\n)\n\n,view as (\nselect \n  csc.advertiser_id, \n  csc.candidate_id, \n  min(hire_timestamp) as hire_timestamp,\n  min(job_id_mapping.indeed_apply_timestamp) as indeed_apply_timestamp, \n  min(case when csc.cand_new_status = 'Interviewed' then FROM_UNIXTIME(csc.unixtime) end) as interview_timestamp,\n  min(case when csc.cand_new_status = 'Reviewed' then FROM_UNIXTIME(csc.unixtime) end) as reviewed_timestamp,\n  min(case when csc.cand_new_status = 'Phone Screened' then FROM_UNIXTIME(csc.unixtime) end) as phone_screen_timestamp,\n  MAX(accountid) AS accountid,\n  MAX(agg_job_id) AS agg_job_id,\n  MAX(ats_job_id) AS ats_job_id,\n  MAX(indeed_apply_ctk) AS indeed_apply_ctk,\n  MAX(apply_id) AS apply_id\nfrom status_change as csc\njoin hires on hires.advertiser_id = cast(csc.advertiser_id as int) and hires.candidate_id = cast(csc.candidate_id as int) and csc.cand_new_status != 'Hired'\njoin job_id_mapping on job_id_mapping.advertiser_id = cast(csc.advertiser_id as int) and job_id_mapping.candidate_id = cast(csc.candidate_id as int)\ngroup by 1,2\n)\nselect \n\tdate_trunc('month', hire_timestamp)\tas month,\n    COUNT(*) AS hires,\n    avg(date_diff('day', indeed_apply_timestamp, phone_screen_timestamp)) as time_to_contacting,\n    sum(Case when phone_screen_timestamp is not null then 1 else 0 end) as hires_w_contacting,\n    avg(date_diff('day', indeed_apply_timestamp, hire_timestamp)) as time_to_hire,\n    avg(date_diff('day', indeed_apply_timestamp, reviewed_timestamp)) as time_to_review,\n    avg(date_diff('day', reviewed_timestamp, hire_timestamp)) as time_review_to_hire\nfrom view\ngroup by 1\norder by 1 ",
  "queryTables" : [ "datalakehive.imhotep.candidate_apply", "datalakehive.imhotep.candstatuschange2", "datalakehive.imhotep.candstatuschange2", "datalakehive.imhotep.candstatuschange2", "datalakehive.imhotep.dradiscandstatuschange", "datalakehive.imhotep.dradiscandstatuschange", "datalakehive.imhotep.dradiscandstatuschange" ],
  "queryIndex" : 333,
  "runStartToQueryComplete" : 2330
}, {
  "elapsedMillis" : 19807,
  "totalScheduledMillis" : 6928793,
  "cpuMillis" : 956872,
  "queuedMillis" : 0,
  "executeMillis" : 8287,
  "getResultMillis" : 0,
  "iterateMillis" : 13400,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 26360827481,
  "query" : "WITH scalablesegmenthomepageimpression_ AS (\n  SELECT accountid\n         ,agg_job_id \n         ,qual_attributes_job_js_overlap_positive_count\n         ,qual_job_attributes_count\n         ,'scalablesegmenthomepageimpression' AS source\n  FROM datalake.imhotep.scalablesegmenthomepageimpression\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-02') AND IMHOTEP_UNIXTIME('2024-10-03')\n  AND qual_attributes_job_js_overlap_positive_count > 0\n  AND  qual_job_attributes_count > 0 \n  GROUP BY 1, 2, 3, 4\n)\n\nSELECT a.account_id\n       ,a.job_id\n       ,a.overlappingjobandjsqualifications AS cnt_overlapping_quals_mce\n       ,b.qual_attributes_job_js_overlap_positive_count AS cnt_overlapping_quals_scalableseg\n       ,a.numjobqualifications AS cnt_job_quals_mce\n       ,b.qual_job_attributes_count AS cnt_job_quals_scalableseg\nFROM datalake.imhotep.mce_evaluation AS a \nINNER JOIN scalablesegmenthomepageimpression_ AS b\nON a.account_id = b.accountid \nAND a.job_id = b.agg_job_id\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-02') AND IMHOTEP_UNIXTIME('2024-10-03')\nAND a.overlappingjobandjsqualifications != b.qual_attributes_job_js_overlap_positive_count \nAND a.numjobqualifications != b.qual_job_attributes_count\nLIMIT 10 ",
  "queryTables" : [ "datalakehive.imhotep.mce_evaluation", "datalakehive.imhotep.scalablesegmenthomepageimpression" ],
  "queryIndex" : 334,
  "runStartToQueryComplete" : 2204
}, {
  "elapsedMillis" : 1524,
  "totalScheduledMillis" : 198454,
  "cpuMillis" : 9711,
  "queuedMillis" : 0,
  "executeMillis" : 686,
  "getResultMillis" : 0,
  "iterateMillis" : 1302,
  "rows" : 100000,
  "error" : null,
  "scannedBytes" : 16664714,
  "query" : "SELECT\n    acctid,\n    'JsdpMultiResumeBootstrap' AS backfillPrefix\nFROM datalake.jssdi.profile_snapshot\nWHERE\n    \"day\" = '2024-10-11'\nLIMIT 100000",
  "queryTables" : [ "datalakehive.jssdi.profile_snapshot" ],
  "queryIndex" : 335,
  "runStartToQueryComplete" : 2187
}, {
  "elapsedMillis" : 98015,
  "totalScheduledMillis" : 166427681,
  "cpuMillis" : 50797552,
  "queuedMillis" : 0,
  "executeMillis" : 7352,
  "getResultMillis" : 0,
  "iterateMillis" : 90711,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 953287107888,
  "query" : "WITH\n\tverified_accounts AS (\n    \tSELECT \n        \taccountid\n        FROM imhotep.passdailysnapshot \n        WHERE \n        \tday = '2024-09-01' -- date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND verified = 1\n            AND deleted != 1\n    ),\n    profiles AS (\n    \tSELECT \n        \tacctid AS accountid\n            , coalesce(try(resumes[1].locale), '') AS resume_locale\n            , CAST(\n            \tROW(\n                \tcoalesce(try(regexp_replace(trim(resumes[1].location.city), '\\s+', ' ')), '')\n            \t\t, coalesce(try(resumes[1].location.admin1), '')\n            \t\t, coalesce(try(resumes[1].location.admin2), '')\n                    , coalesce(try(resumes[1].location.country), ''))\n                AS ROW(\n                \tcity varchar\n                    , admin1 varchar\n                    , admin2 varchar\n                    , country varchar)) AS resume_location\n            , coalesce(try(regexp_replace(lower(trim(resumes[1].workexperience[1].title)), '\\s+', ' ')), '') AS resume_most_recent_title\n            , coalesce(try(regexp_replace(lower(trim(resumes[1].workexperience[1].normalizedtitle)), '\\s+', ' ')), '') AS resume_most_recent_normalized_title\n \n            , CAST(\n            \tROW(\n                \tcoalesce(try(regexp_replace(trim(defaultinfo.contactinformation.location.city), '\\s+', ' ')), '')\n            \t\t, coalesce(try(defaultinfo.contactinformation.location.admin1), '')\n            \t\t, coalesce(try(defaultinfo.contactinformation.location.admin2), '')\n                    , coalesce(try(defaultinfo.contactinformation.location.country), ''))\n                AS ROW(\n                \tcity varchar\n                    , admin1 varchar\n                    , admin2 varchar\n                    , country varchar)) AS profile_location\n \t\t\t, coalesce(try(\n            \tflatten(filter(\n                    transform(\n                        map_values(normalizeddata['User'].preferences)\n                        , p -> coalesce(try(\n                        \tfilter(\n                            \ttransform(\n                                \tfilter(\n                                \t\tmap_values(p.titles) \n                                \t\t, x -> x.sentiment = 'POSITIVE')\n                                    , x -> coalesce(regexp_replace(lower(trim(x.title)), '\\s+', ' '), ''))\n                                , x -> x != ''))\n                            , ARRAY[]))\n                    , x -> x IS NOT NULL))[1])\n                , '') AS profile_desired_title\n            /*\n            , coalesce(try(\n            \tflatten(filter(\n                    transform(\n                        map_values(normalizeddata['User'].preferences)\n                        , p -> coalesce(try(\n                        \tfilter(\n                            \ttransform(\n                                \tfilter(\n                                \t\tmap_values(p.locations) \n                                \t\t, x -> x.sentiment = 'POSITIVE')\n                                    , x -> ARRAY[\n                                    \tcoalesce(try(regexp_replace(trim(x.location.city), '\\s+', ' ')), '')\n                                        , coalesce(try(x.location.admin1), '')\n                                        , coalesce(try(x.location.country), '')])\n                                , loc -> any_match(loc, x -> x != '')))\n                            , ARRAY[]))\n                    , x -> x IS NOT NULL)))\n                , ARRAY[]) AS profile_reloc_locations\n            */\n        FROM jssdi.profile_snapshot\n        WHERE \n        \tday = '2024-09-01' --date_format(date_add('day', -1, CURRENT_DATE), '%Y-%m-%d')\n            AND (\n            \t-- cardinality(resumes) = 0\n            \t-- OR (\n                \tcardinality(resumes) = 1\n                    AND resumes[1].resumetype IN ('INDEED_RESUME', 'PARSED')\n                    AND resumes[1].state IN ('PUBLIC', 'HIDDEN')\n                    AND resumes[1].datecreated > 0\n                    AND resumes[1].datemodified > 0\n                    AND resumes[1].location.country = 'US'\n                    AND resumes[1].locale = 'en_US'\n                -- )\n             )\n    ),\n    profile_summary AS (\n        SELECT\n            p.accountid AS profile_accountid \n            , v.accountid AS verified_accountid\n            , IF(profile_desired_title != ''\n                , profile_desired_title\n                , resume_most_recent_normalized_title) AS best_title\n            , CASE\n            \tWHEN (resume_location.city != '' OR resume_location.admin2 != '')\n            \t\t\tAND resume_location.admin1 != '' \n                \t\tAND resume_location.country = 'US' THEN\n                \tresume_location\n                WHEN (profile_location.city != '' OR profile_location.admin2 != '')\n            \t\t\tAND profile_location.admin1 != '' \n                \t\tAND profile_location.country = 'US' THEN\n                \tprofile_location\n                ELSE\n                    NULL\n            END AS best_location\n        FROM \n        \tprofiles p\n            JOIN verified_accounts v ON (p.accountid = v.accountid)\n    ),\n    clicks AS (\n      SELECT acctid as accountid, jobid, sponsored as is_sponsored_click\n      FROM datalake.imhotep.clickanalytics\n      WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2023-09-01') AND IMHOTEP_UNIXTIME('2024-09-01') AND acctid is not null\n      LIMIT 1000\n\t)\n    SELECT\n    profile_accountid\n\t, best_title\n    , best_location.city\n    , best_location.admin1\n    , best_location.admin2\n    , jobid\n    , is_sponsored_click\nFROM \n\tprofile_summary p JOIN clicks c on p.profile_accountid = c.accountid\nWHERE \n    \n    best_location IS NOT NULL\nGROUP BY 1, 2, 3, 4, 5, 6, 7\nORDER BY 1 DESC limit 10\n",
  "queryTables" : [ "datalakehive.imhotep.clickanalytics", "skipperhive.imhotep.passdailysnapshot", "skipperhive.jssdi.profile_snapshot" ],
  "queryIndex" : 336,
  "runStartToQueryComplete" : 2328
}, {
  "elapsedMillis" : 1836,
  "totalScheduledMillis" : 707,
  "cpuMillis" : 127,
  "queuedMillis" : 0,
  "executeMillis" : 1837,
  "getResultMillis" : 0,
  "iterateMillis" : 11,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 337,
  "runStartToQueryComplete" : 2251
}, {
  "elapsedMillis" : 2793,
  "totalScheduledMillis" : 60971,
  "cpuMillis" : 7180,
  "queuedMillis" : 0,
  "executeMillis" : 2032,
  "getResultMillis" : 0,
  "iterateMillis" : 1179,
  "rows" : 1000,
  "error" : null,
  "scannedBytes" : 167394810,
  "query" : "SELECT \n\tline_item_resource_id,\n\telement_at(resource_tags, 'aws_eks_deployment') AS ws_eks_deployment,\n\telement_at(resource_tags, 'aws_eks_namespace') AS aws_eks_namespace,\n\telement_at(resource_tags, 'aws_eks_node') AS aws_eks_node,\n\telement_at(resource_tags, 'aws_eks_workload_name') AS aws_eks_workload_name,\n\telement_at(resource_tags, 'aws_eks_workload_type') AS aws_eks_workload_type\nFROM datalake.cloudcost_qa.aws_container_cost\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-09-01') AND IMHOTEP_UNIXTIME('2024-09-14')\nand element_at(resource_tags, 'aws_eks_deployment') != ''\nand element_at(resource_tags, 'aws_eks_namespace') != ''\nand element_at(resource_tags, 'aws_eks_node') != ''\nand element_at(resource_tags, 'aws_eks_workload_name') != ''\nand element_at(resource_tags, 'aws_eks_workload_type') != ''\nLIMIT 1000\n",
  "queryTables" : [ "datalakehive.cloudcost_qa.aws_container_cost" ],
  "queryIndex" : 338,
  "runStartToQueryComplete" : 2259
}, {
  "elapsedMillis" : 1483,
  "totalScheduledMillis" : 410,
  "cpuMillis" : 119,
  "queuedMillis" : 0,
  "executeMillis" : 802,
  "getResultMillis" : 0,
  "iterateMillis" : 697,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 339,
  "runStartToQueryComplete" : 2262
}, {
  "elapsedMillis" : 5933,
  "totalScheduledMillis" : 5632873,
  "cpuMillis" : 470661,
  "queuedMillis" : 0,
  "executeMillis" : 1489,
  "getResultMillis" : 0,
  "iterateMillis" : 4477,
  "rows" : 500,
  "error" : null,
  "scannedBytes" : 28581903081,
  "query" : "with examples as (\nSELECT\n\t\tmoderationtext,\n        flagged,\n        flaggedcategories,\n        flaggedwords,\n \t\tappName,\n        moderationApi, \n        moderationType,\n        FROM_UNIXTIME(unixtime) as request_date,\n        requestid       \nFROM\n      \tdatalake.imhotep.llmproxy lp -- switching to imhotep for speed even though there is a 1D data lag\n        --logrepo.log.llm_proxy lp\n    WHERE\n    \tflagged = '1'\n        and lp.metric = 'moderation-text'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and ('ContentGenerationService' = '' or appName='ContentGenerationService')\n        and ('openAIModerations' = '' or moderationApi='openAIModerations')\n        and ('input' = '' or moderationType='input')\nlimit 500\n)\n\nSELECT\n        m.*,\n        r.model,\n        r.moderationHeader,\n        r.locale        \nFROM datalake.imhotep.llmproxy r\n        join examples m on m.requestid = r.requestid -- switching to imhotep for speed even though there is a 1D data lag\n    WHERE\n        r.unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\n        and r.metric = 'llm-proxy'",
  "queryTables" : [ "datalakehive.imhotep.llmproxy", "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 340,
  "runStartToQueryComplete" : 2273
}, {
  "elapsedMillis" : 912,
  "totalScheduledMillis" : 50,
  "cpuMillis" : 3,
  "queuedMillis" : 1,
  "executeMillis" : 835,
  "getResultMillis" : 0,
  "iterateMillis" : 103,
  "rows" : 3939,
  "error" : null,
  "scannedBytes" : 33440,
  "query" : "SELECT *\nFROM llmExplainerEvaluationLog\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME(now())\n",
  "queryTables" : [ "skipperhive.imhotep.llmexplainerevaluationlog" ],
  "queryIndex" : 341,
  "runStartToQueryComplete" : 2269
}, {
  "elapsedMillis" : 856,
  "totalScheduledMillis" : 40,
  "cpuMillis" : 2,
  "queuedMillis" : 0,
  "executeMillis" : 812,
  "getResultMillis" : 0,
  "iterateMillis" : 54,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 6493,
  "query" : "SELECT *\nFROM llmExplainerEvaluationRunLog\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME(now())\n",
  "queryTables" : [ "skipperhive.imhotep.llmexplainerevaluationrunlog" ],
  "queryIndex" : 342,
  "runStartToQueryComplete" : 2272
}, {
  "elapsedMillis" : 10086,
  "totalScheduledMillis" : 168064,
  "cpuMillis" : 37773,
  "queuedMillis" : 0,
  "executeMillis" : 6995,
  "getResultMillis" : 0,
  "iterateMillis" : 3116,
  "rows" : 240,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing')\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '')\nORDER BY date, t0, t1, t2 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 343,
  "runStartToQueryComplete" : 2281
}, {
  "elapsedMillis" : 9239,
  "totalScheduledMillis" : 97440,
  "cpuMillis" : 33753,
  "queuedMillis" : 0,
  "executeMillis" : 6619,
  "getResultMillis" : 0,
  "iterateMillis" : 2643,
  "rows" : 30,
  "error" : null,
  "scannedBytes" : 1026047522,
  "query" : "WITH sub1 as (\n    SELECT\n    \tDATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d') as date,\n        tw.tier_0 AS t0,\n        tw.tier_1 AS t1,\n        tw.tier_2 AS t2,\n        tw.tier_3 AS t3,\n        COUNT(CASE WHEN\n        \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND \n            (\n              (\n                  ji.priority = 'Blocker' AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 30\n              ) OR (\n              \t  -- Fallback to capture Critical and all other issues until bot updates to prevent data gaps \n                  ji.priority NOT IN ('Blocker') AND\n                  -- ji.resolutiontimestamp > 0 AND\n                  -- last 30 days\n                  (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 AND\n                  -- SLO days to complete\n                  (TRY_CAST(ji.resolutiontimestamp AS BIGINT) - TRY_CAST(ji.createtimestamp AS BIGINT)) / 86400000 <= 90\n              )\n            ) THEN 1 END\n        ) AS REM_30,\n        COUNT(\n        \tCASE WHEN\n            \tji.status IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release') AND\n            \t-- ji.resolutiontimestamp > 0 AND\n                (ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30\n                THEN 1 END\n        ) AS ALL_REM_30,\n        COUNT(\n        \tCASE WHEN\n            \t(\n                    ji.status NOT IN ('Closed', 'Done', 'Pending Code Cleanup', 'Pending Closure', 'Pending Analysis', 'Test Paused', 'Test in Progress', 'Pending Prod Release')\n            \t    -- OR ji.resolutiontimestamp = 0\n                ) AND (\n                \t(\n                    \tji.priority = 'Blocker' AND\n                        -- SLO days to complete\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 30\n                    ) OR (\n                    \t-- SLO days to complete\n                    \tji.priority NOT IN ('Blocker') AND\n                \t\t(ji.unixtime - (TRY_CAST(ji.createtimestamp AS BIGINT) / 1000)) / 86400 > 90\n                    )\n                )\n                THEN 1 END\n        ) AS UNREM\n    FROM datalake.imhotep.jiracloudissues AS ji\n    JOIN datalake.imhotep.ownershipSnapshot AS ow ON ji.projectkey = ow.key AND ji.day = ow.day\n    JOIN datalake.imhotep.teamwrksSnapshot AS tw ON ow.accountablePartyId = tw.teamId AND tw.day = ow.day\n    WHERE\n        ji.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        ow.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        tw.unixtime BETWEEN IMHOTEP_UNIXTIME('30d') AND IMHOTEP_UNIXTIME('today') AND\n        CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y') AND\n        NOT CONTAINS(TRANSFORM(ji.labelstok, s -> LOWER(s)), 'a11y-exemption') AND\n        -- ((ji.unixtime - (TRY_CAST(ji.resolutiontimestamp AS BIGINT) / 1000)) / 86400 <= 30 OR ji.resolutiontimestamp = 0) AND\n        ji.projectkey NOT IN  ('QL', 'A11Y', 'PIED', 'HIGHLANDER', 'JSA11Y', 'EMPA11Y', 'FIM', 'DSYNC', 'CHUBSERV', 'CHUBUI', 'COMMSHUB', 'CHUBSEND') AND\n        ji.issuetype = 'Bug' AND\n        LOWER(tw.tier_1) NOT IN ('global revenue', 'marketing') AND\n\t\ttw.tier_3 != 'Unspecified'\n    GROUP BY DATE_FORMAT(FROM_UNIXTIME(ji.unixtime), '%Y-%m-%d'), tw.tier_0, tw.tier_1, tw.tier_2, tw.tier_3\n)\n\nSELECT\n\tdate,\n    t0,\n    t1,\n    t2,\n    t3,\n    CASE\n    \tWHEN ALL_REM_30 IS NULL OR (ALL_REM_30 + UNREM = 0) THEN 100\n        ELSE ROUND((CAST(REM_30 AS DECIMAL(10, 2)) / (CAST(ALL_REM_30 AS DECIMAL(10, 2)) + CAST(UNREM AS DECIMAL(10, 2)))) * 100, 2)\n    END AS SLO_Score,\n    REM_30 AS Within_SLO,\n    ALL_REM_30 AS Remediated_Last_30,\n    UNREM AS Open_Outside_SLO\nFROM sub1\nWHERE\n\t(t0 = 'Indeed') AND\n    (''='' OR t1 = '') AND\n    (''='' OR t2 = '') AND\n    ('Design Systems'='' OR t3 = 'Design Systems')\nORDER BY date, t0, t1, t2, t3 ASC",
  "queryTables" : [ "datalakehive.imhotep.jiracloudissues", "datalakehive.imhotep.ownershipsnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 344,
  "runStartToQueryComplete" : 2282
}, {
  "elapsedMillis" : 4712,
  "totalScheduledMillis" : 503695,
  "cpuMillis" : 161569,
  "queuedMillis" : 0,
  "executeMillis" : 2185,
  "getResultMillis" : 0,
  "iterateMillis" : 2559,
  "rows" : 5145,
  "error" : null,
  "scannedBytes" : 642152707,
  "query" : "/*Username: kmcneany@indeed.com*/ \nwith tier_info as\n\n(\nSELECT\n    \tcast(from_unixtime(unixtime,'Etc/GMT+6') as date) as date\n    \t, applicationid\n\t\t, criticalitytier\n        , teamid\n        , ROW_NUMBER() OVER (PARTITION BY applicationid\n\t\t\tORDER BY cast(from_unixtime(unixtime,'Etc/GMT+6') AS date) desc, applicationid) AS row_num\n\tFROM datalake.imhotep_iceberg.serviceregistrysnapshots srs\n\tWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('3mo') AND IMHOTEP_UNIXTIME('1d')\n\tGROUP BY 1,2,3,4\n),\n\nlatest_tier_info AS\n\n(\n\tSELECT\n    \tapplicationid\n\t\t, criticalitytier \n        , teamid\n    FROM tier_info\n    WHERE row_num = 1\n),\n\nteam_info AS (\n\nSELECT\n    \tcast(from_unixtime(unixtime,'Etc/GMT+6') as date) as date\n    \t, teamid\n\t\t, tier_1_team_id\n\t\t, tier_2_team_id\n        , ROW_NUMBER() OVER (PARTITION BY teamid\n\t\t\tORDER BY cast(from_unixtime(unixtime,'Etc/GMT+6') AS date) desc, tier_1_team_id, tier_2_team_id) AS row_num\n\tFROM datalake.imhotep_iceberg.teamwrkssnapshot\n\tWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('3mo') AND IMHOTEP_UNIXTIME('1d')\n\tGROUP BY 1,2,3,4\n),\n\nlatest_team_info AS (\n\tSELECT\n    \tteamid\n\t\t, tier_1_team_id\n\t\t, tier_2_team_id\n    FROM team_info\n    WHERE row_num = 1\n),\n\nkube_costs AS \n\n(\n\tSELECT \n        service,\n        max(application_id) as application_id\n\tFROM datalake.cloudcost_iceberg.kubecost_costandusage\n\tWHERE \n    \tunixtime BETWEEN IMHOTEP_UNIXTIME('3mo') AND IMHOTEP_UNIXTIME('1d')\n\t\tAND usage_type = 'cpu core' \n        and service <> ''\n\tGROUP BY 1\n)\n\nSELECT\n\tkc.service,\n    kc.application_id,\n    ltier.teamid,\n\tlteam.tier_1_team_id,\n    lteam.tier_2_team_id,\n\tltier.criticalitytier as current_ct\n\nFROM kube_costs AS kc\nLEFT JOIN latest_tier_info as ltier\n\ton kc.application_id = ltier.applicationid\nLEFT JOIN latest_team_info AS lteam\n\tON ltier.teamid = lteam.teamid\n    \n",
  "queryTables" : [ "datalake.cloudcost_iceberg.kubecost_costandusage", "datalake.imhotep_iceberg.serviceregistrysnapshots", "datalake.imhotep_iceberg.teamwrkssnapshot" ],
  "queryIndex" : 345,
  "runStartToQueryComplete" : 2284
}, {
  "elapsedMillis" : 6374,
  "totalScheduledMillis" : 2510,
  "cpuMillis" : 1320,
  "queuedMillis" : 0,
  "executeMillis" : 3858,
  "getResultMillis" : 0,
  "iterateMillis" : 2528,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 826077,
  "query" : "SELECT\n    cast(json_extract(element_at(cast(json_parse(chatmessages) as array<JSON>),\n    cardinality(cast(json_parse(chatmessages) as array<JSON>)) - 1), '$.content') as varchar) as job_description,\n    cast(json_extract(element_at(cast(json_parse(chatmessages) as array<JSON>),\n    cardinality(cast(json_parse(chatmessages) as array<JSON>))), '$.content') as varchar) as jobseeker_resume,\n    rawContent as generated_introduction\nFROM logrepo.log.GeneratedContentLog\nWHERE uid='1i53178bs37nf001'",
  "queryTables" : [ "logrepo.log.generatedcontentlog" ],
  "queryIndex" : 346,
  "runStartToQueryComplete" : 2291
}, {
  "elapsedMillis" : 1406,
  "totalScheduledMillis" : 289,
  "cpuMillis" : 118,
  "queuedMillis" : 0,
  "executeMillis" : 934,
  "getResultMillis" : 0,
  "iterateMillis" : 487,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 347,
  "runStartToQueryComplete" : 2306
}, {
  "elapsedMillis" : 4067,
  "totalScheduledMillis" : 2019914,
  "cpuMillis" : 343284,
  "queuedMillis" : 0,
  "executeMillis" : 818,
  "getResultMillis" : 0,
  "iterateMillis" : 3263,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 36113051009,
  "query" : "SELECT\n\t\t\n\t\tmoderationtext,\n        flagged,\n        flaggedcategories,\n        flaggedwords,\n \t\tappName,\n        moderationApi, \n        moderationType, \n   \t\tmodel,\n        moderationHeader,\n        locale,               \n        metric,\n        FROM_UNIXTIME(unixtime) as request_date,\n        requestid\nFROM\n      \tdatalake.imhotep.llmproxy lp\n    WHERE\n    \trequestid = '0f7ae361-eeda-45db-90bb-26540b984598'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\nlimit 50\n",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 348,
  "runStartToQueryComplete" : 2309
}, {
  "elapsedMillis" : 4983,
  "totalScheduledMillis" : 9582900,
  "cpuMillis" : 2096033,
  "queuedMillis" : 0,
  "executeMillis" : 1652,
  "getResultMillis" : 0,
  "iterateMillis" : 3346,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 101470609755,
  "query" : "SELECT COUNT_IF(ctk IS NULL OR ctk = '') * 1.00 / COUNT() AS empty_ctk_ratio\nFROM datalake.cloudflare_logs.http\nWHERE timestamp BETWEEN timestamp '2024-10-25 10:00' AND timestamp '2024-10-25 11:00'\nAND securityruleid='1e8af07db9614e64ae9403220f6fa2d8'\nAND securityaction!='managedChallenge'\n",
  "queryTables" : [ "datalake.cloudflare_logs.http" ],
  "queryIndex" : 349,
  "runStartToQueryComplete" : 2318
}, {
  "elapsedMillis" : 5888,
  "totalScheduledMillis" : 6243474,
  "cpuMillis" : 1830718,
  "queuedMillis" : 0,
  "executeMillis" : 347,
  "getResultMillis" : 0,
  "iterateMillis" : 5552,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 101470609755,
  "query" : "SELECT COUNT_IF(clientrequestreferer IS NULL OR clientrequestreferer = '') * 1.00 / COUNT() AS empty_referer_ratio\nFROM datalake.cloudflare_logs.http\nWHERE timestamp BETWEEN timestamp '2024-10-25 10:00' AND timestamp '2024-10-25 11:00'\nAND securityruleid='1e8af07db9614e64ae9403220f6fa2d8'\nAND securityaction!='managedChallenge'\n",
  "queryTables" : [ "datalake.cloudflare_logs.http" ],
  "queryIndex" : 350,
  "runStartToQueryComplete" : 2321
}, {
  "elapsedMillis" : 15628,
  "totalScheduledMillis" : 4137818,
  "cpuMillis" : 1536556,
  "queuedMillis" : 0,
  "executeMillis" : 5482,
  "getResultMillis" : 0,
  "iterateMillis" : 10205,
  "rows" : 670444,
  "error" : null,
  "scannedBytes" : 8280204909,
  "query" : "with app_status as(SELECT ats,apply_id, disposition_status, max(unixtime) as unixtime\nFROM signal_back_disposition a\nWHERE \n  -- remove apply_id that existed before july\n  NOT EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition b\n      WHERE unixtime between imhotep_unixtime('2024-01-01') and imhotep_unixtime('2024-07-01')\n        AND a.apply_id=b.apply_id\n  )\n  -- limit to new apply_id from july\n  AND EXISTS (\n      SELECT apply_id\n      FROM signal_back_disposition c\n      WHERE unixtime between imhotep_unixtime('2024-07-01') and imhotep_unixtime('2024-08-01')\n        AND a.apply_id=c.apply_id\nlimit 5000\n  )\n  AND unixtime between imhotep_unixtime('2024-07-01') and imhotep_unixtime('today')\n  \n  GROUP BY 1,2,3\n)\n,max_times as (\nSELECT ats\n, apply_id\n, disposition_status\n, unixtime\n, ROW_NUMBER() OVER(PARTITION BY apply_id ORDER BY unixtime ASC) as status_number\n, COUNT(*) OVER(PARTITION BY apply_id) as total_signals_for_apply\nFROM app_status)\n\nSELECT apply_id, total_signals_for_apply, status_number, FROM_UNIXTIME(unixtime) as disposition_date, disposition_status\nFROM max_times\nWHERE ats='ukg-prodd'\nAND total_signals_for_apply=1 AND disposition_status='NEW'\nORDER BY 1,3\n",
  "queryTables" : [ "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition", "skipperhive.imhotep.signal_back_disposition" ],
  "queryIndex" : 351,
  "runStartToQueryComplete" : 2363
}, {
  "elapsedMillis" : 2699,
  "totalScheduledMillis" : 725,
  "cpuMillis" : 369,
  "queuedMillis" : 1,
  "executeMillis" : 1995,
  "getResultMillis" : 0,
  "iterateMillis" : 727,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 7451880,
  "query" : "--MW 20240721\n\nSELECT proctor.test_name, \n  proctor.experiment_owner_id as owner,\n  proctor.metadata_test_purpose as test_purpose,\n  MAX( CASE WHEN proctor.days_since_silent > 0 OR proctor.definition_allocation_live_bucket_count = 0\n     THEN 0 ELSE 1 END )  as incurs_logging_cost,\n  MAX(days_since_test_update) as days_since_test_update\nFROM datalake.imhotep.proctor_daily_snapshot as proctor\nWHERE proctor.environment = 'production'\n  AND proctor.unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n  AND days_since_test_update >= 0\n  AND proctor.experiment_owner_id in (\n      SELECT teamId \n      FROM datalake.imhotep.teamwrksSnapshot       \n      WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today') \n      AND tier_5 = 'Automation Platform & Exp'\n      AND ('interop-jc-automation' = '' OR teamId = 'interop-jc-automation')\n    )\nGROUP BY 1,2,3\nORDER BY incurs_logging_cost DESC, days_since_test_update DESC, owner",
  "queryTables" : [ "datalakehive.imhotep.proctor_daily_snapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 352,
  "runStartToQueryComplete" : 2361
}, {
  "elapsedMillis" : 1592,
  "totalScheduledMillis" : 485,
  "cpuMillis" : 163,
  "queuedMillis" : 0,
  "executeMillis" : 1239,
  "getResultMillis" : 0,
  "iterateMillis" : 368,
  "rows" : 19,
  "error" : null,
  "scannedBytes" : 708171,
  "query" : "--MW 20240721\n-- Helper query to find your team ids of interest e.g. 'Hiring Events','Wayfinder' ('Hiring Events','Wayfinder' may be left as empty list of \"\") \nSELECT teamId, tier_6 AS team_name, tier_5 AS parent\nFROM datalake.imhotep.teamwrksSnapshot \nWHERE 1=1\n\tAND unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today') \n    AND tier_5 = 'Automation Platform & Exp'\nUNION ALL \nSELECT teamId, tier_4 AS team_name, tier_2 AS parent\nFROM datalake.imhotep.teamwrksSnapshot \nWHERE 1=1\n\tAND unixtime BETWEEN IMHOTEP_UNIXTIME('2024-03-01') AND IMHOTEP_UNIXTIME('2024-03-02') \n    AND tier_2 IN ('Hiring Events','Wayfinder')\n\tAND tier = 'T4'\nORDER BY parent",
  "queryTables" : [ "datalakehive.imhotep.teamwrkssnapshot", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 353,
  "runStartToQueryComplete" : 2360
}, {
  "elapsedMillis" : 1797,
  "totalScheduledMillis" : 10572,
  "cpuMillis" : 1275,
  "queuedMillis" : 1,
  "executeMillis" : 1703,
  "getResultMillis" : 0,
  "iterateMillis" : 119,
  "rows" : 4202,
  "error" : null,
  "scannedBytes" : 34552662,
  "query" : "SELECT\n\tcase when exception like '%No Partition%' then 'No Partition Found' else exception end exceptions,\n    count(1)\nFROM datalake.imhotep.datalakeintakeevent\nWHERE \n\tunixtime BETWEEN IMHOTEP_UNIXTIME('7d') AND IMHOTEP_UNIXTIME('today')\n    and isproviderdataerror = '1'\ngroup by 1",
  "queryTables" : [ "datalakehive.imhotep.datalakeintakeevent" ],
  "queryIndex" : 354,
  "runStartToQueryComplete" : 2403
}, {
  "elapsedMillis" : 1078,
  "totalScheduledMillis" : 741,
  "cpuMillis" : 151,
  "queuedMillis" : 1,
  "executeMillis" : 484,
  "getResultMillis" : 0,
  "iterateMillis" : 604,
  "rows" : 177,
  "error" : null,
  "scannedBytes" : 16033,
  "query" : "describe datalake.imhotep.jobactivitymetrics",
  "queryTables" : [ "datalakehive.information_schema.columns" ],
  "queryIndex" : 355,
  "runStartToQueryComplete" : 2433
}, {
  "elapsedMillis" : 3636,
  "totalScheduledMillis" : 3267683,
  "cpuMillis" : 277513,
  "queuedMillis" : 0,
  "executeMillis" : 796,
  "getResultMillis" : 0,
  "iterateMillis" : 2852,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 38501714023,
  "query" : "SELECT\n\t\tpath,\n        method,\n        status,\n\t\tmoderationtext,\n        flagged,\n        flaggedcategories,\n        flaggedwords,\n \t\tappName,\n        moderationApi, \n        moderationType, \n   \t\tmodel,\n        moderationHeader,\n        locale,               \n        metric,\n        FROM_UNIXTIME(unixtime) as request_date,\n        requestid,\n        requesttrackingid\nFROM\n      \tdatalake.imhotep.llmproxy lp\n    WHERE\n    \trequestid = '0f7ae361-eeda-45db-90bb-26540b984598'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\nlimit 50\n",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 356,
  "runStartToQueryComplete" : 2449
}, {
  "elapsedMillis" : 4202,
  "totalScheduledMillis" : 598305,
  "cpuMillis" : 36586,
  "queuedMillis" : 1,
  "executeMillis" : 2908,
  "getResultMillis" : 0,
  "iterateMillis" : 1310,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 3689157514,
  "query" : "select fcc_id, COUNT(*) as unique_visitors\nfrom datalake.glassdoor.unique_visitors\nwhere day between '2023-11-01' and '2024-10-01' and fcc_id in (119783177,119783888)\nGROUP BY fcc_id",
  "queryTables" : [ "datalakehive.glassdoor.unique_visitors" ],
  "queryIndex" : 357,
  "runStartToQueryComplete" : 2470
}, {
  "elapsedMillis" : 138175,
  "totalScheduledMillis" : 4691450,
  "cpuMillis" : 1030946,
  "queuedMillis" : 0,
  "executeMillis" : 3354,
  "getResultMillis" : 0,
  "iterateMillis" : 134831,
  "rows" : 1500,
  "error" : null,
  "scannedBytes" : 17620813445,
  "query" : "WITH \n\ncalendar AS (\n\n    SELECT\n    \tDATE_ADD('day', days, DATE('2023-10-01')) AS snapshot_date, 1 AS tmpJoin\n    FROM\n        (SELECT SEQUENCE(0, 1500) AS d)\n    CROSS JOIN \n    \tUNNEST (d) as t(days)\n    WHERE\n    \tDATE_ADD('day', days, DATE('2023-10-01')) <= CURRENT_DATE\n\n),\n\nengagement_daily AS (\n\n    SELECT\t\n        DATE(activity_time) AS activity_date,\n        advertiser_id,\n        subscription_tier,\n        usage_category,\n        subscription_id,\n        SUM(contact_sent) AS contacts_sent,\n        SUM(positive_response) AS positive_responses\n    FROM \n        datalake.employer_analytics_platform.xpa_fct_sourcing_engagement xpa\n    WHERE \n        activity_time >= DATE('2023-11-01')\n        AND subscription_tier IN ('Standard','Professional')\n        AND is_free_contact = 0\n    GROUP BY 1, 2, 3, 4, 5\n\n)\n\nSELECT \n\tCAST(snapshot_date AS varchar) AS snapshot_date,\n    subscription_tier,\n    usage_category,\n    SUM(contacts_sent) AS contacts_sent,\n    SUM(positive_responses) AS pos_resp,\n    COUNT(DISTINCT advertiser_id) AS num_adv,\n    TRY((SUM(contacts_sent) * 1.0000) / COUNT(DISTINCT advertiser_id)) AS contacts_per_adv,\n    TRY((SUM(contacts_sent) * 1.0000) / COUNT(DISTINCT subscription_id)) AS contacts_per_sub,\n    TRY((SUM(positive_responses) * 1.0000) / SUM(contacts_sent)) AS pos_resp_rate\nFROM \n\tcalendar\nINNER JOIN\n\tengagement_daily\n        \tON engagement_daily.activity_date > DATE_ADD('day', -14, calendar.snapshot_date)\n            AND engagement_daily.activity_date <= calendar.snapshot_date\nWHERE \n\tsnapshot_date >= DATE('2024-01-01')\nGROUP BY 1, 2, 3\nORDER BY 1, 2, 3",
  "queryTables" : [ "datalakehive.employer_analytics_platform.xpa_fct_sourcing_engagement" ],
  "queryIndex" : 358,
  "runStartToQueryComplete" : 2636
}, {
  "elapsedMillis" : 10453,
  "totalScheduledMillis" : 460532,
  "cpuMillis" : 267981,
  "queuedMillis" : 0,
  "executeMillis" : 6390,
  "getResultMillis" : 0,
  "iterateMillis" : 4087,
  "rows" : 720,
  "error" : null,
  "scannedBytes" : 533780668,
  "query" : "WITH \n  q1 AS(\n      SELECT DISTINCT DATE(trial_start_date) AS date\n      FROM datalake.analysis.dim_resume_trial_attribution\n      WHERE trial_start_date BETWEEN DATE('2023-01-01') AND FROM_UNIXTIME(IMHOTEP_UNIXTIME('today'))\n  ),\n  q2 AS(\n      SELECT DISTINCT\n          DATE(trial_start_date) AS date,\n          trial_subscription_id,\n          attributed_subscription_id,\n          attribution_date,\n          DATE_DIFF('day', trial_start_date, attribution_date) AS days_to_convert\n      FROM datalake.analysis.dim_resume_trial_attribution\n      WHERE trial_start_date BETWEEN DATE('2023-01-01') AND FROM_UNIXTIME(IMHOTEP_UNIXTIME('today'))\n  )\nSELECT \n\tq1.date,\n    COUNT(DISTINCT IF(q2.days_to_convert <= 180, q2.attributed_subscription_id, NULL)) / CAST(COUNT(DISTINCT q2.trial_subscription_id) AS double) AS trial_conversion_rate_180d,\n    COUNT(DISTINCT IF(q2.days_to_convert <= 30, q2.attributed_subscription_id, NULL)) / CAST(COUNT(DISTINCT q2.trial_subscription_id) AS double) AS trial_conversion_rate_30d\nFROM\n\tq1, \n    q2\nWHERE \n\tq2.date BETWEEN q1.date - INTERVAL '42' DAY AND q1.date - INTERVAL '14' DAY\nGROUP BY 1\nORDER BY 1",
  "queryTables" : [ "datalakehive.analysis.dim_resume_trial_attribution", "datalakehive.analysis.dim_resume_trial_attribution" ],
  "queryIndex" : 359,
  "runStartToQueryComplete" : 2511
}, {
  "elapsedMillis" : 1255,
  "totalScheduledMillis" : 663,
  "cpuMillis" : 22,
  "queuedMillis" : 0,
  "executeMillis" : 364,
  "getResultMillis" : 0,
  "iterateMillis" : 901,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 4699038,
  "query" : "select rsvp_accountid,\n       rsvp_status,\n       rsvp_uuid,\n       unixtime,\n       disposition,\n       jobseeker_show_time_delta,\n       employer_show_time_delta,\n       event_client_id\nFROM imhotep.interviewrecord\nWHERE day >= '2022-12-01' AND day < '2022-12-02'\n  and event_client_id IN ('api_events')\nLIMIT 100",
  "queryTables" : [ "skipperhive.imhotep.interviewrecord" ],
  "queryIndex" : 360,
  "runStartToQueryComplete" : 2518
}, {
  "elapsedMillis" : 41118,
  "totalScheduledMillis" : 76070844,
  "cpuMillis" : 13486655,
  "queuedMillis" : 0,
  "executeMillis" : 9326,
  "getResultMillis" : 0,
  "iterateMillis" : 31839,
  "rows" : 2,
  "error" : null,
  "scannedBytes" : 438388579727,
  "query" : "with cjs as (\nSELECT \n    dcr.job_hash,\n    sum(cast(dcr.numhiresMade as int)) as numhiresMade,\n    sum(cast(dcr.numHiresMadeIndeed as int)) as numHiresMadeIndeed,\n    sum(cast(dcr.numHiresMadeExternal as int)) as numHiresMadeExternal\nFROM datalake.imhotep.dradisClosedJobReason as dcr\nWHERE dcr.unixtime BETWEEN IMHOTEP_UNIXTIME('2024-04-01') AND IMHOTEP_UNIXTIME('2024-10-24')\n\tand dcr.numhiresMade> '0'\n    and closedJobReason = 'notIndeed'\ngroup by 1\n)\n,job_id_job_hash_mapping as (\nSELECT \n\tjobhash_underscore, max(agg_job_id) as agg_job_id\nFROM datalake.imhotep.job_id_job_hash_mapping\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-23') AND IMHOTEP_UNIXTIME('2024-10-24')\n\tand jobhash_underscore in (select distinct job_hash from cjs)\ngroup by 1 \n)\n,jam as (\nselect \n\tagg_job_id,\n    max(job_product) as job_product,\n    max(job_country_code) as job_country_code,\n    sum(clicks) as clicks,\n    sum(apply_starts) as apply_starts,\n    sum(applies) as applies,\n    max(job_age_since_created) as max_job_age_days\nfrom datalake.imhotep.jobactivitymetrics\nwhere day between '2022-04-01' and 'today'\t\n\tand agg_job_id in (select distinct agg_job_id from job_id_job_hash_mapping)\ngroup by 1 \n)\n\n,view as (\nselect \n\tjob_id_job_hash_mapping.agg_job_id,\n    coalesce(applies,0) as applies,\n    coalesce(apply_starts,0) as apply_starts,\n    coalesce(clicks,0) as clicks,\n    numhiresMade,\n    numHiresMadeIndeed,\n    numHiresMadeExternal,\n    job_product,\n    job_country_code,\n    max_job_age_days\nfrom cjs \njoin job_id_job_hash_mapping on job_id_job_hash_mapping.jobhash_underscore = cjs.job_hash\nleft join jam\n\ton job_id_job_hash_mapping.agg_job_id = jam.agg_job_id\n)\n\nselect \n   case when hs.jobid is not null then 1 else 0 end as reported_internally, \n   sum(view.numhiresMade) as hires,\n   1.00*count(distinct case when view.applies>0 then view.agg_job_id end)/count(distinct view.agg_job_id) as pct_job_w_apply,\n   1.00*count(distinct case when view.applies<10 then view.agg_job_id end)/count(distinct view.agg_job_id) as pct_job_w_lessthan10applies,\n   avg(view.applies) as avg_applies,\n   avg(view.apply_starts) as avg_applystarts,\n   avg(view.clicks) as avg_clicks,\n   avg(max_job_age_days) as avg_job_age\nfrom view \nleft join datalake.imhotep.hiredsignal hs \n\ton hs.unixtime BETWEEN IMHOTEP_UNIXTIME('2022-04-01') AND IMHOTEP_UNIXTIME('2024-10-24') \n    and view.agg_job_id = hs.jobid \ngroup by 1",
  "queryTables" : [ "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.dradisclosedjobreason", "datalakehive.imhotep.hiredsignal", "datalakehive.imhotep.job_id_job_hash_mapping", "datalakehive.imhotep.job_id_job_hash_mapping", "datalakehive.imhotep.jobactivitymetrics" ],
  "queryIndex" : 361,
  "runStartToQueryComplete" : 2571
}, {
  "elapsedMillis" : 21896,
  "totalScheduledMillis" : 69231051,
  "cpuMillis" : 16994005,
  "queuedMillis" : 0,
  "executeMillis" : 5750,
  "getResultMillis" : 0,
  "iterateMillis" : 16180,
  "rows" : 18,
  "error" : null,
  "scannedBytes" : 295241964864,
  "query" : "with js as (\nselect \n    account_id\n    , sourcing_eligible \n    , has_first_and_last_name_flag\n    , location_geocoded_flag \n    , has_occ_category_suid\n    , has_skills_suid\n    , has_credential_suid\n    , has_min_pay\n    , wordcount_over_50\nfrom (\n    select \n          this_month.account_id \n          , minimally_complete\n          , if(sourcing_eligible, 1, 0) sourcing_eligible\n          \n          --First and last name are filled out\n          , case when (has_resume_first_name and has_resume_last_name) then 1 else 0 end as has_first_and_last_name_flag\n          \n          -- Location is geocoded to city, postal, or address precision\n          --   , case when resume_location_geocode_precision like 'POSTAL' or resume_location_geocode_precision like 'ADDRESS' then 1 else 0 end as location_geocoded_flag\n          , case when if(resume_location_geocode_precision in ('POSTAL', 'ADDRESS'), 1, 0)\n                    + if(profile_location_geocode_precision in ('POSTAL', 'ADDRESS'), 1, 0) \n                    > 0 then 1 else 0 end as location_geocoded_flag\n          \n          --User-submitted OR resume-extracted occupation category SUID\n          , case when coalesce(CARDINALITY(normalizeddata_resumeextracted_occupations_suids_array),0) \n                    + coalesce(CARDINALITY(normalizeddata_user_pos_occupations_suids_array),0)\n                    + coalesce(CARDINALITY(normalizeddata_user_preferences_occupations_suids_array),0) \n                    > 0 then 1 else 0 end as has_occ_category_suid\n          \n          --User-submitted OR resume-extracted skills SUID\n          , case when coalesce(CARDINALITY(normalizeddata_resumeextracted_skills_suids_array),0) \n                    + coalesce(CARDINALITY(normalizeddata_user_pos_skills_suids_array),0) \n                    + coalesce(CARDINALITY(filter(resume_skills_array_taxonomy_concept_title_suid, q -> q!='')),0) \n                    > 0 then 1 else 0 end as has_skills_suid\n          \n          --User-submitted OR resume-extracted credential SUID (education OR certification OR license)\n          , case when coalesce(CARDINALITY(filter(normalizeddata_resumeextracted_certifications_suids_array, q -> q!='')),0)\n                    + coalesce(CARDINALITY(normalizeddata_user_pos_certifications_suids_array),0)\n                    + coalesce(CARDINALITY(normalizeddata_resumeextracted_licenses_suids_array),0) \n                    + coalesce(CARDINALITY(normalizeddata_user_pos_licenses_suids_array),0)\n                    + coalesce(CARDINALITY(filter(resume_education_0_degrees_taxonomy_concept_degree_suid, q -> q!='')),0)\n                    + coalesce(CARDINALITY(filter(normalizeddata_user_pos_educations_degree_suids_array, q -> q!='')),0)\n                    + coalesce(CARDINALITY(resume_military_array_branch),0)\n                    > 0 then 1 else 0 end as has_credential_suid\n          \n          --User-submmited desired minimum pay\n          , case when coalesce(CARDINALITY(normalizeddata_user_desiredminpay_array),0) > 0 then 1 else 0 end as has_min_pay\n          \n          --Resume wordcount over 50\n          , case when resume_total_wordcount > 50 then 1 else 0 end as wordcount_over_50\n          \n    from datalake.imhotep.js_fact_store_snapshot_prod this_month\n    where \n        hour between '2024-06-30' and '2024-07-01'\n        --and not minimally_complete\n          -- and upper(resume_geo_country) = 'US'\n    )\ngroup by 1,2,3,4,5,6,7,8,9\n),\n\nuswr as (\n    select \n        distinct account_id \n    from datalake.imhotep.usersession_with_reach\n    where hour between '2024-06-01' and '2024-07-01'\n        and (not (contains(grp, 'spider') or contains(grp, 'privileged')) or grp is null)\n        and coalesce(islikelybot,0) != 1\n        and account_id > 0\n)\n\nselect \n    sourcing_eligible + has_first_and_last_name_flag + location_geocoded_flag + has_occ_category_suid + has_skills_suid + has_credential_suid + has_min_pay + wordcount_over_50 as num_attr \n    , if(uswr.account_id is not null, 1, 0) active_flag \n    , count(distinct js.account_id) as num_js \nfrom js \nleft join uswr \n    on js.account_id = uswr.account_id \ngroup by 1,2\norder by 1,2",
  "queryTables" : [ "datalakehive.imhotep.js_fact_store_snapshot_prod", "datalakehive.imhotep.usersession_with_reach" ],
  "queryIndex" : 362,
  "runStartToQueryComplete" : 2557
}, {
  "elapsedMillis" : 3852,
  "totalScheduledMillis" : 64945,
  "cpuMillis" : 18114,
  "queuedMillis" : 1,
  "executeMillis" : 2457,
  "getResultMillis" : 0,
  "iterateMillis" : 1413,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 65985385,
  "query" : "SELECT scope, lower(indexname), count(*) as numevents, count(distinct indexname) as numindexnames\nFROM datalake.imhotep.datalakeinversionevent\nWHERE day>='2024-10-27'\nand eventtype='INVERSION_COMPLETE'\n\ngroup by 1, 2\nhaving count(distinct indexname) > 1\nLIMIT 1000",
  "queryTables" : [ "datalakehive.imhotep.datalakeinversionevent" ],
  "queryIndex" : 363,
  "runStartToQueryComplete" : 2562
}, {
  "elapsedMillis" : 11304,
  "totalScheduledMillis" : 547257,
  "cpuMillis" : 53456,
  "queuedMillis" : 0,
  "executeMillis" : 4849,
  "getResultMillis" : 0,
  "iterateMillis" : 6482,
  "rows" : 117301,
  "error" : null,
  "scannedBytes" : 8963747431,
  "query" : "SELECT \nappName,\n\tadvertiserId,\n\tadvertisementId,\n    adsAdvertisementAction.unixTime,\n    format_datetime(from_unixtime(adsAdvertisementAction.unixTime), 'yyyy-MM-dd') as occurred,\n    \n    timeStamp,\n    status,\n    action,\n    jobsQuery,\n    \n    startDate,\n    format_datetime(from_unixtime(startDate), 'yyyy-MM-dd') as startDateParsed,\n    endDate,\n    format_datetime(from_unixtime(endDate), 'yyyy-MM-dd') as startDateParsed,\n    budgetType,\n    budgetOnetimeLimitWithCurrency_currency,\n    budgetMonthlyLimitWithCurrency_currency,\n    CASE budgetType\n    \tWHEN 'AVG_DAILY'|| 'LIFETIME' THEN  budgetOnetimeLimitWithCurrency_currency\n        WHEN 'MONTHLY' THEN budgetMonthlyLimitWithCurrency_currency\n        \n    END as budgetCurrency,\n    currencyCode,\n    CASE budgetType\n    \tWHEN 'AVG_DAILY'|| 'LIFETIME' THEN  budgetOnetimeLimitWithCurrency_amount\n        WHEN 'MONTHLY' THEN CAST(budgetMonthlyLimitWithCurrency_amount AS bigint)\n        \n    END as budgetValue,\n    budgetProrationBehavior__autosync, --prorateFirstMonth,\n    channelConfigs__autosync, --selectedChannels,\n    name,\n    trackingToken,\n    ctxAccountId__autosync,--accountId,\n    srep, --masquerade user\n    ctxAccountId__autosync,\n    ctxOAuthClientId__autosync,  -- Oyster API app\n\temp.primary_work_email as indeedUser\nFROM datalake.imhotep.adsAdvertisementAction\nLEFT JOIN datalake.imhotep.indeedemployeesnapshot emp ON srep = emp.id AND emp.unixtime BETWEEN IMHOTEP_UNIXTIME('10d') AND IMHOTEP_UNIXTIME('today')\nWHERE adsAdvertisementAction.unixtime BETWEEN IMHOTEP_UNIXTIME('10d') AND IMHOTEP_UNIXTIME('today')\nand srep is null-- and emp.primary_work_email is null\nAND appName in (\n\t'OcampPluginWebappDaemon', \n    'CamtPluginWebappDaemon', \n    'eax-api', \n    'EaxApiDaemon', \n    'one-host-prod', \n    'one-host-prod__objective-campaign',\n    'one-host-prod__curios',\n\t'one-host-prod__sjm-st-recommendations-panel',\n    'one-host-prod__eax-reporting'\n) -- configurable*/\nand ctxAccountId__autosync is not null\n--and (budgetMonthlyLimitWithCurrency_currency is null OR budgetMonthlyLimitWithCurrency_currency is null)\n--and\tctxAccountId__autosync = 265254074 \n--and advertiserId = 9709551\n --and advertiserId = 68631927 \n-- and advertisementId = 423211709\nORDER BY advertiserId, advertisementId, unixtime DESC\n--limit 100\n",
  "queryTables" : [ "datalakehive.imhotep.adsadvertisementaction", "datalakehive.imhotep.indeedemployeesnapshot" ],
  "queryIndex" : 364,
  "runStartToQueryComplete" : 2581
}, {
  "elapsedMillis" : 28374,
  "totalScheduledMillis" : 30977549,
  "cpuMillis" : 9390117,
  "queuedMillis" : 0,
  "executeMillis" : 10646,
  "getResultMillis" : 0,
  "iterateMillis" : 17802,
  "rows" : 2632,
  "error" : null,
  "scannedBytes" : 318807177706,
  "query" : "with sj_perf as (select\na.activity_month,\na.advertiser_id,\na.advertisement_id,\na.job_id,\n'sponsored' as sponsored,\nsum(a.impressions) sj_impressions,\nsum(a.clicks) sj_clicks,\nsum(a.apply_starts) sj_apply_starts,\nsum(a.applies) sj_applies,\nsum(a.charged_amount_local) charged_amount_local\nfrom datalake.core.jobs_fct_job_performance_sponsored_monthly a\nwhere a.activity_month between cast('2023-10-01' as date) and cast('2024-09-30' as date)\nand a.advertiser_id in (72681337)\ngroup by 1,2,3,4\n),\n\noj_id as (select distinct\na.job_id\nfrom datalake.core.jobs_dim_job_attributes_latest a\nwhere source_id in (1008003) and upper(a.job_country) IN ('IT') --and source_id in (2071173,17291098,19548234,10719845)\n),\n\noj_perf as (select\na.activity_month,\na.job_id,\nsum(a.impressions) oj_impressions,\nsum(a.clicks) oj_clicks,\nsum(a.apply_starts) oj_apply_starts,\nsum(a.applies) oj_applies\nfrom datalake.core.jobs_fct_job_performance_organic_monthly a\ninner join oj_id b\non a.job_id = b.job_id\nwhere a.activity_month between cast('2023-10-01' as date) and cast('2024-09-30' as date)\ngroup by 1,2\n),\n\nall_perf as (select\nCOALESCE(a.activity_month,b.activity_month) activity_month,\nCOALESCE(a.job_id,b.job_id) job_id,\na.advertisement_id,\na.sponsored,\na.sj_impressions,\na.sj_clicks,\na.sj_apply_starts,\na.sj_applies,\na.charged_amount_local,\nb.oj_impressions,\nb.oj_clicks,\nb.oj_apply_starts,\nb.oj_applies\nfrom sj_perf a\nfull outer join oj_perf b\non a.job_id = b.job_id\nand a.activity_month = b.activity_month\n)\n\nselect\n--a.activity_month,\na.advertisement_id,\na.job_id,\nCOALESCE(a.sponsored,'organic') job_type,\nc.campaign_name,\nb.job_city,\n--b.job_state,\nb.job_country,\nb.norm_title,\nb.norm_title_category,\nb.actual_title,\na.sj_impressions,\na.sj_clicks,\na.sj_apply_starts,\na.sj_applies,\na.charged_amount_local,\na.oj_impressions,\na.oj_clicks,\na.oj_apply_starts,\na.oj_applies\n\nfrom all_perf a\n\nleft join datalake.core.jobs_dim_job_attributes_latest b\non a.job_id = b.job_id\n\nleft join datalake.core.jobs_dim_campaign_latest c  \non a.advertisement_id = c.advertisement_id  ",
  "queryTables" : [ "datalakehive.core.jobs_dim_campaign_latest", "datalakehive.core.jobs_dim_job_attributes_latest", "datalakehive.core.jobs_dim_job_attributes_latest", "datalakehive.core.jobs_fct_job_performance_organic_monthly", "datalakehive.core.jobs_fct_job_performance_sponsored_monthly" ],
  "queryIndex" : 365,
  "runStartToQueryComplete" : 2608
}, {
  "elapsedMillis" : 61478,
  "totalScheduledMillis" : 34759855,
  "cpuMillis" : 9282789,
  "queuedMillis" : 0,
  "executeMillis" : 45155,
  "getResultMillis" : 0,
  "iterateMillis" : 16367,
  "rows" : 126402,
  "error" : null,
  "scannedBytes" : 97907188092,
  "query" : "with rules as (SELECT distinct rule_id\nFROM datalake.imhotep.waldorulesnapshot\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-01') AND IMHOTEP_UNIXTIME('2024-10-08') \nAND CONTAINS(rule_tags, 'cat_risk') AND type=0 AND visibility='nowhere'),\n\nmatches as (SELECT distinct jobid, ruleid\nFROM datalake.imhotep.waldorulematch\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-01') AND IMHOTEP_UNIXTIME('2024-10-08')),\n\nsj as (SELECT distinct jobid, 1 as nowhere\nFROM datalake.imhotep.searchablejobs\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-01') AND IMHOTEP_UNIXTIME('2024-10-08')\nAND waldovisibilitylevel='nowhere' AND feedid!=50461),\n\nfiji as (select distinct jobid, feedSourceId, feedid, sourceid\nfrom datalake.imhotep.fijievents \nwhere unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-01') AND IMHOTEP_UNIXTIME('2024-10-08')\nAND event='jobScanned'),\n\nfraudulent_jobs as (select matches.jobid, 1 as fraud\nfrom matches\ninner join rules\nON matches.ruleid=rules.rule_id\ninner join sj\nON matches.jobid = sj.jobid\nwhere sj.nowhere=1)\n\nselect feedSourceId, CAST(sourceid as VARCHAR) as sourceid, CAST(feedid as VARCHAR) as feedid, count() as numJobs, sum(fraud) as numFrauds, sum(fraud) * 100.000/count() as fraud_percent\nfrom (select fiji.*, coalesce(fraud, 0) as fraud\n      from fiji\n      left join fraudulent_jobs\n      on fiji.jobid=fraudulent_jobs.jobid)\ngroup by feedSourceId, sourceid, feedid\nORDER BY fraud_percent DESC",
  "queryTables" : [ "datalakehive.imhotep.fijievents", "datalakehive.imhotep.searchablejobs", "datalakehive.imhotep.waldorulematch", "datalakehive.imhotep.waldorulesnapshot" ],
  "queryIndex" : 366,
  "runStartToQueryComplete" : 2641
}, {
  "elapsedMillis" : 1305,
  "totalScheduledMillis" : 341,
  "cpuMillis" : 90,
  "queuedMillis" : 0,
  "executeMillis" : 1309,
  "getResultMillis" : 0,
  "iterateMillis" : 7,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 16674960,
  "query" : "SELECT *\nFROM datalake.imhotep.prestoquery\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\nLIMIT 10",
  "queryTables" : [ "datalakehive.imhotep.prestoquery" ],
  "queryIndex" : 367,
  "runStartToQueryComplete" : 2583
}, {
  "elapsedMillis" : 13641,
  "totalScheduledMillis" : 423200,
  "cpuMillis" : 205569,
  "queuedMillis" : 0,
  "executeMillis" : 3355,
  "getResultMillis" : 0,
  "iterateMillis" : 10535,
  "rows" : 6464625,
  "error" : null,
  "scannedBytes" : 604135035,
  "query" : "WITH date_range AS (\n    SELECT\n        IMHOTEP_UNIXTIME('1month') AS start_date,\n        IMHOTEP_UNIXTIME('today') AS end_date\n)\nSELECT\n    signin.accountid\nFROM\n    datalake.imhotep.passsigninattempt signin\nJOIN\n    date_range dr ON signin.unixtime BETWEEN dr.start_date AND dr.end_date\nWHERE\n    signin.successful = 1\nGROUP BY\n    signin.accountid\nHAVING\n    COUNT(signin.accountid) >= 2",
  "queryTables" : [ "datalakehive.imhotep.passsigninattempt" ],
  "queryIndex" : 368,
  "runStartToQueryComplete" : 2600
}, {
  "elapsedMillis" : 7940,
  "totalScheduledMillis" : 468989,
  "cpuMillis" : 173116,
  "queuedMillis" : 1,
  "executeMillis" : 5787,
  "getResultMillis" : 0,
  "iterateMillis" : 2182,
  "rows" : 4753,
  "error" : null,
  "scannedBytes" : 4271778345,
  "query" : "WITH \nmost_recent_appid AS (\n\tSELECT j.appid, MAX(j.unixtime) AS lastStartupUnixtime \n    FROM datalake.imhotep.javaruntimeversions j\n    WHERE \n    \tj.unixtime BETWEEN IMHOTEP_UNIXTIME('2023-01-01') AND IMHOTEP_UNIXTIME('today') \n        AND j.env='prod'\n\tGROUP BY appid\n),\nmost_recent_appname AS (\n\tSELECT j.appinstance, MAX(j.unixtime) AS lastStartupUnixtime \n    FROM datalake.imhotep.javaruntimeversions j\n    WHERE \n    \tj.unixtime BETWEEN IMHOTEP_UNIXTIME('2023-01-01') AND IMHOTEP_UNIXTIME('today') \n        AND j.env='prod'\n\tGROUP BY appinstance\n),\njava_starts_appid AS (\n    SELECT \n        FROM_UNIXTIME(j.unixtime) AS startupTime,\n        j.appid,\n        j.gitRepo AS gitRepository,\n        CASE\t\n            WHEN javaversion LIKE '1.8.%' THEN 8\n            WHEN javaversion LIKE '11.%' THEN 11\n            WHEN javaversion LIKE '17.%' THEN 17 \n            ELSE -1\n        END AS javaMajorVersion\n    FROM datalake.imhotep.javaruntimeversions j \n    JOIN \n        most_recent_appid m ON j.appid=m.appid AND m.lastStartupUnixtime=j.unixtime\n    WHERE \n        j.unixtime BETWEEN IMHOTEP_UNIXTIME('2023-01-01') AND IMHOTEP_UNIXTIME('today')\n        AND j.env='prod'\n    ORDER BY startupTime DESC\n),\njava_starts_appname AS (\n    SELECT \n        FROM_UNIXTIME(j.unixtime) AS startupTime,\n        j.appinstance,\n        j.gitRepo AS gitRepository,\n        CASE\t\n            WHEN javaversion LIKE '1.8.%' THEN 8\n            WHEN javaversion LIKE '11.%' THEN 11\n            WHEN javaversion LIKE '17.%' THEN 17 \n            ELSE -1\n        END AS javaMajorVersion\n    FROM datalake.imhotep.javaruntimeversions j \n    JOIN \n        most_recent_appname m ON j.appinstance=m.appinstance AND m.lastStartupUnixtime=j.unixtime\n    WHERE \n        j.unixtime BETWEEN IMHOTEP_UNIXTIME('2023-01-01') AND IMHOTEP_UNIXTIME('today')\n        AND j.env='prod'\n    ORDER BY startupTime DESC\n)\n\nSELECT \n    j.startupTime,\n\tsrs.applicationId,\n    srs.applicationName,\n    j.javaMajorVersion,\n    srs.giturl as gitRepository,\n    srs.runtimeType,\n    srs.criticalityTier,\n    srs.applicationType,\n    (srs.applicationType = 'CRONJOB' and CARDINALITY(srs.orcdeploymentnameprod) > 0) isCronJobInProd,\n    srs.lifeCycle,\n    srs.state,\n    srs.teamId,\n    t.displayname, \n    t.intakejira AS teamJira, t.email AS teamEmail, t.usersslackchannel AS teamSlack,\n    t.tier_0, t.tier_1, t.tier_2, t.tier_3, t.tier_4, \n    t.engineeringleadldap AS ldapTDM, \n    t.productleadldap AS ldapPM, \n    t.programleadldap AS ldapPGM, \n    t.technologyleadldap AS ldapTLA\nFROM datalake.imhotep.serviceregistrysnapshots srs\nLEFT JOIN datalake.imhotep.teamwrkssnapshot t ON srs.teamId=t.teamId\n--LEFT JOIN java_starts_appname j ON srs.applicationName=j.appinstance\nLEFT JOIN java_starts_appid j ON srs.applicationId=j.appid\nWHERE\n\tsrs.unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND t.unixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND srs.applicationType IN ('CRONJOB', 'DAEMON')\n    AND arrays_overlap(srs.runtimeType, ARRAY['jvm', 'spark'])\n    AND srs.lifeCycle != 'UNAVAILABLE'",
  "queryTables" : [ "datalakehive.imhotep.javaruntimeversions", "datalakehive.imhotep.javaruntimeversions", "datalakehive.imhotep.serviceregistrysnapshots", "datalakehive.imhotep.teamwrkssnapshot" ],
  "queryIndex" : 369,
  "runStartToQueryComplete" : 2608
}, {
  "elapsedMillis" : 4731,
  "totalScheduledMillis" : 3285669,
  "cpuMillis" : 272886,
  "queuedMillis" : 0,
  "executeMillis" : 2318,
  "getResultMillis" : 0,
  "iterateMillis" : 2431,
  "rows" : 0,
  "error" : null,
  "scannedBytes" : 38501013375,
  "query" : "SELECT\n\t\tpath,\n        method,\n        status,\n\t\tmoderationtext,\n        flagged,\n        flaggedcategories,\n        flaggedwords,\n \t\tappName,\n        moderationApi, \n        moderationType, \n   \t\tmodel,\n        moderationHeader,\n        locale,               \n        metric,\n        FROM_UNIXTIME(unixtime) as request_date,\n        requestid,\n        requesttrackingid\nFROM\n      \tdatalake.imhotep.llmproxy lp\n    WHERE\n    \trequestid = '0f7ae361-eeda-45db-90bb-26540b984598'\n        and unixtime > imhotep_unixtime(DATE(CURRENT_DATE) - interval '4' day)\nlimit 50\n",
  "queryTables" : [ "datalakehive.imhotep.llmproxy" ],
  "queryIndex" : 370,
  "runStartToQueryComplete" : 2606
}, {
  "elapsedMillis" : 7411,
  "totalScheduledMillis" : 385668,
  "cpuMillis" : 155949,
  "queuedMillis" : 1,
  "executeMillis" : 6613,
  "getResultMillis" : 0,
  "iterateMillis" : 851,
  "rows" : 1309,
  "error" : null,
  "scannedBytes" : 4313424379,
  "query" : "WITH tickets AS (\n    SELECT \n    \toriginalissuekey AS jiraId,\n        LTRIM(REGEXP_EXTRACT(labels, '(appid--([0-9a-fA-F]{8}\\b-[0-9a-fA-F]{4}\\b-[0-9a-fA-F]{4}\\b-[0-9a-fA-F]{4}\\b-[0-9a-fA-F]{12}))'), 'appid--') AS applicationId\n    FROM imhotep.jiraactions\n    WHERE \n        action='create' \n        AND reporterusername='mredding' \n        AND labels LIKE '%Java17_Migration%' \n        AND unixtime BETWEEN IMHOTEP_UNIXTIME('2023-11-01') AND IMHOTEP_UNIXTIME('today')\n), processed AS (\n    SELECT \n        FROM_UNIXTIME(ja.unixtime) AS action_time, \n        ja.originalissuekey AS originalTicketId, \n        ja.issuekey AS currentTicketId, \n        ja.action, \n        ja.prevstatus, \n        ja.status, \n        ja.resolution\n    FROM imhotep.jiraactions ja\n    WHERE\n        ja.originalissuekey IN (SELECT jiraId FROM tickets)\n        AND ((ja.action='create') OR (ja.action='update' AND prevstatus!=status))\n        AND unixtime BETWEEN IMHOTEP_UNIXTIME('2023-11-01') AND IMHOTEP_UNIXTIME('today')\n), most_recent AS (\n\tSELECT originalTicketId, MAX(action_time) AS action_time FROM processed\n    GROUP BY originalTicketId\n)\n\nSELECT p.*, t.applicationId\nFROM processed p\nJOIN \n    most_recent m ON p.originalTicketId=m.originalTicketId AND p.action_time=m.action_time\nJOIN\n\ttickets t ON p.originalTicketId=t.jiraId",
  "queryTables" : [ "skipperhive.imhotep.jiraactions", "skipperhive.imhotep.jiraactions", "skipperhive.imhotep.jiraactions", "skipperhive.imhotep.jiraactions", "skipperhive.imhotep.jiraactions" ],
  "queryIndex" : 371,
  "runStartToQueryComplete" : 2613
}, {
  "elapsedMillis" : 9126,
  "totalScheduledMillis" : 10004510,
  "cpuMillis" : 487159,
  "queuedMillis" : 0,
  "executeMillis" : 6411,
  "getResultMillis" : 0,
  "iterateMillis" : 2829,
  "rows" : 281489,
  "error" : null,
  "scannedBytes" : 33504417648,
  "query" : "WITH jp_js AS (\n        SELECT DISTINCT(account_id) AS accountid\n        FROM datalake.imhotep.js_fact_store_snapshot_prod\n        WHERE hour >= '2024-10-21' AND hour < '2024-10-23'\n        AND (resume_geo_country = 'jp' or resume_location_country='jp' or resume_ip_country='jp' or resume_locale IN ('ja', 'ja_JP'))\n        AND resume_work_size > 0\n    )\n    SELECT DISTINCT mdp.accountid AS acctid\n    FROM datalake.mdp_offline_feature_store.jobseeker_feature_jsdp_resume_extraction_accountid mdp\n    INNER JOIN jp_js ON jp_js.accountid = mdp.accountid\n    WHERE eventtime BETWEEN date_parse('2024-10-18 05:21', '%Y-%m-%d %H:%i') \n          AND date_parse('2024-10-21 06:05', '%Y-%m-%d %H:%i')",
  "queryTables" : [ "datalake.mdp_offline_feature_store.jobseeker_feature_jsdp_resume_extraction_accountid", "datalakehive.imhotep.js_fact_store_snapshot_prod" ],
  "queryIndex" : 372,
  "runStartToQueryComplete" : 2625
}, {
  "elapsedMillis" : 19594,
  "totalScheduledMillis" : 49076328,
  "cpuMillis" : 11722581,
  "queuedMillis" : 0,
  "executeMillis" : 3757,
  "getResultMillis" : 0,
  "iterateMillis" : 15855,
  "rows" : 65,
  "error" : null,
  "scannedBytes" : 162615905536,
  "query" : "SELECT\n\tDATE_TRUNC('month', DATE(SUBSTR(day, 1, 10))) AS activity_month,\n    pcss AS parent_company_size_segment,\n    COUNT(DISTINCT agg_job_id) AS num_jobs\nFROM \n\tdatalake.imhotep.jobactivitymetrics\nWHERE \n\tday >= '2023-10-01'\n    AND day < '2024-11-01'\n\tAND is_job_searchable=1\nGROUP BY\n\t1, 2\n",
  "queryTables" : [ "datalakehive.imhotep.jobactivitymetrics" ],
  "queryIndex" : 373,
  "runStartToQueryComplete" : 2640
}, {
  "elapsedMillis" : 14350,
  "totalScheduledMillis" : 9528101,
  "cpuMillis" : 2376791,
  "queuedMillis" : 0,
  "executeMillis" : 8143,
  "getResultMillis" : 0,
  "iterateMillis" : 8705,
  "rows" : 10,
  "error" : null,
  "scannedBytes" : 33479778486,
  "query" : "WITH scalablesegmenthomepageimpression_ AS (\n  SELECT accountid\n         ,agg_job_id \n         ,qual_attributes_job_js_overlap_positive_count\n         ,qual_job_attributes_count\n         ,qual_job_attributes\n         ,'scalablesegmenthomepageimpression' AS source\n  FROM datalake.imhotep.scalablesegmenthomepageimpression\n  WHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-02') AND IMHOTEP_UNIXTIME('2024-10-03')\n  AND qual_attributes_job_js_overlap_positive_count > 0\n  AND  qual_job_attributes_count > 0 \n  GROUP BY 1, 2, 3, 4, 5\n)\n\nSELECT a.account_id\n       ,a.job_id\n       ,a.overlappingjobandjsqualifications AS cnt_overlapping_quals_mce\n       ,b.qual_attributes_job_js_overlap_positive_count AS cnt_overlapping_quals_scalableseg\n       ,a.numjobqualifications AS cnt_job_quals_mce\n       ,b.qual_job_attributes_count AS cnt_job_quals_scalableseg\n       ,b.qual_job_attributes\nFROM datalake.imhotep.mce_evaluation AS a \nINNER JOIN scalablesegmenthomepageimpression_ AS b\nON a.account_id = b.accountid \nAND a.job_id = b.agg_job_id\nWHERE unixtime BETWEEN IMHOTEP_UNIXTIME('2024-10-02') AND IMHOTEP_UNIXTIME('2024-10-03')\nAND a.overlappingjobandjsqualifications != b.qual_attributes_job_js_overlap_positive_count \nAND a.numjobqualifications != b.qual_job_attributes_count\nLIMIT 10 ",
  "queryTables" : [ "datalakehive.imhotep.mce_evaluation", "datalakehive.imhotep.scalablesegmenthomepageimpression" ],
  "queryIndex" : 374,
  "runStartToQueryComplete" : 2640
}, {
  "elapsedMillis" : 6576,
  "totalScheduledMillis" : 2250,
  "cpuMillis" : 1170,
  "queuedMillis" : 0,
  "executeMillis" : 4316,
  "getResultMillis" : 0,
  "iterateMillis" : 2270,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 826077,
  "query" : "SELECT\n    cast(json_extract(element_at(cast(json_parse(chatmessages) as array<JSON>),\n    cardinality(cast(json_parse(chatmessages) as array<JSON>)) - 1), '$.content') as varchar) as job_description,\n    cast(json_extract(element_at(cast(json_parse(chatmessages) as array<JSON>),\n    cardinality(cast(json_parse(chatmessages) as array<JSON>))), '$.content') as varchar) as jobseeker_resume,\n    rawContent as generated_introduction\nFROM logrepo.log.GeneratedContentLog\nWHERE uid='1i53178bs37nf001'",
  "queryTables" : [ "logrepo.log.generatedcontentlog" ],
  "queryIndex" : 375,
  "runStartToQueryComplete" : 2649
}, {
  "elapsedMillis" : 525,
  "totalScheduledMillis" : 210,
  "cpuMillis" : 162,
  "queuedMillis" : 0,
  "executeMillis" : 484,
  "getResultMillis" : 0,
  "iterateMillis" : 51,
  "rows" : 76,
  "error" : null,
  "scannedBytes" : 5380,
  "query" : "desc datalake.imhotep.prestoquery",
  "queryTables" : [ "datalakehive.information_schema.columns" ],
  "queryIndex" : 376,
  "runStartToQueryComplete" : 2660
}, {
  "elapsedMillis" : 158444,
  "totalScheduledMillis" : 903643,
  "cpuMillis" : 416693,
  "queuedMillis" : 0,
  "executeMillis" : 4278,
  "getResultMillis" : 0,
  "iterateMillis" : 154178,
  "rows" : 1,
  "error" : null,
  "scannedBytes" : 181074159,
  "query" : "select count(1) from logrepo.raw_log.orgClk where unixtime >= imhotep_unixtime('2019-11-02 00:00:00') and unixtime < imhotep_unixtime('2019-11-02 01:00:00')",
  "queryTables" : [ "logrepo.raw_log.orgclk" ],
  "queryIndex" : 377,
  "runStartToQueryComplete" : 2860
}, {
  "elapsedMillis" : 1625,
  "totalScheduledMillis" : 2180,
  "cpuMillis" : 394,
  "queuedMillis" : 0,
  "executeMillis" : 485,
  "getResultMillis" : 0,
  "iterateMillis" : 1148,
  "rows" : 11150,
  "error" : null,
  "scannedBytes" : 34364230,
  "query" : "select current_charge from datalake.public.oyster_min_spend_yellowstone_staging where day = '2024-10-23'",
  "queryTables" : [ "datalakehive.public.oyster_min_spend_yellowstone_staging" ],
  "queryIndex" : 378,
  "runStartToQueryComplete" : 2719
}, {
  "elapsedMillis" : 1228,
  "totalScheduledMillis" : 5584,
  "cpuMillis" : 433,
  "queuedMillis" : 0,
  "executeMillis" : 1009,
  "getResultMillis" : 0,
  "iterateMillis" : 330,
  "rows" : 100,
  "error" : null,
  "scannedBytes" : 1278917,
  "query" : "  SELECT \n  \tadvertiserid as advertiser_id\n    , t.permission as product_perm\n    , accountid as account_id\n  FROM \n  \tdatalake.imhotep.advertiserusers  \n  \tCROSS JOIN UNNEST (permissionSetNames) as T (permission)\n  WHERE \n  \tunixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    --AND advertiserid = '39970340'\n  GROUP BY\n    advertiserid \n    , t.permission \n    , accountid\n\nLIMIT 100",
  "queryTables" : [ "datalakehive.imhotep.advertiserusers" ],
  "queryIndex" : 379,
  "runStartToQueryComplete" : 2721
}, {
  "elapsedMillis" : 3331,
  "totalScheduledMillis" : 46382,
  "cpuMillis" : 7221,
  "queuedMillis" : 0,
  "executeMillis" : 809,
  "getResultMillis" : 0,
  "iterateMillis" : 2545,
  "rows" : 4,
  "error" : null,
  "scannedBytes" : 942025332,
  "query" : "  SELECT \n  \tadvertiserid as advertiser_id\n    , t.permission as product_perm\n    , accountid as account_id\n  FROM \n  \tdatalake.imhotep.advertiserusers  \n  \tCROSS JOIN UNNEST (permissionSetNames) as T (permission)\n  WHERE \n  \tunixtime BETWEEN IMHOTEP_UNIXTIME('1d') AND IMHOTEP_UNIXTIME('today')\n    AND advertiserid = '39970340'\n  GROUP BY\n    advertiserid \n    , t.permission \n    , accountid\n\nLIMIT 100",
  "queryTables" : [ "datalakehive.imhotep.advertiserusers" ],
  "queryIndex" : 380,
  "runStartToQueryComplete" : 2723
}, {
  "elapsedMillis" : 985,
  "totalScheduledMillis" : 1089,
  "cpuMillis" : 355,
  "queuedMillis" : 0,
  "executeMillis" : 373,
  "getResultMillis" : 0,
  "iterateMillis" : 623,
  "rows" : 11150,
  "error" : null,
  "scannedBytes" : 34364230,
  "query" : "select current_charge from datalake.public.oyster_min_spend_yellowstone_staging where day = '2024-10-23'",
  "queryTables" : [ "datalakehive.public.oyster_min_spend_yellowstone_staging" ],
  "queryIndex" : 381,
  "runStartToQueryComplete" : 2737
} ]